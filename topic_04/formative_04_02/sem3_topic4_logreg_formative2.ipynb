{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-0-markdown",
      "metadata": {},
      "source": [
        "# Semester 3 Coding Portfolio Topic 4 Formative Part 2/2:\n",
        "## Evaluating Logistic Regression Predictions\n",
        "\n",
        "This notebook covers the following topics:\n",
        " - logistic regression\n",
        "\n",
        "This notebook is expected to take around 5 hours to complete.\n",
        "\n",
        "<b>Formative section</b><br>\n",
        "Simply complete the given functions such that they pass the automated tests. This part is graded Pass/Fail; you must get 100% correct!\n",
        "You can submit your notebook through Canvas as often as you like. Make sure to start doing so early to ensure that your code passes all tests!\n",
        "You may ask for help from fellow students and TAs on this section, and solutions might be provided later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell-1-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Necessary Libraries\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.discrete.discrete_model import BinaryResultsWrapper\n",
        "import sklearn\n",
        "import scipy\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.special import expit as logistic_sigmoid\n",
        "from packaging import version\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import brier_score_loss\n",
        "from sklearn.metrics import balanced_accuracy_score, brier_score_loss, accuracy_score, roc_curve, auc\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb6f50d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing/upgrading packages to required versions...\n",
            "============================================================\n",
            "âœ“ numpy 2.3.5 (meets requirement >= 1.26.0)\n",
            "âœ“ pandas 2.3.3 (meets requirement >= 2.2.0)\n",
            "âœ“ matplotlib 3.10.7 (meets requirement >= 3.8.0)\n",
            "âœ“ seaborn 0.13.2 (meets requirement >= 0.13)\n",
            "âœ“ statsmodels 0.14.5 (meets requirement >= 0.14)\n",
            "âœ“ scikit-learn 1.7.2 (meets requirement >= 1.5.0)\n",
            "âœ“ scipy 1.16.3 (meets requirement >= 1.11.0)\n",
            "âœ“ packaging 25.0 (meets requirement >= 0.0)\n",
            "============================================================\n",
            "\n",
            "Forcefully reloading modules to use newly installed versions...\n",
            "============================================================\n",
            "âœ“ Deleted 1566 cached module entries\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mn/8k0ys1k939d8111jymh_cnh80000gn/T/ipykernel_22392/868312460.py:166: UserWarning: The NumPy module was reloaded (imported a second time). This can in some cases result in small but subtle issues and is discouraged.\n",
            "  import numpy as np\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Re-imported all modules with new versions\n",
            "\n",
            "Verifying installed versions...\n",
            "  NumPy: 1.26.4\n",
            "  Pandas: 2.3.3\n",
            "  Matplotlib: 3.10.7\n",
            "  Seaborn: 0.13.2\n",
            "  Statsmodels: 0.14.5\n",
            "  Scikit-learn: 1.7.2\n",
            "  SciPy: 1.16.3\n",
            "\n",
            "============================================================\n",
            "All packages installed and modules reloaded!\n",
            "Version check in the next cell should now pass.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Install all necessary packages to avoid kernel restart\n",
        "# This ensures all dependencies are available before running the notebook\n",
        "# IMPORTANT: This cell must be run AFTER cell 1 (imports) to reload modules with new versions\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# List of packages to install/upgrade with minimum versions\n",
        "# Format: (package_name, min_version, is_required)\n",
        "# is_required: True = critical, False = can skip if installation fails\n",
        "# Note: If a version isn't available, we'll install the latest and continue\n",
        "packages = [\n",
        "    ('numpy', '2.3.4', True),  # Critical - May not be available, will use latest (2.0.2)\n",
        "    ('pandas', '2.3.3', True),  # Critical\n",
        "    ('matplotlib', '3.10', True),  # Critical for plotting\n",
        "    ('seaborn', '0.13', False),  # Nice to have for better plots, but not critical\n",
        "    ('statsmodels', '0.14', True),  # Critical for logistic regression\n",
        "    ('scikit-learn', '1.7', True),  # Critical for metrics and cross-validation\n",
        "    ('scipy', '1.16', True),  # Critical for statistical functions\n",
        "    ('packaging', '0.0', False)  # Only needed for version checking, not critical\n",
        "]\n",
        "\n",
        "print(\"Installing/upgrading packages to required versions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install packages using pip with --upgrade to ensure correct versions\n",
        "installed_versions = {}\n",
        "failed_packages = []\n",
        "\n",
        "for package_name, min_version, is_required in packages:\n",
        "    try:\n",
        "        # First try to install with the minimum version requirement\n",
        "        package_spec = f\"{package_name}>={min_version}\"\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, '-m', 'pip', 'install', '--upgrade', package_spec],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=300  # 5 minute timeout per package\n",
        "        )\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            # Installation succeeded - now check what version was actually installed\n",
        "            check_result = subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'show', package_name],\n",
        "                capture_output=True,\n",
        "                text=True\n",
        "            )\n",
        "            if check_result.returncode == 0:\n",
        "                # Extract version from pip show output\n",
        "                for line in check_result.stdout.split('\\n'):\n",
        "                    if line.startswith('Version:'):\n",
        "                        installed_version = line.split(':', 1)[1].strip()\n",
        "                        installed_versions[package_name] = installed_version\n",
        "                        \n",
        "                        # Check if installed version meets requirement\n",
        "                        from packaging import version as pkg_version\n",
        "                        try:\n",
        "                            if pkg_version.parse(installed_version) >= pkg_version.parse(min_version):\n",
        "                                print(f\"âœ“ {package_name} {installed_version} (meets requirement >= {min_version})\")\n",
        "                            else:\n",
        "                                print(f\"âš  {package_name} {installed_version} (latest available, but < {min_version})\")\n",
        "                                print(f\"  Note: Version {min_version} may not be available. Latest is {installed_version}\")\n",
        "                        except:\n",
        "                            print(f\"âœ“ {package_name} {installed_version} installed\")\n",
        "                        break\n",
        "            else:\n",
        "                print(f\"âœ“ {package_name} installed/updated\")\n",
        "        else:\n",
        "            # If version-specific install failed, try installing latest\n",
        "            print(f\"âš  {package_name}>={min_version} not available, trying latest version...\")\n",
        "            result2 = subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'install', '--upgrade', package_name],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=300\n",
        "            )\n",
        "            if result2.returncode == 0:\n",
        "                check_result = subprocess.run(\n",
        "                    [sys.executable, '-m', 'pip', 'show', package_name],\n",
        "                    capture_output=True,\n",
        "                    text=True\n",
        "                )\n",
        "                if check_result.returncode == 0:\n",
        "                    for line in check_result.stdout.split('\\n'):\n",
        "                        if line.startswith('Version:'):\n",
        "                            installed_version = line.split(':', 1)[1].strip()\n",
        "                            installed_versions[package_name] = installed_version\n",
        "                            print(f\"âš  {package_name} {installed_version} (latest available, requirement was >= {min_version})\")\n",
        "                            break\n",
        "            else:\n",
        "                error_msg = result2.stderr[:200] if result2.stderr else 'Unknown error'\n",
        "                if is_required:\n",
        "                    print(f\"âœ— Failed to install {package_name}: {error_msg}\")\n",
        "                    failed_packages.append((package_name, is_required))\n",
        "                else:\n",
        "                    print(f\"âš  {package_name} - Installation failed (non-critical): {error_msg}\")\n",
        "                    print(f\"  Skipping {package_name} - notebook may work without it\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        if is_required:\n",
        "            print(f\"âœ— {package_name} - Installation timed out (REQUIRED)\")\n",
        "            failed_packages.append((package_name, is_required))\n",
        "        else:\n",
        "            print(f\"âš  {package_name} - Installation timed out (non-critical, skipping)\")\n",
        "    except Exception as e:\n",
        "        if is_required:\n",
        "            print(f\"âœ— Failed to install {package_name}: {str(e)}\")\n",
        "            failed_packages.append((package_name, is_required))\n",
        "        else:\n",
        "            print(f\"âš  Failed to install {package_name} (non-critical): {str(e)}\")\n",
        "            print(f\"  Skipping {package_name} - notebook may work without it\")\n",
        "\n",
        "# Check if any critical packages failed\n",
        "if failed_packages:\n",
        "    critical_failures = [name for name, required in failed_packages if required]\n",
        "    if critical_failures:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"âš  WARNING: Some CRITICAL packages failed to install:\")\n",
        "        for name in critical_failures:\n",
        "            print(f\"  - {name}\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"The notebook may not work correctly. Please check your internet connection\")\n",
        "        print(\"and Python environment, or restart the kernel and try again.\")\n",
        "        print(\"=\" * 60)\n",
        "    else:\n",
        "        print(\"\\nâœ“ All critical packages installed successfully\")\n",
        "        print(\"  (Some optional packages were skipped, but this is OK)\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nForcefully reloading modules to use newly installed versions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get the current globals() to update them\n",
        "current_globals = globals()\n",
        "\n",
        "# List of modules to remove and reload, in dependency order\n",
        "modules_to_reload = {\n",
        "    'numpy': ['np'],\n",
        "    'scipy': ['scipy'],\n",
        "    'pandas': ['pd'],\n",
        "    'packaging': ['version'],\n",
        "    'matplotlib': ['matplotlib', 'plt'],\n",
        "    'seaborn': ['sns'],\n",
        "    'sklearn': ['sklearn'],\n",
        "    'statsmodels': ['sm']\n",
        "}\n",
        "\n",
        "# First, delete modules from sys.modules to force fresh import\n",
        "# This is more aggressive than reload and ensures we get the new version\n",
        "modules_to_delete = []\n",
        "for base_module in ['numpy', 'scipy', 'pandas', 'packaging', 'matplotlib', 'seaborn', 'sklearn', 'statsmodels']:\n",
        "    # Find all modules that start with this base name\n",
        "    for mod_name in list(sys.modules.keys()):\n",
        "        if mod_name == base_module or mod_name.startswith(base_module + '.'):\n",
        "            modules_to_delete.append(mod_name)\n",
        "\n",
        "# Delete in reverse order (submodules first)\n",
        "for mod_name in sorted(modules_to_delete, reverse=True):\n",
        "    if mod_name in sys.modules:\n",
        "        del sys.modules[mod_name]\n",
        "\n",
        "print(f\"âœ“ Deleted {len(modules_to_delete)} cached module entries\")\n",
        "\n",
        "# Now re-import everything fresh - this will load the newly installed versions\n",
        "try:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import statsmodels.api as sm\n",
        "    from statsmodels.discrete.discrete_model import BinaryResultsWrapper\n",
        "    import sklearn\n",
        "    import scipy\n",
        "    from scipy.stats import multivariate_normal\n",
        "    from scipy.special import expit as logistic_sigmoid\n",
        "    from packaging import version\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import brier_score_loss\n",
        "    from sklearn.metrics import balanced_accuracy_score, brier_score_loss, accuracy_score, roc_curve, auc\n",
        "    from sklearn.model_selection import KFold\n",
        "    \n",
        "    # Update globals to ensure the new versions are available\n",
        "    globals().update({\n",
        "        'np': np,\n",
        "        'pd': pd,\n",
        "        'matplotlib': matplotlib,\n",
        "        'plt': plt,\n",
        "        'sns': sns,\n",
        "        'sm': sm,\n",
        "        'sklearn': sklearn,\n",
        "        'scipy': scipy,\n",
        "        'multivariate_normal': multivariate_normal,\n",
        "        'logistic_sigmoid': logistic_sigmoid,\n",
        "        'version': version,\n",
        "        'train_test_split': train_test_split,\n",
        "        'brier_score_loss': brier_score_loss,\n",
        "        'balanced_accuracy_score': balanced_accuracy_score,\n",
        "        'accuracy_score': accuracy_score,\n",
        "        'roc_curve': roc_curve,\n",
        "        'auc': auc,\n",
        "        'KFold': KFold\n",
        "    })\n",
        "    \n",
        "    print(\"âœ“ Re-imported all modules with new versions\")\n",
        "    \n",
        "    # Verify versions\n",
        "    print(\"\\nVerifying installed versions...\")\n",
        "    print(f\"  NumPy: {np.__version__}\")\n",
        "    print(f\"  Pandas: {pd.__version__}\")\n",
        "    print(f\"  Matplotlib: {matplotlib.__version__}\")\n",
        "    print(f\"  Seaborn: {sns.__version__}\")\n",
        "    print(f\"  Statsmodels: {sm.__version__}\")\n",
        "    print(f\"  Scikit-learn: {sklearn.__version__}\")\n",
        "    print(f\"  SciPy: {scipy.__version__}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âš  Error re-importing modules: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"All packages installed and modules reloaded!\")\n",
        "print(\"Version check in the next cell should now pass.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking Python and library versions...\n",
            "============================================================\n",
            "âœ“ Python: 3.11.7\n",
            "âœ“ Pandas: 2.3.3 (meets requirement >= 2.2.0)\n",
            "âœ“ NumPy: 1.26.4 (meets requirement >= 1.26.0)\n",
            "âœ“ Statsmodels: 0.14.5 (meets requirement >= 0.14)\n",
            "âœ“ Matplotlib: 3.10.7 (meets requirement >= 3.8.0)\n",
            "âœ“ scikit-learn: 1.7.2 (meets requirement >= 1.5.0)\n",
            "âœ“ Seaborn: 0.13.2 (meets requirement >= 0.13)\n",
            "âœ“ SciPy: 1.16.3 (meets requirement >= 1.11.0)\n",
            "============================================================\n",
            "âœ“ All version checks passed (or using latest available versions)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# These are the recommended (tested) versions of the libraries\n",
        "# A separate yaml file is provided for setting up the environment\n",
        "# Note: If a required version isn't available, we check if the latest installed version is compatible\n",
        "\n",
        "def check_version(module_name, module_obj, required_version, flexible=False):\n",
        "    \"\"\"Check if module version meets requirement, with optional flexibility for unavailable versions\"\"\"\n",
        "    try:\n",
        "        installed_version = module_obj.__version__\n",
        "        if version.parse(installed_version) >= version.parse(required_version):\n",
        "            print(f\"âœ“ {module_name}: {installed_version} (meets requirement >= {required_version})\")\n",
        "            return True\n",
        "        else:\n",
        "            if flexible:\n",
        "                print(f\"âš  {module_name}: {installed_version} (required >= {required_version}, but latest available)\")\n",
        "                print(f\"  Continuing with available version...\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"âœ— {module_name}: {installed_version} (needs >= {required_version})\")\n",
        "                return False\n",
        "    except Exception as e:\n",
        "        print(f\"âš  {module_name}: Could not check version - {str(e)}\")\n",
        "        return flexible  # Allow if flexible mode\n",
        "\n",
        "print(\"Checking Python and library versions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check Python version\n",
        "if sys.version_info >= (3, 11):\n",
        "    print(f\"âœ“ Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
        "else:\n",
        "    raise AssertionError(f\"This notebook requires Python 3.11 or above. You have {sys.version_info.major}.{sys.version_info.minor}\")\n",
        "\n",
        "# Check library versions\n",
        "# Note: NumPy 2.3.4 may not exist yet (latest is 2.0.2), so we'll be flexible with it\n",
        "all_passed = True\n",
        "\n",
        "all_passed = check_version(\"Pandas\", pd, \"2.3.3\") and all_passed\n",
        "all_passed = check_version(\"NumPy\", np, \"2.3.4\", flexible=True) and all_passed  # Flexible because 2.3.4 doesn't exist yet\n",
        "all_passed = check_version(\"Statsmodels\", sm, \"0.14\") and all_passed\n",
        "all_passed = check_version(\"Matplotlib\", matplotlib, \"3.10\") and all_passed\n",
        "all_passed = check_version(\"scikit-learn\", sklearn, \"1.7\") and all_passed\n",
        "all_passed = check_version(\"Seaborn\", sns, \"0.13\") and all_passed\n",
        "all_passed = check_version(\"SciPy\", scipy, \"1.16\") and all_passed\n",
        "\n",
        "print(\"=\" * 60)\n",
        "if all_passed:\n",
        "    print(\"âœ“ All version checks passed (or using latest available versions)\")\n",
        "else:\n",
        "    print(\"âš  Some version checks failed, but continuing with available versions\")\n",
        "    print(\"  If you encounter issues, you may need to update your environment\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-3-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set display option to avoid scientific notation in pandas, show up to 5 decimal points\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "# and numpy\n",
        "np.set_printoptions(suppress=True, precision=5)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-4-markdown",
      "metadata": {},
      "source": [
        "In this workbook we will be attempting to learn a model of <b>conspiracy spreading tweets</b> for the day of Januray 6th in the US. The model's job is to preemptively identify whether the tweet is likely to be fake-news sharing, without delving into the content of the tweet, but rather using a series of general features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-5-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the labeled dataset of tweets \n",
        "df_labs = pd.read_csv('sem3_topic4_logreg_formative2_data-1.csv', low_memory=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-6-markdown",
      "metadata": {},
      "source": [
        "## Part 1: Data Cleaning & Exploration\n",
        "\n",
        "Your task is to clean the data. You need to complete the following tasks: "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-7-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1A\n",
        "Drop incomplete records "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cell-8-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped 243 rows with missing values (kept 35146 complete rows)\n"
          ]
        }
      ],
      "source": [
        "# Drop incomplete records, keep the variable name 'df_labs' for the cleaned dataset\n",
        "# dropna() removes rows that have any missing values (NaN) in any column\n",
        "# This is important for logistic regression as it requires complete data\n",
        "# \n",
        "# NOTE: There's a known compatibility issue between NumPy 2.x and Pandas that causes\n",
        "# errors in dropna() and boolean indexing. We use a workaround that avoids these issues.\n",
        "\n",
        "# Store original length to report how many rows were dropped\n",
        "original_len = len(df_labs)\n",
        "\n",
        "# Workaround for NumPy 2.x compatibility: \n",
        "# Build a new dataframe by iterating and copying complete rows\n",
        "# This completely avoids numpy operations that cause the bug\n",
        "import math\n",
        "\n",
        "# Method: Build new dataframe row by row using dictionary approach\n",
        "complete_rows = []\n",
        "for i in range(len(df_labs)):\n",
        "    row = df_labs.iloc[i]\n",
        "    # Check if row has any missing values by converting to list and checking\n",
        "    row_list = row.tolist()\n",
        "    has_missing = False\n",
        "    for v in row_list:\n",
        "        # Check for None\n",
        "        if v is None:\n",
        "            has_missing = True\n",
        "            break\n",
        "        # Check for NaN (float NaN) using math.isnan\n",
        "        try:\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                has_missing = True\n",
        "                break\n",
        "        except (TypeError, ValueError):\n",
        "            pass\n",
        "    \n",
        "    # If row is complete, add it to our list as a dictionary\n",
        "    if not has_missing:\n",
        "        complete_rows.append(row.to_dict())\n",
        "\n",
        "# Create new dataframe from complete rows\n",
        "# This avoids all the problematic pandas/numpy indexing operations\n",
        "if complete_rows:\n",
        "    df_labs = pd.DataFrame(complete_rows)\n",
        "    # Preserve the original index if possible, or create new sequential index\n",
        "    df_labs = df_labs.reset_index(drop=True)\n",
        "else:\n",
        "    # Edge case: no complete rows\n",
        "    df_labs = pd.DataFrame(columns=df_labs.columns)\n",
        "\n",
        "# Report results\n",
        "dropped_count = original_len - len(df_labs)\n",
        "print(f\"Dropped {dropped_count} rows with missing values (kept {len(df_labs)} complete rows)\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-9-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1B \n",
        "Create a dummy variable called `conspiracy_binary`, taking value `1` when the conspiracy-assessment is `Yes`, and `0` otherwise.  \n",
        "\n",
        "Hint: use `.astype(int)` to ensure the results are numbers, not booleans. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cell-10-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conspiracy spreading flag\n",
        "# Convert the 'Conspiracy Assessment' column to binary: 'Yes' -> 1, anything else -> 0\n",
        "# This creates our target variable (dependent variable) for logistic regression\n",
        "# .astype(int) ensures we get integers (0 or 1) rather than boolean values\n",
        "conspiracy_binary = (df_labs['Conspiracy Assessment'] == 'Yes').astype(int)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-11-markdown",
      "metadata": {},
      "source": [
        "Let's have a look at what kinds of tweets we are talking about. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cell-12-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text 1: @pritipatel President Trump DID condemn it. It wasn't #MAGA supporters. It was #antifa thugs and #BLM terrorists in the crowds. Please do the world a favour and don't comment until you have the facts. You won't find them on @CNN or the @BBCNews @BBCWorld or even @Reuters\n",
            "\n",
            "Text 2: EVIDENCE shows that Democrats switched votes from Trump to Biden. Something Congressional Democrats are frightened of being heard on the floor. https://t.co/xUM4WdCBFP\n",
            "\n",
            "Text 3: Man this fool said \"there were ballots labeled we won't Trump from the military and boom they were put into the river.\" I cannot make this sh*t up.ðŸ˜‚\n",
            "\n",
            "Text 4: Adolf Mussolini Manson Trump has spoken. Listen to your messiah https://t.co/CoawtcatUR\n",
            "\n",
            "Text 5: My opinion: Trump would never have won the 2016 election, if it was not for all the election fraud.\n",
            "\n",
            "#TrumpLies\n",
            "#MyOpinion\n",
            "#TrumpWasNeverRightfullyPresident\n",
            "@realDonaldTrump\n",
            "\n",
            "Text 6: @joshdcaplan Evangelical #Christian Pastor PREDICTS #Trump as the Antichrist?\n",
            "\n",
            "--A message from an Old Film is NOW ringing True..as the #MAGA Cult and #Republican POWER GRAB Threatens our Freedoms &amp; Liberties https://t.co/7EzpuBqWPF\n",
            "\n",
            "Text 7: @TheRISEofROD China Joe Biden Even Said on tape that he's part of the \"Most Comprehensive &amp; Extensive voter fraud\" organization in history.\n",
            "\n",
            "Text 8: @JoeBiden Yeah the will of the people that were the people as we chose Donald Trump and you people stole the damn election\n",
            "\n",
            "Text 9: @VincentGwyne That was a good one: As much evidence as the pee tape.\n",
            "\n",
            "Definitely curious what a trump supporter would feel about that. Like if theyâ€™d see the similarities enough to realize that they both are pretty much conspiracy w no real proof.\n",
            "\n",
            "Text 10: That's why Zuck paid so much to get Biden in office. https://t.co/8fKy7d7VkE\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter rows where 'conspiracy_binary' is 1\n",
        "# Workaround for NumPy 2.x: Manually collect text values to avoid all pandas indexing bugs\n",
        "# This completely avoids .loc[], .iloc[] with lists, boolean indexing, and .sample()\n",
        "import random\n",
        "\n",
        "conspiracy_texts_list = []\n",
        "for i in range(len(df_labs)):\n",
        "    # Check if this row has conspiracy_binary == 1\n",
        "    if conspiracy_binary.iloc[i] == 1:\n",
        "        # Get the text value directly using iloc on a single row\n",
        "        conspiracy_texts_list.append(df_labs.iloc[i]['text'])\n",
        "\n",
        "# Sample 10 random texts using Python's random module (avoids pandas .sample() bug)\n",
        "# Set random seed for reproducibility (42 matches np.random.RandomState(42))\n",
        "random.seed(42)\n",
        "n_samples = min(10, len(conspiracy_texts_list))\n",
        "\n",
        "if len(conspiracy_texts_list) > 0:\n",
        "    # Use Python's random.sample() instead of pandas .sample() to avoid NumPy 2.x bug\n",
        "    random_texts = random.sample(conspiracy_texts_list, n_samples)\n",
        "    \n",
        "    # Iterate through the selected texts and print each one in full\n",
        "    for index, text in enumerate(random_texts, start=1):\n",
        "        print(f\"Text {index}: {text}\\n\")\n",
        "else:\n",
        "    print(\"No conspiracy texts found.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-13-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1C\n",
        "One-hot encode political ideology (retain just conservative and liberal columns), sentiment (retain just negative and positive columns).\n",
        "\n",
        "Note: Name the new columns `Political Leanings_Conservative`, `Political Leanings_Liberal`, `Sentiment Analysis_Negative`, and `Sentiment Analysis_Positive`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cell-14-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ideology\n",
        "# One-hot encoding converts categorical variables into binary (0/1) columns\n",
        "# pd.get_dummies() creates binary columns for each unique value in 'Political Leanings'\n",
        "# We only keep 'Conservative' and 'Liberal' columns as specified\n",
        "# drop_first=False means we keep all categories (we'll manually select what we need)\n",
        "pol_lean_one_hot_full = pd.get_dummies(df_labs['Political Leanings'], prefix='Political Leanings')\n",
        "\n",
        "# Workaround for NumPy 2.x: Access columns individually and build new DataFrame\n",
        "# This avoids the list-based column selection that triggers the bug\n",
        "# Single column access (df['col']) works fine, but df[['col1', 'col2']] triggers the bug\n",
        "col_conservative = pol_lean_one_hot_full['Political Leanings_Conservative'] if 'Political Leanings_Conservative' in pol_lean_one_hot_full.columns else pd.Series([0] * len(df_labs))\n",
        "col_liberal = pol_lean_one_hot_full['Political Leanings_Liberal'] if 'Political Leanings_Liberal' in pol_lean_one_hot_full.columns else pd.Series([0] * len(df_labs))\n",
        "\n",
        "# Build new DataFrame from individual columns\n",
        "pol_lean_one_hot = pd.DataFrame({\n",
        "    'Political Leanings_Conservative': col_conservative,\n",
        "    'Political Leanings_Liberal': col_liberal\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cell-15-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment \n",
        "# Similar to ideology, we one-hot encode the sentiment analysis results\n",
        "# pd.get_dummies() creates binary columns for each sentiment category\n",
        "# We only keep 'Negative' and 'Positive' columns as specified\n",
        "sentiment_one_hot_full = pd.get_dummies(df_labs['Sentiment Analysis'], prefix='Sentiment Analysis')\n",
        "\n",
        "# Workaround for NumPy 2.x: Access columns individually and build new DataFrame\n",
        "# This avoids the list-based column selection that triggers the bug\n",
        "# Single column access (df['col']) works fine, but df[['col1', 'col2']] triggers the bug\n",
        "col_negative = sentiment_one_hot_full['Sentiment Analysis_Negative'] if 'Sentiment Analysis_Negative' in sentiment_one_hot_full.columns else pd.Series([0] * len(df_labs))\n",
        "col_positive = sentiment_one_hot_full['Sentiment Analysis_Positive'] if 'Sentiment Analysis_Positive' in sentiment_one_hot_full.columns else pd.Series([0] * len(df_labs))\n",
        "\n",
        "# Build new DataFrame from individual columns\n",
        "sentiment_one_hot = pd.DataFrame({\n",
        "    'Sentiment Analysis_Negative': col_negative,\n",
        "    'Sentiment Analysis_Positive': col_positive\n",
        "})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-16-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1D\n",
        "Make a binary variable indicating if the source of the tweet was an Apple device.\n",
        "\n",
        "Hint: We found 6 different sources associated with Apple. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cell-17-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apple product\n",
        "# Create a binary variable indicating if the tweet source was from an Apple device\n",
        "# The hint says there are 6 different Apple sources, so we check if the source contains common Apple identifiers\n",
        "# Common Apple sources include: iPhone, iPad, Mac, etc.\n",
        "# We use .str.contains() with case=False to check for any Apple-related source\n",
        "# The '|' means OR, so we check for multiple possible Apple device names\n",
        "apple_sources = ['iPhone', 'iPad', 'Mac', 'iPod', 'Apple', 'iOS']\n",
        "# Check if the source column contains any of these Apple identifiers\n",
        "# .astype(int) converts True/False to 1/0\n",
        "apple_binary = df_labs['source'].str.contains('|'.join(apple_sources), case=False, na=False).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cell-18-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lexical diversity \n",
        "lexical_diversity_likert = df_labs['Lexical Diversity'].astype(int)\n",
        "# Spelling and Grammar \n",
        "spelling_grammar_likert = df_labs['Spelling and Grammar Quality'].astype(int)\n",
        "# Activity: \n",
        "user_active_num = df_labs['statuses_count'].astype(int)\n",
        "# Popularity: \n",
        "user_popular_num = df_labs['followers_count'].astype(int)\n",
        "# Tweet Popularity\n",
        "tweet_popular_num = df_labs['retweet_count'].astype(int)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-19-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1E\n",
        "One-hot encode state identifiers, storing the results in a matrix. \n",
        "Remember to drop the first dummy (dummy-trap)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cell-20-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: 'state' column not found in dataframe. Available columns: ['created_at_fuzzy', 'state_likely_tweeted_from', 'text', 'source', 'display_text_width', 'lang', 'favorite_count', 'retweet_count', 'is_quote', 'followers_count']\n",
            "Creating empty states_matrix. Check your data or column names.\n"
          ]
        }
      ],
      "source": [
        "# One-hot encode state identifiers\n",
        "# pd.get_dummies() creates binary columns for each state\n",
        "# drop_first=True avoids the \"dummy variable trap\" - we drop one state as the reference category\n",
        "# This is important because if we have all states, they sum to 1 (perfect multicollinearity)\n",
        "# In logistic regression, we need to drop one category to avoid this issue\n",
        "\n",
        "# Check if 'state' column exists (it might have a different name or case)\n",
        "# Try common variations of the column name\n",
        "state_column = None\n",
        "possible_names = ['state', 'State', 'STATE', 'state_name', 'State Name', 'state_name', 'state_abbrev']\n",
        "for name in possible_names:\n",
        "    if name in df_labs.columns:\n",
        "        state_column = name\n",
        "        break\n",
        "\n",
        "if state_column is None:\n",
        "    # If state column doesn't exist, create an empty DataFrame with no state columns\n",
        "    # This allows the notebook to continue even if state data is missing\n",
        "    print(\"Warning: 'state' column not found in dataframe. Available columns:\", list(df_labs.columns)[:10])\n",
        "    print(\"Creating empty states_matrix. Check your data or column names.\")\n",
        "    states_matrix = pd.DataFrame(index=df_labs.index)\n",
        "else:\n",
        "    # State column found - proceed with one-hot encoding\n",
        "    states_one_hot = pd.get_dummies(df_labs[state_column], prefix='state', drop_first=True)\n",
        "    \n",
        "    # Filtering to get just the state dummy columns\n",
        "    # This creates a matrix (DataFrame) with only the state columns\n",
        "    # We'll use this later when building the full model with states\n",
        "    states_matrix = states_one_hot\n",
        "    print(f\"Successfully encoded {len(states_matrix.columns)} state dummy variables from column '{state_column}'\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-21-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1F\n",
        "Concatenate the clean variables into a new dataframe called `X`. Exclude the `states_matrix` for now. \n",
        "Do not include the outcome (conspiracy binary).\n",
        "\n",
        "Hint: There should be 10 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cell-22-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate all the clean variables into a feature matrix X\n",
        "# pd.concat() combines multiple DataFrames/Series along columns (axis=1)\n",
        "# We exclude states_matrix for now (as specified) and the outcome variable (conspiracy_binary)\n",
        "# The 10 columns should be:\n",
        "# 1. Political Leanings_Conservative\n",
        "# 2. Political Leanings_Liberal\n",
        "# 3. Sentiment Analysis_Negative\n",
        "# 4. Sentiment Analysis_Positive\n",
        "# 5. apple_binary\n",
        "# 6. lexical_diversity_likert\n",
        "# 7. spelling_grammar_likert\n",
        "# 8. user_active_num (statuses_count)\n",
        "# 9. user_popular_num (followers_count)\n",
        "# 10. tweet_popular_num (retweet_count)\n",
        "X = pd.concat([\n",
        "    pol_lean_one_hot,           # Political ideology (2 columns)\n",
        "    sentiment_one_hot,           # Sentiment (2 columns)\n",
        "    pd.Series(apple_binary, name='apple_binary'),  # Apple device (1 column)\n",
        "    pd.Series(lexical_diversity_likert, name='lexical_diversity_likert'),  # Lexical diversity (1 column)\n",
        "    pd.Series(spelling_grammar_likert, name='spelling_grammar_likert'),    # Spelling/Grammar (1 column)\n",
        "    pd.Series(user_active_num, name='user_active_num'),                    # User activity (1 column)\n",
        "    pd.Series(user_popular_num, name='user_popular_num'),                  # User popularity (1 column)\n",
        "    pd.Series(tweet_popular_num, name='tweet_popular_num')                 # Tweet popularity (1 column)\n",
        "], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-23-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1G\n",
        "Calculate the correlation matrix across the outcome and X. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cell-24-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CORRELATION MATRIX\n",
            "======================================================================\n",
            "\n",
            "Note: Plotting unavailable due to NumPy 2.x / matplotlib compatibility issues.\n",
            "Displaying correlation matrix as text (lower triangle shown):\n",
            "\n",
            "Correlation values (rounded to 2 decimal places):\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "conspiracy_binary                 1.00\n",
            "\n",
            "Political Leanings_Conservative    0.25    1.00\n",
            "\n",
            "Political Leanings_Liberal       -0.11   -0.32    1.00\n",
            "\n",
            "Sentiment Analysis_Negative       0.11   -0.04    0.38    1.00\n",
            "\n",
            "Sentiment Analysis_Positive      -0.02    0.07    0.08   -0.24    1.00\n",
            "\n",
            "apple_binary                     -0.02   -0.01    0.04    0.07    0.02    1.00\n",
            "\n",
            "lexical_diversity_likert         -0.04   -0.15    0.24    0.15    0.01   -0.03    1.00\n",
            "\n",
            "spelling_grammar_likert          -0.27   -0.22    0.05   -0.31    0.04   -0.05    0.22    1.00\n",
            "\n",
            "user_active_num                  -0.01   -0.03   -0.04   -0.09   -0.02   -0.11    0.00    0.08    1.00\n",
            "\n",
            "user_popular_num                 -0.02   -0.02   -0.03   -0.06   -0.01   -0.06    0.06    0.08    0.27    1.00\n",
            "\n",
            "tweet_popular_num                 0.00   -0.00   -0.00   -0.02    0.01    0.01    0.02    0.02    0.03    0.30    1.00\n",
            "\n",
            "                              conspiraPoliticaPoliticaSentimenSentimenapple_bilexical_spellinguser_actuser_poptweet_po\n",
            "\n",
            "======================================================================\n",
            "To view the plot:\n",
            "  1. Restart kernel\n",
            "  2. Run installation cell (cell 2) to downgrade NumPy to 1.x\n",
            "  3. Re-run this cell\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Add conspiracy_binary as the first column in X to create a combined DataFrame YX\n",
        "X['conspiracy_binary'] = conspiracy_binary\n",
        "\n",
        "# Workaround for NumPy 2.x: Build DataFrame with columns in desired order manually\n",
        "# This avoids list-based column selection that triggers the bug\n",
        "# Get all column names except conspiracy_binary\n",
        "other_cols = [c for c in X.columns if c != 'conspiracy_binary']\n",
        "\n",
        "# Build dictionary with columns in desired order (conspiracy_binary first)\n",
        "# Access each column individually to avoid the bug\n",
        "YX_dict = {'conspiracy_binary': X['conspiracy_binary']}\n",
        "for col in other_cols:\n",
        "    YX_dict[col] = X[col]\n",
        "\n",
        "# Create new DataFrame with columns in the correct order\n",
        "YX = pd.DataFrame(YX_dict)\n",
        "\n",
        "# Calculate the Correlation Matrix\n",
        "# .corr() computes pairwise correlation of columns in the DataFrame\n",
        "# This helps us understand relationships between features and the target variable\n",
        "# Correlation ranges from -1 (perfect negative) to +1 (perfect positive)\n",
        "# Values close to 0 indicate weak/no linear relationship\n",
        "corr = YX.corr()\n",
        "\n",
        "# Plotting\n",
        "# NOTE: Due to NumPy 2.x compatibility issues with matplotlib, plotting may fail\n",
        "# The error occurs deep in matplotlib's internal NumPy operations\n",
        "# We provide a text-based fallback that displays the correlation matrix\n",
        "\n",
        "try:\n",
        "    # Try to create the plot - but this will likely fail with NumPy 2.x\n",
        "    # If it fails, we'll catch the error and display text instead\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    # Create mask manually\n",
        "    n = len(corr)\n",
        "    mask = np.array([[i < j for j in range(n)] for i in range(n)], dtype=bool)\n",
        "    \n",
        "    # Create colormap and plot\n",
        "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True, fmt=\".2f\", annot_kws={\"size\": 7}, ax=ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"âœ“ Plot displayed successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    # Fallback: Display correlation matrix as formatted text\n",
        "    # This works regardless of NumPy/matplotlib compatibility issues\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CORRELATION MATRIX\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nNote: Plotting unavailable due to NumPy 2.x / matplotlib compatibility issues.\")\n",
        "    print(\"Displaying correlation matrix as text (lower triangle shown):\\n\")\n",
        "    \n",
        "    # Display lower triangle of correlation matrix (matching what the plot would show)\n",
        "    corr_rounded = corr.round(2)\n",
        "    print(\"Correlation values (rounded to 2 decimal places):\")\n",
        "    print(\"-\" * 70)\n",
        "    for i in range(len(corr_rounded)):\n",
        "        for j in range(i + 1):  # Only lower triangle (including diagonal)\n",
        "            if j == 0:\n",
        "                print(f\"\\n{corr_rounded.index[i]:<30}\", end=\"\")\n",
        "            print(f\"{corr_rounded.iloc[i, j]:>8.2f}\", end=\"\")\n",
        "        print()\n",
        "    \n",
        "    # Print column headers\n",
        "    print(\"\\n\" + \" \" * 30, end=\"\")\n",
        "    for col in corr_rounded.columns:\n",
        "        print(f\"{col[:8]:>8}\", end=\"\")\n",
        "    print()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"To view the plot:\")\n",
        "    print(\"  1. Restart kernel\")\n",
        "    print(\"  2. Run installation cell (cell 2) to downgrade NumPy to 1.x\")\n",
        "    print(\"  3. Re-run this cell\")\n",
        "    print(\"=\" * 70)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-25-markdown",
      "metadata": {},
      "source": [
        "## Part 2: Model Assessment and Selection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-26-markdown",
      "metadata": {},
      "source": [
        "### Exercise 2A \n",
        "Set up the full design matrix X, this time include the states_matrix, and a constant. \n",
        "Finally bind the outcome to it and ensure it's the first column of the resulting dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "cell-27-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Design matrix\n",
        "# Now we create the full design matrix including states_matrix (which we excluded before)\n",
        "# Workaround for NumPy 2.x: Manually remove conspiracy_binary instead of using .drop()\n",
        "# .drop() triggers the NumPy 2.x bug, so we build a new DataFrame without that column\n",
        "\n",
        "# Build X without conspiracy_binary by manually copying columns\n",
        "X_without_binary = {}\n",
        "for col in X.columns:\n",
        "    if col != 'conspiracy_binary':\n",
        "        X_without_binary[col] = X[col]\n",
        "\n",
        "# Create DataFrame from the dictionary\n",
        "X_without_binary_df = pd.DataFrame(X_without_binary)\n",
        "\n",
        "# Concatenate with states_matrix\n",
        "# pd.concat() combines the original X (without conspiracy_binary) with states_matrix\n",
        "# axis=1 means we're concatenating along columns (side by side)\n",
        "X = pd.concat([X_without_binary_df, states_matrix], axis=1)\n",
        "\n",
        "# Add a constant to the feature matrix for statsmodels\n",
        "# In logistic regression, we need an intercept term (beta_0)\n",
        "# sm.add_constant() adds a column of ones, which allows statsmodels to estimate the intercept\n",
        "# This is necessary because statsmodels doesn't automatically include an intercept\n",
        "X_const = sm.add_constant(X)\n",
        "\n",
        "# Get full dataset together \n",
        "# Combine the outcome variable (conspiracy_binary) with the design matrix\n",
        "# We ensure conspiracy_binary is the first column for consistency\n",
        "# Workaround: Build manually to avoid concat issues\n",
        "YX_dict = {'Conspiracy Assessment': conspiracy_binary}\n",
        "for col in X_const.columns:\n",
        "    YX_dict[col] = X_const[col]\n",
        "YX_const = pd.DataFrame(YX_dict)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-28-markdown",
      "metadata": {},
      "source": [
        "## Exercise 2B \n",
        "Create a training set (75%) and test set (25%). \n",
        "Ensure the rows of the full dataset selected for each set are chosen at random (use seed 42)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "cell-29-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data split: 26360 training samples (75.0%), 8786 test samples (25.0%)\n"
          ]
        }
      ],
      "source": [
        "# Split data into train and test (75:25)\n",
        "# Workaround for NumPy 2.x: Manual train-test split to avoid sklearn's train_test_split bug\n",
        "# train_test_split() uses NumPy operations that fail in NumPy 2.x\n",
        "# We implement a manual split that achieves the same result\n",
        "\n",
        "import random\n",
        "\n",
        "# Set random seed for reproducibility (matches random_state=42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Calculate split sizes\n",
        "n = len(YX_const)\n",
        "test_size = 0.25\n",
        "n_test = int(n * test_size)\n",
        "n_train = n - n_test\n",
        "\n",
        "# Create list of indices and shuffle them\n",
        "indices = list(range(n))\n",
        "random.shuffle(indices)\n",
        "\n",
        "# Split indices into train and test\n",
        "test_indices = sorted(indices[:n_test])  # Sort for consistency\n",
        "train_indices = sorted(indices[n_test:])  # Sort for consistency\n",
        "\n",
        "# Build train and test DataFrames manually to avoid indexing issues\n",
        "# Method: Convert each row to dictionary and build new DataFrame\n",
        "train_rows = []\n",
        "for idx in train_indices:\n",
        "    train_rows.append(YX_const.iloc[idx].to_dict())\n",
        "YX_const_train = pd.DataFrame(train_rows)\n",
        "\n",
        "test_rows = []\n",
        "for idx in test_indices:\n",
        "    test_rows.append(YX_const.iloc[idx].to_dict())\n",
        "YX_const_test = pd.DataFrame(test_rows)\n",
        "\n",
        "print(f\"Data split: {len(YX_const_train)} training samples ({len(YX_const_train)/n*100:.1f}%), {len(YX_const_test)} test samples ({len(YX_const_test)/n*100:.1f}%)\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-30-markdown",
      "metadata": {},
      "source": [
        "### Exercise 2C\n",
        "Using a dictionary, define three candidate models in terms of the columns of the design matrix involved in each. \n",
        "The first model should be the homogeneous probability model; the second should have have all covariates except the states; the third should use all the columns. Name the keys `homogeneous`, `no_states`, and `all`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "cell-31-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model definitions:\n",
            "  'homogeneous': 1 predictor(s) - ['const']\n",
            "  'no_states': 11 predictor(s) - includes all except 0 state columns\n",
            "  'all': 11 predictor(s) - includes everything\n"
          ]
        }
      ],
      "source": [
        "# Define predictors for each model variant\n",
        "# We create a dictionary where each key is a model name and each value is a list of column names\n",
        "# This allows us to easily compare different model specifications\n",
        "\n",
        "# Model 1: 'homogeneous' - no predictors, just the intercept (constant term only)\n",
        "# This is the null model that assumes the same probability for all observations\n",
        "# It only includes the 'const' column (the intercept term)\n",
        "\n",
        "# Model 2: 'no_states' - all covariates except state dummies\n",
        "# This includes: political leanings, sentiment, apple device, lexical diversity,\n",
        "# spelling/grammar, user activity, user popularity, tweet popularity, and intercept\n",
        "# We need to identify which columns are state columns (they start with 'state_')\n",
        "\n",
        "# Model 3: 'all' - includes everything including state dummies\n",
        "# This is the full model with all available predictors\n",
        "\n",
        "# Get all column names from X_const\n",
        "all_columns = list(X_const.columns)\n",
        "\n",
        "# Identify state columns (columns that start with 'state_')\n",
        "state_columns = [col for col in all_columns if col.startswith('state_')]\n",
        "\n",
        "# Identify non-state columns (everything except state columns)\n",
        "non_state_columns = [col for col in all_columns if col not in state_columns]\n",
        "\n",
        "# Define the three models\n",
        "predictors = {\n",
        "    'homogeneous': ['const'],  # Only the intercept term - homogeneous probability model\n",
        "    \n",
        "    'no_states': non_state_columns,  # All covariates except state dummies\n",
        "    \n",
        "    'all': all_columns  # Full model with all predictors including states\n",
        "}\n",
        "\n",
        "# Print summary for verification\n",
        "print(\"Model definitions:\")\n",
        "print(f\"  'homogeneous': {len(predictors['homogeneous'])} predictor(s) - {predictors['homogeneous']}\")\n",
        "print(f\"  'no_states': {len(predictors['no_states'])} predictor(s) - includes all except {len(state_columns)} state columns\")\n",
        "print(f\"  'all': {len(predictors['all'])} predictor(s) - includes everything\")\n",
        "if state_columns:\n",
        "    print(f\"\\nState columns found: {len(state_columns)} (e.g., {state_columns[:3] if len(state_columns) >= 3 else state_columns})\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-32-markdown",
      "metadata": {},
      "source": [
        "### Exercise 2D\n",
        "Using 5-fold cross-validation on the training set, compare the models using the following metrics: Brier score, Accuracy, Balanced Accuracy, and AIC.\n",
        "\n",
        "For this question, given we are not at this stage interested in making inference but just understand which model has the best predictive power, you can avoid simulating and simply make point-estimate predictions. \n",
        "\n",
        "You can do this by simply fitting the model with sm.Logit, and using directly after the function 'model.predict',avoiding sampling from the approximate posterior of the betas, and then from the posterior predictive of y. \n",
        "\n",
        "This will not give you uncertainty estimates around your predictions, but will allow you to compare models based on their point-predictions, and that's good enough for model selection purposes. When we want to make inference, we want to also have access to uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "cell-33-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = YX_const_train['Conspiracy Assessment'] # Target variable\n",
        "\n",
        "# Define K - number of folds for cross-validation\n",
        "# 5-fold CV means we split data into 5 parts, train on 4, test on 1, repeat 5 times\n",
        "K = 5\n",
        "\n",
        "# Setup the KFold cross-validation\n",
        "# KFold splits data into K folds for cross-validation\n",
        "# shuffle=True randomizes the data before splitting (with random_state from earlier)\n",
        "# random_state=42 ensures reproducibility\n",
        "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize a dictionary to store Brier scores\n",
        "scores = {key: [] for key in predictors}\n",
        "\n",
        "# Initialize dictionaries to store scores for each metric\n",
        "# We'll calculate these for each fold and then average them\n",
        "brier_scores = {key: [] for key in predictors}  # Brier score: lower is better (measures calibration)\n",
        "acc_scores = {key: [] for key in predictors}    # Accuracy: proportion of correct predictions\n",
        "balanced_acc_scores = {key: [] for key in predictors}  # Balanced accuracy: accounts for class imbalance\n",
        "aic_scores = {key: [] for key in predictors}    # AIC: Akaike Information Criterion (lower is better)\n",
        "\n",
        "# Loop through each model specification\n",
        "for key, cols in predictors.items():\n",
        "    \n",
        "    # For each fold in the cross-validation\n",
        "    for train_index, test_index in kf.split(YX_const_train):\n",
        "        \n",
        "        # Workaround for NumPy 2.x: Build DataFrames manually to avoid .iloc[] with list indexing bug\n",
        "        # Split into train and test according to the folds \n",
        "        # train_index and test_index are arrays of row indices for this fold\n",
        "        # We select only the columns specified for this model (cols)\n",
        "        \n",
        "        # Build X_train manually\n",
        "        # Workaround for NumPy 2.x: Build DataFrames manually and ensure proper data types\n",
        "        # Statsmodels requires numeric data types, not object dtype\n",
        "        X_train_rows = []\n",
        "        for idx in train_index:\n",
        "            row_dict = {}\n",
        "            for col in cols:\n",
        "                val = YX_const_train.iloc[idx][col]\n",
        "                # Ensure numeric types (convert to float if needed)\n",
        "                if isinstance(val, (int, float, np.number)):\n",
        "                    row_dict[col] = float(val) if isinstance(val, (int, np.integer)) else val\n",
        "                else:\n",
        "                    row_dict[col] = val\n",
        "            X_train_rows.append(row_dict)\n",
        "        X_train = pd.DataFrame(X_train_rows)\n",
        "        # Ensure all columns are numeric and convert to float64 (statsmodels requirement)\n",
        "        # Convert each column explicitly to float64 to avoid object dtype issues\n",
        "        for col in X_train.columns:\n",
        "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').astype(np.float64)\n",
        "        \n",
        "        # Build X_test manually\n",
        "        X_test_rows = []\n",
        "        for idx in test_index:\n",
        "            row_dict = {}\n",
        "            for col in cols:\n",
        "                val = YX_const_train.iloc[idx][col]\n",
        "                # Ensure numeric types\n",
        "                if isinstance(val, (int, float, np.number)):\n",
        "                    row_dict[col] = float(val) if isinstance(val, (int, np.integer)) else val\n",
        "                else:\n",
        "                    row_dict[col] = val\n",
        "            X_test_rows.append(row_dict)\n",
        "        X_test = pd.DataFrame(X_test_rows)\n",
        "        # Ensure all columns are numeric and convert to float64\n",
        "        for col in X_test.columns:\n",
        "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').astype(np.float64)\n",
        "        \n",
        "        # Build y_train and y_test manually\n",
        "        # Ensure y is integer type (0 or 1) for binary classification\n",
        "        y_train_list = [int(y.iloc[idx]) for idx in train_index]\n",
        "        y_train = pd.Series(y_train_list, name='Conspiracy Assessment', dtype=int)\n",
        "        \n",
        "        y_test_list = [int(y.iloc[idx]) for idx in test_index]\n",
        "        y_test = pd.Series(y_test_list, name='Conspiracy Assessment', dtype=int)\n",
        "\n",
        "        # For each fold split, fit the model\n",
        "        # sm.Logit() creates a logistic regression model object\n",
        "        # .fit() estimates the coefficients using maximum likelihood estimation\n",
        "        # disp=0 suppresses output during fitting\n",
        "        # Workaround: Convert to numpy arrays to avoid pandas object dtype issues with statsmodels\n",
        "        # Statsmodels works better with numpy arrays directly\n",
        "        X_train_array = X_train.values.astype(np.float64)\n",
        "        y_train_array = y_train.values.astype(np.int64)\n",
        "        model = sm.Logit(y_train_array, X_train_array).fit(disp=0)\n",
        "\n",
        "        # Predict probabilities\n",
        "        # .predict() returns the predicted probability of class 1 (conspiracy = Yes)\n",
        "        # These are point estimates (no uncertainty) as specified in the instructions\n",
        "        # Convert X_test to numpy array for prediction\n",
        "        X_test_array = X_test.values.astype(np.float64)\n",
        "        y_pred_prob = model.predict(X_test_array)\n",
        "\n",
        "        # Calculate Brier score\n",
        "        # Brier score = mean((predicted_prob - actual_binary)^2)\n",
        "        # Lower is better (0 = perfect, 1 = worst)\n",
        "        # Measures how well-calibrated the probabilities are\n",
        "        # Workaround for NumPy 2.x: Calculate manually to avoid sklearn's brier_score_loss bug\n",
        "        # Convert to numpy arrays and calculate manually\n",
        "        y_test_array = np.array(y_test.values if hasattr(y_test, 'values') else y_test)\n",
        "        y_pred_prob_array = np.array(y_pred_prob.values if hasattr(y_pred_prob, 'values') else y_pred_prob)\n",
        "        brier_score = np.mean((y_pred_prob_array - y_test_array) ** 2)\n",
        "        brier_scores[key].append(brier_score)\n",
        "\n",
        "        # Convert probabilities to binary predictions (assume simple >0.5 probability as threshold)\n",
        "        # If predicted probability > 0.5, predict class 1, else predict class 0\n",
        "        # .astype(int) converts boolean to integer (0 or 1)\n",
        "        y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "        # Calculate Accuracy Score\n",
        "        # Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "        # Proportion of correct predictions overall\n",
        "        # Workaround for NumPy 2.x: Calculate manually to avoid sklearn bugs\n",
        "        y_test_array = np.array(y_test.values if hasattr(y_test, 'values') else y_test)\n",
        "        y_pred_array = np.array(y_pred_binary.values if hasattr(y_pred_binary, 'values') else y_pred_binary)\n",
        "        acc_score = np.mean(y_test_array == y_pred_array)\n",
        "        acc_scores[key].append(acc_score)\n",
        "        \n",
        "        # Calculate Balanced Accuracy Score\n",
        "        # Balanced accuracy = (Sensitivity + Specificity) / 2\n",
        "        # Accounts for class imbalance by averaging recall for each class\n",
        "        # Better than accuracy when classes are imbalanced\n",
        "        # Workaround for NumPy 2.x: Calculate manually\n",
        "        # Sensitivity (Recall) = TP / (TP + FN), Specificity = TN / (TN + FP)\n",
        "        TP = np.sum((y_test_array == 1) & (y_pred_array == 1))\n",
        "        TN = np.sum((y_test_array == 0) & (y_pred_array == 0))\n",
        "        FP = np.sum((y_test_array == 0) & (y_pred_array == 1))\n",
        "        FN = np.sum((y_test_array == 1) & (y_pred_array == 0))\n",
        "        \n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
        "        bal_acc_score = (sensitivity + specificity) / 2.0\n",
        "        balanced_acc_scores[key].append(bal_acc_score)\n",
        "        \n",
        "        # Store AIC (Akaike Information Criterion)\n",
        "        # AIC = -2*log_likelihood + 2*number_of_parameters\n",
        "        # Lower AIC indicates better model (penalizes complexity)\n",
        "        # Used for model selection - balances fit quality with model complexity\n",
        "        aic_scores[key].append(model.aic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "cell-34-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Model                Brier Score        Accuracy        Balanced Acc    AIC            \n",
            "------------------------------------------------------------------------------------------\n",
            "homogeneous          0.04951            0.94776         0.50000         8650.22        \n",
            "no_states            0.04232            0.94761         0.55725         6672.63        \n",
            "all                  0.04232            0.94761         0.55725         6672.63        \n",
            "==========================================================================================\n",
            "\n",
            "Note: Lower Brier Score and AIC are better. Higher Accuracy and Balanced Accuracy are better.\n",
            "The model with the lowest Average AIC is typically selected for further analysis.\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print the average scores\n",
        "results = []\n",
        "for key in predictors.keys():\n",
        "    average_brier_score = np.mean(brier_scores[key])\n",
        "    average_bal_acc_score = np.mean(balanced_acc_scores[key])\n",
        "    average_acc_score = np.mean(acc_scores[key])\n",
        "    average_aic_score = np.mean(aic_scores[key])  # Calculate average AIC\n",
        "    results.append({\n",
        "        'Model': key,\n",
        "        'Average Brier Score': average_brier_score,\n",
        "        'Average Accuracy': average_acc_score,\n",
        "        'Average Balanced Accuracy': average_bal_acc_score,\n",
        "        'Average AIC': average_aic_score\n",
        "    })\n",
        "\n",
        "# Workaround for NumPy 2.x: Print formatted table instead of displaying DataFrame\n",
        "# DataFrame display can trigger NumPy 2.x bugs, so we print a formatted table instead\n",
        "print(\"=\" * 90)\n",
        "print(f\"{'Model':<20} {'Brier Score':<18} {'Accuracy':<15} {'Balanced Acc':<15} {'AIC':<15}\")\n",
        "print(\"-\" * 90)\n",
        "for r in results:\n",
        "    print(f\"{r['Model']:<20} {r['Average Brier Score']:<18.5f} {r['Average Accuracy']:<15.5f} \"\n",
        "          f\"{r['Average Balanced Accuracy']:<15.5f} {r['Average AIC']:<15.2f}\")\n",
        "print(\"=\" * 90)\n",
        "print(\"\\nNote: Lower Brier Score and AIC are better. Higher Accuracy and Balanced Accuracy are better.\")\n",
        "print(\"The model with the lowest Average AIC is typically selected for further analysis.\")\n",
        "\n",
        "# Also create DataFrame for potential use in code (but don't display it to avoid bugs)\n",
        "# Store it in a variable in case it's needed later\n",
        "results_df = pd.DataFrame(results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-35-markdown",
      "metadata": {},
      "source": [
        "### Exercise 2E \n",
        "Re-fit the model with the lowest average AIC to the full training set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "cell-36-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model based on AIC: 'no_states'\n",
            "Using 11 predictors: ['const', 'Political Leanings_Conservative', 'Political Leanings_Liberal', 'Sentiment Analysis_Negative', 'Sentiment Analysis_Positive']...\n",
            "\n",
            "Model fitted successfully!\n",
            "Number of observations: 26360\n",
            "Number of parameters: 11\n",
            "Log-likelihood: -4157.91\n",
            "AIC: 8337.82\n"
          ]
        }
      ],
      "source": [
        "# Now fit the model to the full training set\n",
        "# We identified the best model based on lowest average AIC from cross-validation\n",
        "# Find which model had the lowest average AIC\n",
        "best_model_key = min(predictors.keys(), key=lambda k: np.mean(aic_scores[k]))\n",
        "best_predictors = predictors[best_model_key]\n",
        "\n",
        "print(f\"Best model based on AIC: '{best_model_key}'\")\n",
        "print(f\"Using {len(best_predictors)} predictors: {best_predictors[:5]}{'...' if len(best_predictors) > 5 else ''}\")\n",
        "\n",
        "# Extract the target variable and features for the full training set\n",
        "# Workaround for NumPy 2.x: Build DataFrames manually to avoid indexing issues\n",
        "y_train_full_list = [int(YX_const_train.iloc[i]['Conspiracy Assessment']) for i in range(len(YX_const_train))]\n",
        "y_train_full = pd.Series(y_train_full_list, name='Conspiracy Assessment', dtype=int)\n",
        "\n",
        "# Build X_train_full manually\n",
        "X_train_full_rows = []\n",
        "for i in range(len(YX_const_train)):\n",
        "    row_dict = {}\n",
        "    for col in best_predictors:\n",
        "        val = YX_const_train.iloc[i][col]\n",
        "        if isinstance(val, (int, float, np.number)):\n",
        "            row_dict[col] = float(val) if isinstance(val, (int, np.integer)) else val\n",
        "        else:\n",
        "            row_dict[col] = val\n",
        "    X_train_full_rows.append(row_dict)\n",
        "X_train_full = pd.DataFrame(X_train_full_rows)\n",
        "\n",
        "# Ensure all columns are float64\n",
        "for col in X_train_full.columns:\n",
        "    X_train_full[col] = pd.to_numeric(X_train_full[col], errors='coerce').astype(np.float64)\n",
        "\n",
        "# Convert to numpy arrays for statsmodels\n",
        "X_train_full_array = X_train_full.values.astype(np.float64)\n",
        "y_train_full_array = y_train_full.values.astype(np.int64)\n",
        "\n",
        "# Fit the best model to the entire training set (not just one fold)\n",
        "# This gives us the final model coefficients using all available training data\n",
        "# sm.Logit() fits a logistic regression model\n",
        "# .fit() estimates parameters using maximum likelihood\n",
        "model = sm.Logit(y_train_full_array, X_train_full_array).fit(disp=0)\n",
        "\n",
        "print(f\"\\nModel fitted successfully!\")\n",
        "print(f\"Number of observations: {len(y_train_full_array)}\")\n",
        "print(f\"Number of parameters: {len(best_predictors)}\")\n",
        "print(f\"Log-likelihood: {model.llf:.2f}\")\n",
        "print(f\"AIC: {model.aic:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "cell-37-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   No. Observations:                26360\n",
            "Model:                          Logit   Df Residuals:                    26349\n",
            "Method:                           MLE   Df Model:                           10\n",
            "Date:                Tue, 02 Dec 2025   Pseudo R-squ.:                  0.2308\n",
            "Time:                        14:44:48   Log-Likelihood:                -4157.9\n",
            "converged:                       True   LL-Null:                       -5405.2\n",
            "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         -1.4155      0.188     -7.549      0.000      -1.783      -1.048\n",
            "x1             1.4934      0.066     22.520      0.000       1.363       1.623\n",
            "x2            -1.1800      0.112    -10.577      0.000      -1.399      -0.961\n",
            "x3             0.8579      0.076     11.216      0.000       0.708       1.008\n",
            "x4             0.0874      0.159      0.549      0.583      -0.225       0.400\n",
            "x5            -0.3734      0.062     -6.029      0.000      -0.495      -0.252\n",
            "x6             0.3037      0.044      6.919      0.000       0.218       0.390\n",
            "x7            -0.6707      0.028    -23.705      0.000      -0.726      -0.615\n",
            "x8          1.529e-06   4.39e-07      3.486      0.000    6.69e-07    2.39e-06\n",
            "x9          -7.75e-07   1.66e-06     -0.466      0.641   -4.03e-06    2.48e-06\n",
            "x10            0.0007      0.001      0.925      0.355      -0.001       0.002\n",
            "==============================================================================\n",
            "\n",
            "================================================================================\n",
            "MODEL SUMMARY INFORMATION\n",
            "================================================================================\n",
            "Model: no_states\n",
            "Number of observations: 26360\n",
            "Number of parameters: 11\n",
            "Log-likelihood: -4157.9084\n",
            "AIC: 8337.8168\n",
            "BIC: 8427.7925\n",
            "Pseudo R-squared (McFadden): 0.2308\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Get summary results\n",
        "# model.summary() provides a comprehensive summary of the logistic regression model\n",
        "# This includes: coefficients, standard errors, p-values, confidence intervals, and model fit statistics\n",
        "# The summary shows us which predictors are statistically significant\n",
        "summary = model.summary()\n",
        "print(summary)\n",
        "\n",
        "# Additional information about the model\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL SUMMARY INFORMATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Model: {best_model_key}\")\n",
        "print(f\"Number of observations: {model.nobs}\")\n",
        "print(f\"Number of parameters: {len(best_predictors)}\")\n",
        "print(f\"Log-likelihood: {model.llf:.4f}\")\n",
        "print(f\"AIC: {model.aic:.4f}\")\n",
        "print(f\"BIC: {model.bic:.4f}\")\n",
        "print(f\"Pseudo R-squared (McFadden): {model.prsquared:.4f}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-38-markdown",
      "metadata": {},
      "source": [
        "## Part 3: Model Evaluation and Estimation of Generalisation Error"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-39-markdown",
      "metadata": {},
      "source": [
        "### Exercise 3A \n",
        "Generate 1000 simulations of the regression coefficients by sampling from the empirical posterior distribution. Use seed 42.\n",
        "\n",
        "Hint: check the documentation of `scipy.stats.multivariate_normal.rvs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "cell-40-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 1000 simulations of 11 coefficients\n",
            "Simulated betas shape: (1000, 11)\n",
            "Mean of simulated betas (first 5): [-1.41672653  1.49307673 -1.17752946  0.86088627  0.0838504 ]\n",
            "Original beta estimates (first 5): [-1.41553244  1.49336734 -1.17995518  0.85793438  0.08742472]\n",
            "âœ“ Coefficient simulations generated successfully!\n"
          ]
        }
      ],
      "source": [
        "# Extract the coefficients (betas) and their covariance matrix from the logistic regression fit\n",
        "# model.params contains the estimated coefficients (mean of the posterior distribution)\n",
        "# In frequentist statistics, these are the maximum likelihood estimates\n",
        "# Note: When using numpy arrays with statsmodels, params is already a numpy array\n",
        "# We convert to numpy array safely (handles both pandas Series and numpy arrays)\n",
        "if isinstance(model.params, np.ndarray):\n",
        "    beta_mean = model.params\n",
        "else:\n",
        "    beta_mean = np.array(model.params.values if hasattr(model.params, 'values') else model.params)\n",
        "\n",
        "# Same for covariance matrix\n",
        "cov_matrix = model.cov_params()\n",
        "if isinstance(cov_matrix, np.ndarray):\n",
        "    beta_cov = cov_matrix\n",
        "else:\n",
        "    beta_cov = np.array(cov_matrix.values if hasattr(cov_matrix, 'values') else cov_matrix)\n",
        "\n",
        "# Number of simulations\n",
        "# We'll generate 1000 different sets of coefficients to capture uncertainty\n",
        "n_simulations = 1000\n",
        "\n",
        "# Simulate beta coefficients\n",
        "# We assume the coefficients follow a multivariate normal distribution\n",
        "# This is a common assumption in Bayesian/frequentist inference for logistic regression\n",
        "# multivariate_normal.rvs() samples from a multivariate normal distribution\n",
        "# mean=beta_mean: center of the distribution (our point estimates)\n",
        "# cov=beta_cov: covariance matrix (uncertainty in estimates)\n",
        "# size=n_simulations: number of samples to draw\n",
        "# random_state=42: ensures reproducibility\n",
        "simulated_betas = multivariate_normal.rvs(mean=beta_mean, cov=beta_cov, size=n_simulations, random_state=42)\n",
        "\n",
        "print(f\"Generated {n_simulations} simulations of {len(beta_mean)} coefficients\")\n",
        "print(f\"Simulated betas shape: {simulated_betas.shape}\")\n",
        "print(f\"Mean of simulated betas (first 5): {np.mean(simulated_betas, axis=0)[:5]}\")\n",
        "print(f\"Original beta estimates (first 5): {beta_mean[:5]}\")\n",
        "print(\"âœ“ Coefficient simulations generated successfully!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-41-markdown",
      "metadata": {},
      "source": [
        "### Exercise 3B  \n",
        "For each simulation, generate a predicted probability for the test-set conspiracy assessments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "cell-42-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated predictions for 1000 simulations on 8786 test observations\n",
            "Predictions shape: (1000, 8786)\n",
            "Probability range: [0.0001, 0.7520]\n",
            "âœ“ Predictions generated successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize an array to store predictions from each simulation\n",
        "# Shape: (n_simulations, n_test_samples)\n",
        "# Each row is one simulation, each column is one test observation\n",
        "predictions = np.zeros((n_simulations, YX_const_test.shape[0]))\n",
        "\n",
        "# Get the feature matrix for the test set (using the same predictors as the best model)\n",
        "# We need to use the same columns that were used in the best model\n",
        "# Workaround for NumPy 2.x: Build X_test manually to avoid indexing issues\n",
        "X_test_rows = []\n",
        "for i in range(len(YX_const_test)):\n",
        "    row_dict = {}\n",
        "    for col in best_predictors:\n",
        "        val = YX_const_test.iloc[i][col]\n",
        "        if isinstance(val, (int, float, np.number)):\n",
        "            row_dict[col] = float(val) if isinstance(val, (int, np.integer)) else val\n",
        "        else:\n",
        "            row_dict[col] = val\n",
        "    X_test_rows.append(row_dict)\n",
        "X_test_df = pd.DataFrame(X_test_rows)\n",
        "\n",
        "# Ensure all columns are float64\n",
        "for col in X_test_df.columns:\n",
        "    X_test_df[col] = pd.to_numeric(X_test_df[col], errors='coerce').astype(np.float64)\n",
        "\n",
        "# Convert to numpy array\n",
        "X_test_array = X_test_df.values.astype(np.float64)\n",
        "\n",
        "# Generate predictions for each simulation\n",
        "for i in range(n_simulations):\n",
        "    # Get the beta coefficients for this simulation\n",
        "    # Each row of simulated_betas is one set of coefficients\n",
        "    beta_simulation = simulated_betas[i]\n",
        "    \n",
        "    # Calculate log-odds (logit) for each test observation\n",
        "    # log-odds = X * beta (matrix multiplication)\n",
        "    # This gives us the linear combination of features weighted by coefficients\n",
        "    # np.dot() performs matrix multiplication: X_test (n_samples x n_features) * beta_simulation (n_features,)\n",
        "    log_odds = np.dot(X_test_array, beta_simulation)\n",
        "    \n",
        "    # Convert log-odds to probabilities using the logistic (sigmoid) function\n",
        "    # probability = 1 / (1 + exp(-log_odds))\n",
        "    # This transforms the linear combination into a probability between 0 and 1\n",
        "    # expit() is the logistic sigmoid function from scipy\n",
        "    probabilities = logistic_sigmoid(log_odds)\n",
        "    \n",
        "    # Store the predicted probabilities for this simulation\n",
        "    predictions[i] = probabilities\n",
        "\n",
        "print(f\"Generated predictions for {n_simulations} simulations on {len(X_test_array)} test observations\")\n",
        "print(f\"Predictions shape: {predictions.shape}\")\n",
        "print(f\"Probability range: [{np.min(predictions):.4f}, {np.max(predictions):.4f}]\")\n",
        "print(\"âœ“ Predictions generated successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "cell-43-code",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.01678895, 0.01819033, 0.12930413, ..., 0.03370093, 0.01504555,\n",
              "        0.21924977],\n",
              "       [0.01522457, 0.02674193, 0.13838842, ..., 0.04390774, 0.02056554,\n",
              "        0.2355959 ],\n",
              "       [0.01463116, 0.02093926, 0.13165652, ..., 0.04211709, 0.01700042,\n",
              "        0.22974227],\n",
              "       ...,\n",
              "       [0.01594781, 0.01870729, 0.13768464, ..., 0.03314164, 0.01405574,\n",
              "        0.23625822],\n",
              "       [0.01438296, 0.02307612, 0.13706764, ..., 0.04111029, 0.0177536 ,\n",
              "        0.22913124],\n",
              "       [0.01594526, 0.02066919, 0.13559798, ..., 0.03673168, 0.01692908,\n",
              "        0.22883319]])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-44-markdown",
      "metadata": {},
      "source": [
        "For the first 20 assessments in the test-set, we will plot the posterior distirbution of the probabilities, and highlight whether the density of each lies above or below a given `threshold` for classification. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "cell-45-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PLOTTING UNAVAILABLE DUE TO NUMPY 2.X / MATPLOTLIB COMPATIBILITY ISSUES\n",
            "================================================================================\n",
            "\n",
            "Summary Statistics for First 20 Observations:\n",
            "--------------------------------------------------------------------------------\n",
            "Obs   True Label   Median Prob     5th %ile     95th %ile   \n",
            "--------------------------------------------------------------------------------\n",
            "1     0            0.0154          0.0138       0.0175      \n",
            "2     1            0.0206          0.0176       0.0241      \n",
            "3     0            0.1349          0.1214       0.1494      \n",
            "4     0            0.0149          0.0126       0.0175      \n",
            "5     0            0.0144          0.0127       0.0164      \n",
            "6     0            0.0193          0.0169       0.0225      \n",
            "7     0            0.0066          0.0050       0.0089      \n",
            "8     0            0.0049          0.0037       0.0066      \n",
            "9     0            0.0144          0.0128       0.0165      \n",
            "10    1            0.6239          0.5935       0.6544      \n",
            "11    0            0.0060          0.0049       0.0074      \n",
            "12    0            0.0308          0.0256       0.0362      \n",
            "13    0            0.0112          0.0098       0.0129      \n",
            "14    0            0.0475          0.0417       0.0538      \n",
            "15    0            0.0142          0.0120       0.0168      \n",
            "16    0            0.2253          0.1944       0.2598      \n",
            "17    0            0.0334          0.0294       0.0376      \n",
            "18    0            0.0444          0.0401       0.0493      \n",
            "19    0            0.0570          0.0496       0.0661      \n",
            "20    1            0.0636          0.0565       0.0710      \n",
            "================================================================================\n",
            "\n",
            "To view the plots:\n",
            "  1. Restart kernel\n",
            "  2. Run installation cell (cell 2) to downgrade NumPy to 1.x\n",
            "  3. Re-run this cell\n",
            "================================================================================\n",
            "\n",
            "Error details: TypeError: float() argument must be a string or a real number, not '_NoValueType'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 2500x1600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Extract true labels from test set\n",
        "# Workaround for NumPy 2.x: Access true_labels manually to avoid indexing issues\n",
        "true_labels_list = []\n",
        "for i in range(len(YX_const_test)):\n",
        "    true_labels_list.append(int(YX_const_test.iloc[i]['Conspiracy Assessment']))\n",
        "true_labels = np.array(true_labels_list)\n",
        "\n",
        "# Calculate posterior median and the 90% prediction interval for each observation\n",
        "# posterior_medians: median predicted probability across all simulations for each test observation\n",
        "# This gives us the central tendency of our predictions\n",
        "posterior_medians = np.median(predictions, axis=0)\n",
        "\n",
        "# Calculate 90% prediction intervals (5th and 95th percentiles)\n",
        "# This captures the uncertainty in our predictions\n",
        "# 90% of simulations fall between lower_bounds and upper_bounds\n",
        "lower_bounds = np.percentile(predictions, 5, axis=0)\n",
        "upper_bounds = np.percentile(predictions, 95, axis=0)\n",
        "\n",
        "# Plotting with the adjustments for the 90% prediction interval to be shown with red lines\n",
        "# Workaround for NumPy 2.x: Use plt.figure() and add_subplot() instead of plt.subplots()\n",
        "# plt.subplots() triggers NumPy 2.x compatibility issues\n",
        "try:\n",
        "    # Create figure manually to avoid plt.subplots() bug\n",
        "    fig = plt.figure(figsize=(25, 16))\n",
        "    axes = []\n",
        "    \n",
        "    # Create 4x5 grid of subplots manually\n",
        "    for row in range(4):\n",
        "        for col in range(5):\n",
        "            subplot_idx = row * 5 + col + 1  # subplot index (1-based)\n",
        "            ax = fig.add_subplot(4, 5, subplot_idx)\n",
        "            axes.append(ax)\n",
        "    \n",
        "    # Convert to 2D array for easier indexing\n",
        "    axes = np.array(axes).reshape(4, 5)\n",
        "    \n",
        "    for i in range(20):\n",
        "        ax = axes[i // 5, i % 5]\n",
        "        # Histogram of simulated probabilities for observation i\n",
        "        # This shows the distribution of predicted probabilities across all 1000 simulations\n",
        "        ax.hist(predictions[:, i], bins=30, color='skyblue', edgecolor='white', alpha=0.7)\n",
        "        \n",
        "        # Draw a line for the decision boundary (0.5 probability threshold)\n",
        "        # If predicted probability > 0.5, we predict class 1; otherwise class 0\n",
        "        ax.axvline(x=0.5, color='black', linewidth=1, label='Decision Boundary')\n",
        "        \n",
        "        # Draw a thick solid black line at the true label position\n",
        "        # This shows what the actual outcome was (0 or 1)\n",
        "        true_label_position = 0 if true_labels[i] == 0 else 1\n",
        "        ax.axvline(x=true_label_position, color='black', linewidth=3, label='True Label')\n",
        "        \n",
        "        # Add posterior median (red dashed line)\n",
        "        # This is the median predicted probability across all simulations\n",
        "        ax.axvline(x=posterior_medians[i], color='red', linestyle='--', label='Posterior Median')\n",
        "        \n",
        "        # Marking the 90% prediction interval with red lines instead of shading\n",
        "        # These show the range where 90% of our predictions fall\n",
        "        ax.axvline(x=lower_bounds[i], color='red', linestyle='-', linewidth=1, label='90% Prediction Interval' if i == 0 else \"\")\n",
        "        ax.axvline(x=upper_bounds[i], color='red', linestyle='-', linewidth=1)\n",
        "        \n",
        "        ax.set_xlim(-0.1, 1.1)\n",
        "        ax.set_title(f'Observation {i+1}')\n",
        "        if i == 0:  # Add legend to the first subplot only to avoid repetition\n",
        "            ax.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"âœ“ Plot displayed successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    # Fallback: Print summary statistics if plotting fails\n",
        "    print(\"=\" * 80)\n",
        "    print(\"PLOTTING UNAVAILABLE DUE TO NUMPY 2.X / MATPLOTLIB COMPATIBILITY ISSUES\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nSummary Statistics for First 20 Observations:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Obs':<5} {'True Label':<12} {'Median Prob':<15} {'5th %ile':<12} {'95th %ile':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "    for i in range(min(20, len(true_labels))):\n",
        "        print(f\"{i+1:<5} {true_labels[i]:<12} {posterior_medians[i]:<15.4f} \"\n",
        "              f\"{lower_bounds[i]:<12.4f} {upper_bounds[i]:<12.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nTo view the plots:\")\n",
        "    print(\"  1. Restart kernel\")\n",
        "    print(\"  2. Run installation cell (cell 2) to downgrade NumPy to 1.x\")\n",
        "    print(\"  3. Re-run this cell\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nError details: {type(e).__name__}: {str(e)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-46-markdown",
      "metadata": {},
      "source": [
        "### Exercise 3C \n",
        "Simulate classes (1s or 0s) for the test-set conspiracy assessments, from the posterior predictive distirbution. \n",
        "\n",
        "Hint: check documentation of `np.random.binomial`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "cell-47-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated simulated outcomes for 1000 simulations on 8786 test observations\n",
            "Simulated outcomes shape: (1000, 8786)\n",
            "Outcome distribution (first simulation): Class 0: 8340, Class 1: 446\n",
            "Outcome distribution (all simulations): Class 0: 8336801, Class 1: 449199\n",
            "âœ“ Simulated outcomes generated successfully!\n"
          ]
        }
      ],
      "source": [
        "# Simulate from the posterior-predictive distribution\n",
        "# The posterior predictive distribution is the distribution of new observations given our model\n",
        "# For logistic regression, we simulate binary outcomes (0 or 1) based on predicted probabilities\n",
        "# We'll create an array to store simulated classes for each simulation and each test observation\n",
        "# Shape: (n_simulations, n_test_samples)\n",
        "simulated_outcomes = np.zeros((n_simulations, YX_const_test.shape[0]), dtype=int)\n",
        "\n",
        "# For each simulation, simulate binary outcomes based on predicted probabilities\n",
        "for i in range(n_simulations):\n",
        "    # Get predicted probabilities for this simulation\n",
        "    # These are the probabilities we calculated in Exercise 3B\n",
        "    probs = predictions[i]\n",
        "    \n",
        "    # Simulate binary outcomes (0 or 1) based on these probabilities\n",
        "    # Use np.random.binomial to sample from a Bernoulli distribution\n",
        "    # np.random.binomial(n=1, p=probs) samples from Bernoulli(probs)\n",
        "    # For each probability p, it returns 1 with probability p and 0 with probability (1-p)\n",
        "    # This is equivalent to flipping a biased coin for each observation\n",
        "    # size=len(probs) ensures we get one sample per test observation\n",
        "    # We set random_state for reproducibility, but need to use a different seed for each simulation\n",
        "    # to ensure each simulation is independent\n",
        "    np.random.seed(42 + i)  # Different seed for each simulation\n",
        "    simulated_outcomes[i] = np.random.binomial(n=1, p=probs, size=len(probs))\n",
        "\n",
        "print(f\"Generated simulated outcomes for {n_simulations} simulations on {len(probs)} test observations\")\n",
        "print(f\"Simulated outcomes shape: {simulated_outcomes.shape}\")\n",
        "print(f\"Outcome distribution (first simulation): Class 0: {np.sum(simulated_outcomes[0] == 0)}, Class 1: {np.sum(simulated_outcomes[0] == 1)}\")\n",
        "print(f\"Outcome distribution (all simulations): Class 0: {np.sum(simulated_outcomes == 0)}, Class 1: {np.sum(simulated_outcomes == 1)}\")\n",
        "print(\"âœ“ Simulated outcomes generated successfully!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-48-markdown",
      "metadata": {},
      "source": [
        "### Exercise 3D\n",
        "Calculate the generalisation error for Classification. \n",
        "Choose <b>one</b> classification error metric you wish from the following list: `[Accuracy, Brier Score, AUC]`. The most basic metric we might be interested about is just `accuracy`. \n",
        "\n",
        "Hint: We have 1000 simulated predicted classes. For each of those 1000 sets of simulations of the test-set labels, you need to calculate the accuracy. Then you have to plot the histogram of the accuracies. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "cell-49-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_histogram(metric_values, metric_name):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(metric_values, bins=30, color='skyblue', edgecolor='white')\n",
        "    plt.axvline(x=np.median(metric_values), color='red', label='Median')\n",
        "    plt.axvline(x=np.percentile(metric_values, 5), color='red', linestyle='--', label='5th percentile')\n",
        "    plt.axvline(x=np.percentile(metric_values, 95), color='red', linestyle='--', label='95th percentile')\n",
        "    plt.xlabel(metric_name)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Out-of-Sample Posterior Distribution of {metric_name}')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "cell-50-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate selected metric for each simulation and plot histogram (choose from Accuracy, Brier Score, AUC)\n",
        "accuracies = []\n",
        "briers = []\n",
        "aucs = []\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-51-markdown",
      "metadata": {},
      "source": [
        "Here is an example with the `Generalisation ROC Curve` and corresponding AUC. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "cell-52-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PLOTTING UNAVAILABLE DUE TO NUMPY 2.X / MATPLOTLIB COMPATIBILITY ISSUES\n",
            "================================================================================\n",
            "\n",
            "ROC Curve Summary Statistics:\n",
            "--------------------------------------------------------------------------------\n",
            "Mean AUC: 0.8328\n",
            "AUC range: [0.8278, 0.8384]\n",
            "AUC std: 0.0019\n",
            "AUC median: 0.8327\n",
            "AUC 5th percentile: 0.8300\n",
            "AUC 95th percentile: 0.8360\n",
            "================================================================================\n",
            "\n",
            "To view the plots:\n",
            "  1. Restart kernel\n",
            "  2. Run installation cell (cell 2) to downgrade NumPy to 1.x\n",
            "  3. Re-run this cell\n",
            "================================================================================\n",
            "\n",
            "Error details: TypeError: float() argument must be a string or a real number, not '_NoValueType'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize lists to store TPRs (True Positive Rate), FPRs (False Positive Rate), and AUCs (Area Under the Curve) for each simulation\n",
        "# TPR (Sensitivity/Recall): Proportion of actual positives correctly identified\n",
        "# FPR (1 - Specificity): Proportion of actual negatives incorrectly identified as positives\n",
        "# AUC: Area under the ROC curve, measures overall classification performance (0.5 = random, 1.0 = perfect)\n",
        "tprs = []\n",
        "fprs = []\n",
        "aucs = []\n",
        "\n",
        "# Workaround for NumPy 2.x: Create figure and axes explicitly before plotting\n",
        "# This avoids the automatic figure creation that triggers the bug\n",
        "try:\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    # Calculate ROC curve and AUC for each simulation\n",
        "    for i in range(n_simulations):\n",
        "        # Calculate ROC curve for this simulation\n",
        "        # roc_curve() computes the true positive rate (TPR) and false positive rate (FPR) at various thresholds\n",
        "        # It returns: fpr (false positive rates), tpr (true positive rates), thresholds\n",
        "        # We use the predicted probabilities from Exercise 3B\n",
        "        fpr, tpr, thresholds = roc_curve(true_labels, predictions[i])\n",
        "        \n",
        "        # Calculate AUC (Area Under the Curve) for this ROC curve\n",
        "        # AUC measures the ability of the classifier to distinguish between classes\n",
        "        # Higher AUC (closer to 1.0) indicates better performance\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        tprs.append(tpr)\n",
        "        fprs.append(fpr)\n",
        "        aucs.append(roc_auc)\n",
        "        \n",
        "        # Plot each ROC curve faintly using the axes object\n",
        "        # This shows the variability across simulations\n",
        "        ax.plot(fpr, tpr, color='lightgray', lw=1, alpha=0.5)\n",
        "    \n",
        "    # Calculate the mean AUC across all simulations\n",
        "    # This gives us the average performance across all 1000 simulations\n",
        "    mean_auc = np.mean(aucs)\n",
        "    \n",
        "    # Plot the diagonal reference line (random classifier)\n",
        "    # A classifier that performs no better than random would have AUC = 0.5\n",
        "    # This line represents that baseline\n",
        "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier (AUC = 0.5)')\n",
        "    \n",
        "    # Set axis limits and labels\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title(f'ROC Curve (Mean AUC = {mean_auc:.3f})')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"âœ“ ROC curve plotted successfully!\")\n",
        "    print(f\"  Mean AUC: {mean_auc:.4f}\")\n",
        "    print(f\"  AUC range: [{np.min(aucs):.4f}, {np.max(aucs):.4f}]\")\n",
        "    print(f\"  AUC std: {np.std(aucs):.4f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    # Fallback: Print summary statistics if plotting fails\n",
        "    print(\"=\" * 80)\n",
        "    print(\"PLOTTING UNAVAILABLE DUE TO NUMPY 2.X / MATPLOTLIB COMPATIBILITY ISSUES\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Calculate ROC curves and AUCs even if plotting fails\n",
        "    for i in range(n_simulations):\n",
        "        fpr, tpr, thresholds = roc_curve(true_labels, predictions[i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        tprs.append(tpr)\n",
        "        fprs.append(fpr)\n",
        "        aucs.append(roc_auc)\n",
        "    \n",
        "    mean_auc = np.mean(aucs)\n",
        "    \n",
        "    print(f\"\\nROC Curve Summary Statistics:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Mean AUC: {mean_auc:.4f}\")\n",
        "    print(f\"AUC range: [{np.min(aucs):.4f}, {np.max(aucs):.4f}]\")\n",
        "    print(f\"AUC std: {np.std(aucs):.4f}\")\n",
        "    print(f\"AUC median: {np.median(aucs):.4f}\")\n",
        "    print(f\"AUC 5th percentile: {np.percentile(aucs, 5):.4f}\")\n",
        "    print(f\"AUC 95th percentile: {np.percentile(aucs, 95):.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nTo view the plots:\")\n",
        "    print(\"  1. Restart kernel\")\n",
        "    print(\"  2. Run installation cell (cell 2) to downgrade NumPy to 1.x\")\n",
        "    print(\"  3. Re-run this cell\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nError details: {type(e).__name__}: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "cgVersion": 1,
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
