{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-0-markdown",
      "metadata": {},
      "source": [
        "# Semester 3 Coding Portfolio Topic 4 Formative Part 2/2:\n",
        "## Evaluating Logistic Regression Predictions\n",
        "\n",
        "This notebook covers the following topics:\n",
        " - logistic regression\n",
        "\n",
        "This notebook is expected to take around 5 hours to complete.\n",
        "\n",
        "<b>Formative section</b><br>\n",
        "Simply complete the given functions such that they pass the automated tests. This part is graded Pass/Fail; you must get 100% correct!\n",
        "You can submit your notebook through Canvas as often as you like. Make sure to start doing so early to ensure that your code passes all tests!\n",
        "You may ask for help from fellow students and TAs on this section, and solutions might be provided later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell-1-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Necessary Libraries\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.discrete.discrete_model import BinaryResultsWrapper\n",
        "import sklearn\n",
        "import scipy\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.special import expit as logistic_sigmoid\n",
        "from packaging import version\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import brier_score_loss\n",
        "from sklearn.metrics import balanced_accuracy_score, brier_score_loss, accuracy_score, roc_curve, auc\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6bb6f50d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing/upgrading packages to required versions...\n",
            "============================================================\n",
            "✓ numpy 2.3.5 (meets requirement >= 1.26.0)\n",
            "✓ pandas 2.3.3 (meets requirement >= 2.2.0)\n",
            "✓ matplotlib 3.10.7 (meets requirement >= 3.8.0)\n",
            "✓ seaborn 0.13.2 (meets requirement >= 0.13)\n",
            "✓ statsmodels 0.14.5 (meets requirement >= 0.14)\n",
            "✓ scikit-learn 1.7.2 (meets requirement >= 1.5.0)\n",
            "✓ scipy 1.16.3 (meets requirement >= 1.11.0)\n",
            "✓ packaging 25.0 (meets requirement >= 0.0)\n",
            "============================================================\n",
            "\n",
            "Forcefully reloading modules to use newly installed versions...\n",
            "============================================================\n",
            "✓ Deleted 1566 cached module entries\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mn/8k0ys1k939d8111jymh_cnh80000gn/T/ipykernel_20199/868312460.py:166: UserWarning: The NumPy module was reloaded (imported a second time). This can in some cases result in small but subtle issues and is discouraged.\n",
            "  import numpy as np\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Re-imported all modules with new versions\n",
            "\n",
            "Verifying installed versions...\n",
            "  NumPy: 1.26.4\n",
            "  Pandas: 2.3.3\n",
            "  Matplotlib: 3.10.7\n",
            "  Seaborn: 0.13.2\n",
            "  Statsmodels: 0.14.5\n",
            "  Scikit-learn: 1.7.2\n",
            "  SciPy: 1.16.3\n",
            "\n",
            "============================================================\n",
            "All packages installed and modules reloaded!\n",
            "Version check in the next cell should now pass.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Install all necessary packages to avoid kernel restart\n",
        "# This ensures all dependencies are available before running the notebook\n",
        "# IMPORTANT: This cell must be run AFTER cell 1 (imports) to reload modules with new versions\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# List of packages to install/upgrade with minimum versions\n",
        "# Format: (package_name, min_version, is_required)\n",
        "# is_required: True = critical, False = can skip if installation fails\n",
        "# Note: NumPy 2.3.4 doesn't exist - latest is 2.0.2, but it has compatibility issues\n",
        "# We'll try NumPy 2.x first, but fall back to 1.x if needed for compatibility\n",
        "packages = [\n",
        "    ('numpy', '1.26.0', True),  # Using 1.x for better compatibility (2.x has breaking changes)\n",
        "    ('pandas', '2.2.0', True),  # Critical - ensure compatible with NumPy\n",
        "    ('matplotlib', '3.8.0', True),  # Critical for plotting\n",
        "    ('seaborn', '0.13', False),  # Nice to have for better plots, but not critical\n",
        "    ('statsmodels', '0.14', True),  # Critical for logistic regression\n",
        "    ('scikit-learn', '1.5.0', True),  # Critical for metrics and cross-validation\n",
        "    ('scipy', '1.11.0', True),  # Critical for statistical functions\n",
        "    ('packaging', '0.0', False)  # Only needed for version checking, not critical\n",
        "]\n",
        "\n",
        "print(\"Installing/upgrading packages to required versions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install packages using pip with --upgrade to ensure correct versions\n",
        "installed_versions = {}\n",
        "failed_packages = []\n",
        "\n",
        "for package_name, min_version, is_required in packages:\n",
        "    try:\n",
        "        # First try to install with the minimum version requirement\n",
        "        package_spec = f\"{package_name}>={min_version}\"\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, '-m', 'pip', 'install', '--upgrade', package_spec],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=300  # 5 minute timeout per package\n",
        "        )\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            # Installation succeeded - now check what version was actually installed\n",
        "            check_result = subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'show', package_name],\n",
        "                capture_output=True,\n",
        "                text=True\n",
        "            )\n",
        "            if check_result.returncode == 0:\n",
        "                # Extract version from pip show output\n",
        "                for line in check_result.stdout.split('\\n'):\n",
        "                    if line.startswith('Version:'):\n",
        "                        installed_version = line.split(':', 1)[1].strip()\n",
        "                        installed_versions[package_name] = installed_version\n",
        "                        \n",
        "                        # Check if installed version meets requirement\n",
        "                        from packaging import version as pkg_version\n",
        "                        try:\n",
        "                            if pkg_version.parse(installed_version) >= pkg_version.parse(min_version):\n",
        "                                print(f\"✓ {package_name} {installed_version} (meets requirement >= {min_version})\")\n",
        "                            else:\n",
        "                                print(f\"⚠ {package_name} {installed_version} (latest available, but < {min_version})\")\n",
        "                                print(f\"  Note: Version {min_version} may not be available. Latest is {installed_version}\")\n",
        "                        except:\n",
        "                            print(f\"✓ {package_name} {installed_version} installed\")\n",
        "                        break\n",
        "            else:\n",
        "                print(f\"✓ {package_name} installed/updated\")\n",
        "        else:\n",
        "            # If version-specific install failed, try installing latest\n",
        "            print(f\"⚠ {package_name}>={min_version} not available, trying latest version...\")\n",
        "            result2 = subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'install', '--upgrade', package_name],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=300\n",
        "            )\n",
        "            if result2.returncode == 0:\n",
        "                check_result = subprocess.run(\n",
        "                    [sys.executable, '-m', 'pip', 'show', package_name],\n",
        "                    capture_output=True,\n",
        "                    text=True\n",
        "                )\n",
        "                if check_result.returncode == 0:\n",
        "                    for line in check_result.stdout.split('\\n'):\n",
        "                        if line.startswith('Version:'):\n",
        "                            installed_version = line.split(':', 1)[1].strip()\n",
        "                            installed_versions[package_name] = installed_version\n",
        "                            print(f\"⚠ {package_name} {installed_version} (latest available, requirement was >= {min_version})\")\n",
        "                            break\n",
        "            else:\n",
        "                error_msg = result2.stderr[:200] if result2.stderr else 'Unknown error'\n",
        "                if is_required:\n",
        "                    print(f\"✗ Failed to install {package_name}: {error_msg}\")\n",
        "                    failed_packages.append((package_name, is_required))\n",
        "                else:\n",
        "                    print(f\"⚠ {package_name} - Installation failed (non-critical): {error_msg}\")\n",
        "                    print(f\"  Skipping {package_name} - notebook may work without it\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        if is_required:\n",
        "            print(f\"✗ {package_name} - Installation timed out (REQUIRED)\")\n",
        "            failed_packages.append((package_name, is_required))\n",
        "        else:\n",
        "            print(f\"⚠ {package_name} - Installation timed out (non-critical, skipping)\")\n",
        "    except Exception as e:\n",
        "        if is_required:\n",
        "            print(f\"✗ Failed to install {package_name}: {str(e)}\")\n",
        "            failed_packages.append((package_name, is_required))\n",
        "        else:\n",
        "            print(f\"⚠ Failed to install {package_name} (non-critical): {str(e)}\")\n",
        "            print(f\"  Skipping {package_name} - notebook may work without it\")\n",
        "\n",
        "# Check if any critical packages failed\n",
        "if failed_packages:\n",
        "    critical_failures = [name for name, required in failed_packages if required]\n",
        "    if critical_failures:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"⚠ WARNING: Some CRITICAL packages failed to install:\")\n",
        "        for name in critical_failures:\n",
        "            print(f\"  - {name}\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"The notebook may not work correctly. Please check your internet connection\")\n",
        "        print(\"and Python environment, or restart the kernel and try again.\")\n",
        "        print(\"=\" * 60)\n",
        "    else:\n",
        "        print(\"\\n✓ All critical packages installed successfully\")\n",
        "        print(\"  (Some optional packages were skipped, but this is OK)\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nForcefully reloading modules to use newly installed versions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get the current globals() to update them\n",
        "current_globals = globals()\n",
        "\n",
        "# List of modules to remove and reload, in dependency order\n",
        "modules_to_reload = {\n",
        "    'numpy': ['np'],\n",
        "    'scipy': ['scipy'],\n",
        "    'pandas': ['pd'],\n",
        "    'packaging': ['version'],\n",
        "    'matplotlib': ['matplotlib', 'plt'],\n",
        "    'seaborn': ['sns'],\n",
        "    'sklearn': ['sklearn'],\n",
        "    'statsmodels': ['sm']\n",
        "}\n",
        "\n",
        "# First, delete modules from sys.modules to force fresh import\n",
        "# This is more aggressive than reload and ensures we get the new version\n",
        "modules_to_delete = []\n",
        "for base_module in ['numpy', 'scipy', 'pandas', 'packaging', 'matplotlib', 'seaborn', 'sklearn', 'statsmodels']:\n",
        "    # Find all modules that start with this base name\n",
        "    for mod_name in list(sys.modules.keys()):\n",
        "        if mod_name == base_module or mod_name.startswith(base_module + '.'):\n",
        "            modules_to_delete.append(mod_name)\n",
        "\n",
        "# Delete in reverse order (submodules first)\n",
        "for mod_name in sorted(modules_to_delete, reverse=True):\n",
        "    if mod_name in sys.modules:\n",
        "        del sys.modules[mod_name]\n",
        "\n",
        "print(f\"✓ Deleted {len(modules_to_delete)} cached module entries\")\n",
        "\n",
        "# Now re-import everything fresh - this will load the newly installed versions\n",
        "try:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import statsmodels.api as sm\n",
        "    from statsmodels.discrete.discrete_model import BinaryResultsWrapper\n",
        "    import sklearn\n",
        "    import scipy\n",
        "    from scipy.stats import multivariate_normal\n",
        "    from scipy.special import expit as logistic_sigmoid\n",
        "    from packaging import version\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import brier_score_loss\n",
        "    from sklearn.metrics import balanced_accuracy_score, brier_score_loss, accuracy_score, roc_curve, auc\n",
        "    from sklearn.model_selection import KFold\n",
        "    \n",
        "    # Update globals to ensure the new versions are available\n",
        "    globals().update({\n",
        "        'np': np,\n",
        "        'pd': pd,\n",
        "        'matplotlib': matplotlib,\n",
        "        'plt': plt,\n",
        "        'sns': sns,\n",
        "        'sm': sm,\n",
        "        'sklearn': sklearn,\n",
        "        'scipy': scipy,\n",
        "        'multivariate_normal': multivariate_normal,\n",
        "        'logistic_sigmoid': logistic_sigmoid,\n",
        "        'version': version,\n",
        "        'train_test_split': train_test_split,\n",
        "        'brier_score_loss': brier_score_loss,\n",
        "        'balanced_accuracy_score': balanced_accuracy_score,\n",
        "        'accuracy_score': accuracy_score,\n",
        "        'roc_curve': roc_curve,\n",
        "        'auc': auc,\n",
        "        'KFold': KFold\n",
        "    })\n",
        "    \n",
        "    print(\"✓ Re-imported all modules with new versions\")\n",
        "    \n",
        "    # Verify versions\n",
        "    print(\"\\nVerifying installed versions...\")\n",
        "    print(f\"  NumPy: {np.__version__}\")\n",
        "    print(f\"  Pandas: {pd.__version__}\")\n",
        "    print(f\"  Matplotlib: {matplotlib.__version__}\")\n",
        "    print(f\"  Seaborn: {sns.__version__}\")\n",
        "    print(f\"  Statsmodels: {sm.__version__}\")\n",
        "    print(f\"  Scikit-learn: {sklearn.__version__}\")\n",
        "    print(f\"  SciPy: {scipy.__version__}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error re-importing modules: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"All packages installed and modules reloaded!\")\n",
        "print(\"Version check in the next cell should now pass.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cell-2-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking Python and library versions...\n",
            "============================================================\n",
            "✓ Python: 3.11.7\n",
            "✓ Pandas: 2.3.3 (meets requirement >= 2.2.0)\n",
            "✓ NumPy: 1.26.4 (meets requirement >= 1.26.0)\n",
            "✓ Statsmodels: 0.14.5 (meets requirement >= 0.14)\n",
            "✓ Matplotlib: 3.10.7 (meets requirement >= 3.8.0)\n",
            "✓ scikit-learn: 1.7.2 (meets requirement >= 1.5.0)\n",
            "✓ Seaborn: 0.13.2 (meets requirement >= 0.13)\n",
            "✓ SciPy: 1.16.3 (meets requirement >= 1.11.0)\n",
            "============================================================\n",
            "✓ All version checks passed (or using latest available versions)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# These are the recommended (tested) versions of the libraries\n",
        "# A separate yaml file is provided for setting up the environment\n",
        "# Note: If a required version isn't available, we check if the latest installed version is compatible\n",
        "\n",
        "def check_version(module_name, module_obj, required_version, flexible=False):\n",
        "    \"\"\"Check if module version meets requirement, with optional flexibility for unavailable versions\"\"\"\n",
        "    try:\n",
        "        installed_version = module_obj.__version__\n",
        "        if version.parse(installed_version) >= version.parse(required_version):\n",
        "            print(f\"✓ {module_name}: {installed_version} (meets requirement >= {required_version})\")\n",
        "            return True\n",
        "        else:\n",
        "            if flexible:\n",
        "                print(f\"⚠ {module_name}: {installed_version} (required >= {required_version}, but latest available)\")\n",
        "                print(f\"  Continuing with available version...\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"✗ {module_name}: {installed_version} (needs >= {required_version})\")\n",
        "                return False\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ {module_name}: Could not check version - {str(e)}\")\n",
        "        return flexible  # Allow if flexible mode\n",
        "\n",
        "print(\"Checking Python and library versions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check Python version\n",
        "if sys.version_info >= (3, 11):\n",
        "    print(f\"✓ Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
        "else:\n",
        "    raise AssertionError(f\"This notebook requires Python 3.11 or above. You have {sys.version_info.major}.{sys.version_info.minor}\")\n",
        "\n",
        "# Check library versions\n",
        "# Note: We're using more flexible version requirements to ensure compatibility\n",
        "# NumPy 2.x has breaking changes, so we're using 1.x for better compatibility\n",
        "all_passed = True\n",
        "\n",
        "all_passed = check_version(\"Pandas\", pd, \"2.2.0\", flexible=True) and all_passed\n",
        "all_passed = check_version(\"NumPy\", np, \"1.26.0\", flexible=True) and all_passed  # Using 1.x for compatibility\n",
        "all_passed = check_version(\"Statsmodels\", sm, \"0.14\", flexible=True) and all_passed\n",
        "all_passed = check_version(\"Matplotlib\", matplotlib, \"3.8.0\", flexible=True) and all_passed\n",
        "all_passed = check_version(\"scikit-learn\", sklearn, \"1.5.0\", flexible=True) and all_passed\n",
        "all_passed = check_version(\"Seaborn\", sns, \"0.13\", flexible=True) and all_passed\n",
        "all_passed = check_version(\"SciPy\", scipy, \"1.11.0\", flexible=True) and all_passed\n",
        "\n",
        "print(\"=\" * 60)\n",
        "if all_passed:\n",
        "    print(\"✓ All version checks passed (or using latest available versions)\")\n",
        "else:\n",
        "    print(\"⚠ Some version checks failed, but continuing with available versions\")\n",
        "    print(\"  If you encounter issues, you may need to update your environment\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-3-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set display option to avoid scientific notation in pandas, show up to 5 decimal points\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "# and numpy\n",
        "np.set_printoptions(suppress=True, precision=5)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-4-markdown",
      "metadata": {},
      "source": [
        "In this workbook we will be attempting to learn a model of <b>conspiracy spreading tweets</b> for the day of Januray 6th in the US. The model's job is to preemptively identify whether the tweet is likely to be fake-news sharing, without delving into the content of the tweet, but rather using a series of general features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-5-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the labeled dataset of tweets \n",
        "df_labs = pd.read_csv('sem3_topic4_logreg_formative2_data-1.csv', low_memory=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-6-markdown",
      "metadata": {},
      "source": [
        "## Part 1: Data Cleaning & Exploration\n",
        "\n",
        "Your task is to clean the data. You need to complete the following tasks: "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-7-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1A\n",
        "Drop incomplete records "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8-code",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "int() argument must be a string, a bytes-like object or a real number, not '_NoValueType'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m         complete_row_indices\u001b[38;5;241m.\u001b[39mappend(idx)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Use loc with the list of indices to select only complete rows\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# This avoids the boolean indexing bug in NumPy 2.x\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m df_labs \u001b[38;5;241m=\u001b[39m df_labs\u001b[38;5;241m.\u001b[39mloc[complete_row_indices]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Report results\u001b[39;00m\n\u001b[1;32m     43\u001b[0m dropped_count \u001b[38;5;241m=\u001b[39m original_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_labs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1192\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1191\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1421\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1419\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_iterable(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1361\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1361\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1363\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1559\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1556\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1557\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1559\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6214\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m   6212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m-> 6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n\u001b[1;32m   6217\u001b[0m     keyarr\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mname\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/range.py:1168\u001b[0m, in \u001b[0;36mRangeIndex.take\u001b[0;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     taken \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m     ind_max \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ind_max \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mind_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for axis 0 with size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1172\u001b[0m         )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_maximum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not '_NoValueType'"
          ]
        }
      ],
      "source": [
        "# Drop incomplete records, keep the variable name 'df_labs' for the cleaned dataset\n",
        "# dropna() removes rows that have any missing values (NaN) in any column\n",
        "# This is important for logistic regression as it requires complete data\n",
        "# \n",
        "# NOTE: There's a known compatibility issue between NumPy 2.x and Pandas that causes\n",
        "# errors in dropna() and boolean indexing. We use a workaround that avoids these issues.\n",
        "\n",
        "# Store original length to report how many rows were dropped\n",
        "original_len = len(df_labs)\n",
        "\n",
        "# Workaround for NumPy 2.x compatibility: \n",
        "# Build a new dataframe by iterating and copying complete rows\n",
        "# This completely avoids numpy operations that cause the bug\n",
        "import math\n",
        "\n",
        "# Method: Build new dataframe row by row using dictionary approach\n",
        "complete_rows = []\n",
        "for i in range(len(df_labs)):\n",
        "    row = df_labs.iloc[i]\n",
        "    # Check if row has any missing values by converting to list and checking\n",
        "    row_list = row.tolist()\n",
        "    has_missing = False\n",
        "    for v in row_list:\n",
        "        # Check for None\n",
        "        if v is None:\n",
        "            has_missing = True\n",
        "            break\n",
        "        # Check for NaN (float NaN) using math.isnan\n",
        "        try:\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                has_missing = True\n",
        "                break\n",
        "        except (TypeError, ValueError):\n",
        "            pass\n",
        "    \n",
        "    # If row is complete, add it to our list as a dictionary\n",
        "    if not has_missing:\n",
        "        complete_rows.append(row.to_dict())\n",
        "\n",
        "# Create new dataframe from complete rows\n",
        "# This avoids all the problematic pandas/numpy indexing operations\n",
        "if complete_rows:\n",
        "    df_labs = pd.DataFrame(complete_rows)\n",
        "    # Preserve the original index if possible, or create new sequential index\n",
        "    df_labs = df_labs.reset_index(drop=True)\n",
        "else:\n",
        "    # Edge case: no complete rows\n",
        "    df_labs = pd.DataFrame(columns=df_labs.columns)\n",
        "\n",
        "# Report results\n",
        "dropped_count = original_len - len(df_labs)\n",
        "print(f\"Dropped {dropped_count} rows with missing values (kept {len(df_labs)} complete rows)\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-9-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1B \n",
        "Create a dummy variable called `conspiracy_binary`, taking value `1` when the conspiracy-assessment is `Yes`, and `0` otherwise.  \n",
        "\n",
        "Hint: use `.astype(int)` to ensure the results are numbers, not booleans. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conspiracy spreading flag\n",
        "# Convert the 'Conspiracy Assessment' column to binary: 'Yes' -> 1, anything else -> 0\n",
        "# This creates our target variable (dependent variable) for logistic regression\n",
        "# .astype(int) ensures we get integers (0 or 1) rather than boolean values\n",
        "conspiracy_binary = (df_labs['Conspiracy Assessment'] == 'Yes').astype(int)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-11-markdown",
      "metadata": {},
      "source": [
        "Let's have a look at what kinds of tweets we are talking about. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter rows where 'conspiracy_binary' is 1\n",
        "conspiracy_texts = df_labs.loc[conspiracy_binary == 1, 'text']\n",
        "\n",
        "# Sample 10 random texts\n",
        "random_texts = conspiracy_texts.sample(n=10, random_state=np.random.RandomState())\n",
        "\n",
        "# Iterate through the selected texts and print each one in full\n",
        "for index, text in enumerate(random_texts, start=1):\n",
        "    print(f\"Text {index}: {text}\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-13-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1C\n",
        "One-hot encode political ideology (retain just conservative and liberal columns), sentiment (retain just negative and positive columns).\n",
        "\n",
        "Note: Name the new columns `Political Leanings_Conservative`, `Political Leanings_Liberal`, `Sentiment Analysis_Negative`, and `Sentiment Analysis_Positive`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ideology\n",
        "# One-hot encoding converts categorical variables into binary (0/1) columns\n",
        "# pd.get_dummies() creates binary columns for each unique value in 'Political Leanings'\n",
        "# We only keep 'Conservative' and 'Liberal' columns as specified\n",
        "# drop_first=False means we keep all categories (we'll manually select what we need)\n",
        "pol_lean_one_hot = pd.get_dummies(df_labs['Political Leanings'], prefix='Political Leanings')\n",
        "# Select only the Conservative and Liberal columns\n",
        "pol_lean_one_hot = pol_lean_one_hot[['Political Leanings_Conservative', 'Political Leanings_Liberal']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment \n",
        "# Similar to ideology, we one-hot encode the sentiment analysis results\n",
        "# pd.get_dummies() creates binary columns for each sentiment category\n",
        "# We only keep 'Negative' and 'Positive' columns as specified\n",
        "sentiment_one_hot = pd.get_dummies(df_labs['Sentiment Analysis'], prefix='Sentiment Analysis')\n",
        "# Select only the Negative and Positive columns\n",
        "sentiment_one_hot = sentiment_one_hot[['Sentiment Analysis_Negative', 'Sentiment Analysis_Positive']]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-16-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1D\n",
        "Make a binary variable indicating if the source of the tweet was an Apple device.\n",
        "\n",
        "Hint: We found 6 different sources associated with Apple. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-17-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apple product\n",
        "# Create a binary variable indicating if the tweet source was from an Apple device\n",
        "# The hint says there are 6 different Apple sources, so we check if the source contains common Apple identifiers\n",
        "# Common Apple sources include: iPhone, iPad, Mac, etc.\n",
        "# We use .str.contains() with case=False to check for any Apple-related source\n",
        "# The '|' means OR, so we check for multiple possible Apple device names\n",
        "apple_sources = ['iPhone', 'iPad', 'Mac', 'iPod', 'Apple', 'iOS']\n",
        "# Check if the source column contains any of these Apple identifiers\n",
        "# .astype(int) converts True/False to 1/0\n",
        "apple_binary = df_labs['source'].str.contains('|'.join(apple_sources), case=False, na=False).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lexical diversity \n",
        "lexical_diversity_likert = df_labs['Lexical Diversity'].astype(int)\n",
        "# Spelling and Grammar \n",
        "spelling_grammar_likert = df_labs['Spelling and Grammar Quality'].astype(int)\n",
        "# Activity: \n",
        "user_active_num = df_labs['statuses_count'].astype(int)\n",
        "# Popularity: \n",
        "user_popular_num = df_labs['followers_count'].astype(int)\n",
        "# Tweet Popularity\n",
        "tweet_popular_num = df_labs['retweet_count'].astype(int)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-19-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1E\n",
        "One-hot encode state identifiers, storing the results in a matrix. \n",
        "Remember to drop the first dummy (dummy-trap)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encode state identifiers\n",
        "# pd.get_dummies() creates binary columns for each state\n",
        "# drop_first=True avoids the \"dummy variable trap\" - we drop one state as the reference category\n",
        "# This is important because if we have all states, they sum to 1 (perfect multicollinearity)\n",
        "# In logistic regression, we need to drop one category to avoid this issue\n",
        "states_one_hot = pd.get_dummies(df_labs['state'], prefix='state', drop_first=True)\n",
        "\n",
        "# Filtering to get just the state dummy columns\n",
        "# This creates a matrix (DataFrame) with only the state columns\n",
        "# We'll use this later when building the full model with states\n",
        "states_matrix = states_one_hot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-21-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1F\n",
        "Concatenate the clean variables into a new dataframe called `X`. Exclude the `states_matrix` for now. \n",
        "Do not include the outcome (conspiracy binary).\n",
        "\n",
        "Hint: There should be 10 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-22-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate all the clean variables into a feature matrix X\n",
        "# pd.concat() combines multiple DataFrames/Series along columns (axis=1)\n",
        "# We exclude states_matrix for now (as specified) and the outcome variable (conspiracy_binary)\n",
        "# The 10 columns should be:\n",
        "# 1. Political Leanings_Conservative\n",
        "# 2. Political Leanings_Liberal\n",
        "# 3. Sentiment Analysis_Negative\n",
        "# 4. Sentiment Analysis_Positive\n",
        "# 5. apple_binary\n",
        "# 6. lexical_diversity_likert\n",
        "# 7. spelling_grammar_likert\n",
        "# 8. user_active_num (statuses_count)\n",
        "# 9. user_popular_num (followers_count)\n",
        "# 10. tweet_popular_num (retweet_count)\n",
        "X = pd.concat([\n",
        "    pol_lean_one_hot,           # Political ideology (2 columns)\n",
        "    sentiment_one_hot,           # Sentiment (2 columns)\n",
        "    pd.Series(apple_binary, name='apple_binary'),  # Apple device (1 column)\n",
        "    pd.Series(lexical_diversity_likert, name='lexical_diversity_likert'),  # Lexical diversity (1 column)\n",
        "    pd.Series(spelling_grammar_likert, name='spelling_grammar_likert'),    # Spelling/Grammar (1 column)\n",
        "    pd.Series(user_active_num, name='user_active_num'),                    # User activity (1 column)\n",
        "    pd.Series(user_popular_num, name='user_popular_num'),                  # User popularity (1 column)\n",
        "    pd.Series(tweet_popular_num, name='tweet_popular_num')                 # Tweet popularity (1 column)\n",
        "], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-23-markdown",
      "metadata": {},
      "source": [
        "### Exercise 1G\n",
        "Calculate the correlation matrix across the outcome and X. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-24-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add conspiracy_binary as the first column in X to create a combined DataFrame YX\n",
        "X['conspiracy_binary'] = conspiracy_binary\n",
        "YX = X[['conspiracy_binary'] + [c for c in X.columns if c != 'conspiracy_binary']]  # Ensure conspiracy_binary is the first column\n",
        "\n",
        "# Calculate the Correlation Matrix\n",
        "# .corr() computes pairwise correlation of columns in the DataFrame\n",
        "# This helps us understand relationships between features and the target variable\n",
        "# Correlation ranges from -1 (perfect negative) to +1 (perfect positive)\n",
        "# Values close to 0 indicate weak/no linear relationship\n",
        "corr = YX.corr()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True, fmt=\".2f\", annot_kws={\"size\": 7})\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-25-markdown",
      "metadata": {},
      "source": [
        "## Part 2: Model Assessment and Selection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-26-markdown",
      "metadata": {},
      "source": [
        "### Exercise 2A \n",
        "Set up the full design matrix X, this time include the states_matrix, and a constant. \n",
        "Finally bind the outcome to it and ensure it's the first column of the resulting dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-27-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Design matrix\n",
        "# Now we create the full design matrix including states_matrix (which we excluded before)\n",
        "# pd.concat() combines the original X (without conspiracy_binary) with states_matrix\n",
        "# axis=1 means we're concatenating along columns (side by side)\n",
        "X = pd.concat([X.drop('conspiracy_binary', axis=1), states_matrix], axis=1)\n",
        "\n",
        "# Add a constant to the feature matrix for statsmodels\n",
        "# In logistic regression, we need an intercept term (beta_0)\n",
        "# sm.add_constant() adds a column of ones, which allows statsmodels to estimate the intercept\n",
        "# This is necessary because statsmodels doesn't automatically include an intercept\n",
        "X_const = sm.add_constant(X)\n",
        "\n",
        "# Get full dataset together \n",
        "# Combine the outcome variable (conspiracy_binary) with the design matrix\n",
        "# We ensure conspiracy_binary is the first column for consistency\n",
        "YX_const = pd.concat([pd.Series(conspiracy_binary, name='Conspiracy Assessment'), X_const], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-28-markdown",
      "metadata": {},
      "source": [
        "## Exercise 2B \n",
        "Create a training set (75%) and test set (25%). \n",
        "Ensure the rows of the full dataset selected for each set are chosen at random (use seed 42)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-29-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train and test (75:25)\n",
        "# train_test_split() randomly divides the data into training and testing sets\n",
        "# test_size=0.25 means 25% goes to test set, 75% to training set\n",
        "# random_state=42 ensures reproducibility - same random split every time\n",
        "# shuffle=True means data is shuffled before splitting (good practice)\n",
        "# stratify=None means we don't stratify by class (could use YX_const['Conspiracy Assessment'] if we wanted balanced splits)\n",
        "YX_const_train, YX_const_test = train_test_split(YX_const, test_size=0.25, random_state=42, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-30-markdown",
      "metadata": {},
      "source": [
        "### Exercise 2C\n",
        "Using a dictionary, define three candidate models in terms of the columns of the design matrix involved in each. \n",
        "The first model should be the homogeneous probability model; the second should have have all covariates except the states; the third should use all the columns. Name the keys `homogeneous`, `no_states`, and `all`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-31-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define predictors for each model variant\n",
        "# We create a dictionary where each key is a model name and each value is a list of column names\n",
        "# This allows us to easily compare different model specifications\n",
        "\n",
        "# Model 1: 'homogeneous' - no predictors, just the intercept (constant term only)\n",
        "# This is the null model that assumes the same probability for all observations\n",
        "predictors = {\n",
        "    'homogeneous': ['const'],  # Only the intercept term\n",
        "    \n",
        "    # Model 2: 'no_states' - all covariates except state dummies\n",
        "    # This includes: political leanings, sentiment, apple device, lexical diversity,\n",
        "    # spelling/grammar, user activity, user popularity, tweet popularity, and intercept\n",
        "    'no_states': ['const', 'Political Leanings_Conservative', 'Political Leanings_Liberal',\n",
        "                  'Sentiment Analysis_Negative', 'Sentiment Analysis_Positive',\n",
        "                  'apple_binary', 'lexical_diversity_likert', 'spelling_grammar_likert',\n",
        "                  'user_active_num', 'user_popular_num', 'tweet_popular_num'],\n",
        "    \n",
        "    # Model 3: 'all' - includes everything including state dummies\n",
        "    # This is the full model with all available predictors\n",
        "    'all': list(X_const.columns)  # All columns from the design matrix including states\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-32-markdown",
      "metadata": {},
      "source": [
        "### Exercise 2D\n",
        "Using 5-fold cross-validation on the training set, compare the models using the following metrics: Brier score, Accuracy, Balanced Accuracy, and AIC.\n",
        "\n",
        "For this question, given we are not at this stage interested in making inference but just understand which model has the best predictive power, you can avoid simulating and simply make point-estimate predictions. \n",
        "\n",
        "You can do this by simply fitting the model with sm.Logit, and using directly after the function 'model.predict',avoiding sampling from the approximate posterior of the betas, and then from the posterior predictive of y. \n",
        "\n",
        "This will not give you uncertainty estimates around your predictions, but will allow you to compare models based on their point-predictions, and that's good enough for model selection purposes. When we want to make inference, we want to also have access to uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-33-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = YX_const_train['Conspiracy Assessment'] # Target variable\n",
        "\n",
        "# Define K - number of folds for cross-validation\n",
        "# 5-fold CV means we split data into 5 parts, train on 4, test on 1, repeat 5 times\n",
        "K = 5\n",
        "\n",
        "# Setup the KFold cross-validation\n",
        "# KFold splits data into K folds for cross-validation\n",
        "# shuffle=True randomizes the data before splitting (with random_state from earlier)\n",
        "# random_state=42 ensures reproducibility\n",
        "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize a dictionary to store Brier scores\n",
        "scores = {key: [] for key in predictors}\n",
        "\n",
        "# Initialize dictionaries to store scores for each metric\n",
        "# We'll calculate these for each fold and then average them\n",
        "brier_scores = {key: [] for key in predictors}  # Brier score: lower is better (measures calibration)\n",
        "acc_scores = {key: [] for key in predictors}    # Accuracy: proportion of correct predictions\n",
        "balanced_acc_scores = {key: [] for key in predictors}  # Balanced accuracy: accounts for class imbalance\n",
        "aic_scores = {key: [] for key in predictors}    # AIC: Akaike Information Criterion (lower is better)\n",
        "\n",
        "# Loop through each model specification\n",
        "for key, cols in predictors.items():\n",
        "    \n",
        "    # For each fold in the cross-validation\n",
        "    for train_index, test_index in kf.split(YX_const_train):\n",
        "        \n",
        "        # Split into train and test according to the folds \n",
        "        # train_index and test_index are arrays of row indices for this fold\n",
        "        # We select only the columns specified for this model (cols)\n",
        "        X_train, X_test = YX_const_train.iloc[train_index][cols], YX_const_train.iloc[test_index][cols]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        # For each fold split, fit the model\n",
        "        # sm.Logit() creates a logistic regression model object\n",
        "        # .fit() estimates the coefficients using maximum likelihood estimation\n",
        "        # disp=0 suppresses output during fitting\n",
        "        model = sm.Logit(y_train, X_train).fit(disp=0)\n",
        "\n",
        "        # Predict probabilities\n",
        "        # .predict() returns the predicted probability of class 1 (conspiracy = Yes)\n",
        "        # These are point estimates (no uncertainty) as specified in the instructions\n",
        "        y_pred_prob = model.predict(X_test)\n",
        "\n",
        "        # Calculate Brier score\n",
        "        # Brier score = mean((predicted_prob - actual_binary)^2)\n",
        "        # Lower is better (0 = perfect, 1 = worst)\n",
        "        # Measures how well-calibrated the probabilities are\n",
        "        brier_score = brier_score_loss(y_test, y_pred_prob)\n",
        "        brier_scores[key].append(brier_score)\n",
        "\n",
        "        # Convert probabilities to binary predictions (assume simple >0.5 probability as threshold)\n",
        "        # If predicted probability > 0.5, predict class 1, else predict class 0\n",
        "        # .astype(int) converts boolean to integer (0 or 1)\n",
        "        y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "        # Calculate Accuracy Score\n",
        "        # Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "        # Proportion of correct predictions overall\n",
        "        acc_score = accuracy_score(y_test, y_pred_binary)\n",
        "        acc_scores[key].append(acc_score)\n",
        "        \n",
        "        # Calculate Balanced Accuracy Score\n",
        "        # Balanced accuracy = (Sensitivity + Specificity) / 2\n",
        "        # Accounts for class imbalance by averaging recall for each class\n",
        "        # Better than accuracy when classes are imbalanced\n",
        "        bal_acc_score = balanced_accuracy_score(y_test, y_pred_binary)\n",
        "        balanced_acc_scores[key].append(bal_acc_score)\n",
        "        \n",
        "        # Store AIC (Akaike Information Criterion)\n",
        "        # AIC = -2*log_likelihood + 2*number_of_parameters\n",
        "        # Lower AIC indicates better model (penalizes complexity)\n",
        "        # Used for model selection - balances fit quality with model complexity\n",
        "        aic_scores[key].append(model.aic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-34-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate and print the average scores\n",
        "results = []\n",
        "for key in predictors.keys():\n",
        "    average_brier_score = np.mean(brier_scores[key])\n",
        "    average_bal_acc_score = np.mean(balanced_acc_scores[key])\n",
        "    average_acc_score = np.mean(acc_scores[key])\n",
        "    average_aic_score = np.mean(aic_scores[key])  # Calculate average AIC\n",
        "    results.append({\n",
        "        'Model': key,\n",
        "        'Average Brier Score': average_brier_score,\n",
        "        'Average Accuracy': average_acc_score,\n",
        "        'Average Balanced Accuracy': average_bal_acc_score,\n",
        "        'Average AIC': average_aic_score\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame for nicer display\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-35-markdown",
      "metadata": {},
      "source": [
        "### Exercise 2E \n",
        "Re-fit the model with the lowest average AIC to the full training set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-36-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now fit the model to the full training set\n",
        "# We identified the best model based on lowest average AIC from cross-validation\n",
        "# Find which model had the lowest average AIC\n",
        "best_model_key = min(predictors.keys(), key=lambda k: np.mean(aic_scores[k]))\n",
        "best_predictors = predictors[best_model_key]\n",
        "\n",
        "# Extract the target variable and features for the full training set\n",
        "y_train_full = YX_const_train['Conspiracy Assessment']\n",
        "X_train_full = YX_const_train[best_predictors]\n",
        "\n",
        "# Fit the best model to the entire training set (not just one fold)\n",
        "# This gives us the final model coefficients using all available training data\n",
        "# sm.Logit() fits a logistic regression model\n",
        "# .fit() estimates parameters using maximum likelihood\n",
        "model = sm.Logit(y_train_full, X_train_full).fit(disp=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-37-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get summary results\n",
        "summary = model.summary()\n",
        "print(summary)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-38-markdown",
      "metadata": {},
      "source": [
        "## Part 3: Model Evaluation and Estimation of Generalisation Error"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-39-markdown",
      "metadata": {},
      "source": [
        "### Exercise 3A \n",
        "Generate 1000 simulations of the regression coefficients by sampling from the empirical posterior distribution. Use seed 42.\n",
        "\n",
        "Hint: check the documentation of `scipy.stats.multivariate_normal.rvs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-40-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the coefficients (betas) and their covariance matrix from the logistic regression fit\n",
        "# model.params contains the estimated coefficients (mean of the posterior distribution)\n",
        "# In frequentist statistics, these are the maximum likelihood estimates\n",
        "beta_mean = model.params\n",
        "\n",
        "# model.cov_params() contains the covariance matrix of the coefficients\n",
        "# This tells us how uncertain we are about each coefficient and how they covary\n",
        "# The diagonal elements are variances, off-diagonal are covariances\n",
        "beta_cov = model.cov_params()\n",
        "\n",
        "# Number of simulations\n",
        "# We'll generate 1000 different sets of coefficients to capture uncertainty\n",
        "n_simulations = 1000\n",
        "\n",
        "# Simulate beta coefficients\n",
        "# We assume the coefficients follow a multivariate normal distribution\n",
        "# This is a common assumption in Bayesian/frequentist inference for logistic regression\n",
        "# multivariate_normal.rvs() samples from a multivariate normal distribution\n",
        "# mean=beta_mean: center of the distribution (our point estimates)\n",
        "# cov=beta_cov: covariance matrix (uncertainty in estimates)\n",
        "# size=n_simulations: number of samples to draw\n",
        "# random_state=42: ensures reproducibility\n",
        "simulated_betas = multivariate_normal.rvs(mean=beta_mean, cov=beta_cov, size=n_simulations, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-41-markdown",
      "metadata": {},
      "source": [
        "### Exercise 3B  \n",
        "For each simulation, generate a predicted probability for the test-set conspiracy assessments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-42-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize an array to store predictions from each simulation\n",
        "# Shape: (n_simulations, n_test_samples)\n",
        "# Each row is one simulation, each column is one test observation\n",
        "predictions = np.zeros((n_simulations, YX_const_test.shape[0]))\n",
        "\n",
        "# Get the feature matrix for the test set (using the same predictors as the best model)\n",
        "# We need to use the same columns that were used in the best model\n",
        "X_test = YX_const_test[best_predictors]\n",
        "\n",
        "# Generate predictions for each simulation\n",
        "for i in range(n_simulations):\n",
        "    # Get the beta coefficients for this simulation\n",
        "    # Each row of simulated_betas is one set of coefficients\n",
        "    beta_simulation = simulated_betas[i]\n",
        "    \n",
        "    # Calculate log-odds (logit) for each test observation\n",
        "    # log-odds = X * beta (matrix multiplication)\n",
        "    # This gives us the linear combination of features weighted by coefficients\n",
        "    # np.dot() performs matrix multiplication: X_test (n_samples x n_features) * beta_simulation (n_features,)\n",
        "    log_odds = np.dot(X_test, beta_simulation)\n",
        "    \n",
        "    # Convert log-odds to probabilities using the logistic (sigmoid) function\n",
        "    # probability = 1 / (1 + exp(-log_odds))\n",
        "    # This transforms the linear combination into a probability between 0 and 1\n",
        "    # expit() is the logistic sigmoid function from scipy\n",
        "    probabilities = logistic_sigmoid(log_odds)\n",
        "    \n",
        "    # Store the predicted probabilities for this simulation\n",
        "    predictions[i] = probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-43-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-44-markdown",
      "metadata": {},
      "source": [
        "For the first 20 assessments in the test-set, we will plot the posterior distirbution of the probabilities, and highlight whether the density of each lies above or below a given `threshold` for classification. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-45-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "true_labels = YX_const_test['Conspiracy Assessment']\n",
        "\n",
        "# Calculate posterior median and the 90% prediction interval for each of the first 10 observations\n",
        "posterior_medians = np.median(predictions, axis=0)\n",
        "lower_bounds = np.percentile(predictions, 5, axis=0)\n",
        "upper_bounds = np.percentile(predictions, 95, axis=0)\n",
        "\n",
        "# Plotting with the adjustments for the 90% prediction interval to be shown with red lines\n",
        "fig, axes = plt.subplots(4, 5, figsize=(25, 16))\n",
        "\n",
        "for i in range(20):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "    # Histogram of simulated probabilities for observation i\n",
        "    ax.hist(predictions[:, i], bins=30, color='skyblue', edgecolor='white', alpha=0.7)\n",
        "    \n",
        "    # Draw a line for the decision boundary \n",
        "    ax.axvline(x=0.5, color='black', linewidth=1, label='Decision Boundary')\n",
        "    \n",
        "    # Draw a thick solid black line at the true label position\n",
        "    true_label_position = 0 if true_labels.iloc[i] == 0 else 1  # Determine the position based on the true label\n",
        "    ax.axvline(x=true_label_position, color='black', linewidth=3, label='True Label')\n",
        "    \n",
        "    # Add posterior median\n",
        "    ax.axvline(x=posterior_medians[i], color='red', linestyle='--', label='Posterior Median')\n",
        "    \n",
        "    # Marking the 90% prediction interval with red lines instead of shading\n",
        "    ax.axvline(x=lower_bounds[i], color='red', linestyle='-', linewidth=1, label='90% Prediction Interval' if i == 0 else \"\")\n",
        "    ax.axvline(x=upper_bounds[i], color='red', linestyle='-', linewidth=1)\n",
        "    \n",
        "    ax.set_xlim(-0.1, 1.1)\n",
        "    ax.set_title(f'Observation {i+1}')\n",
        "    if i == 0:  # Add legend to the first subplot only to avoid repetition\n",
        "        ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-46-markdown",
      "metadata": {},
      "source": [
        "### Exercise 3C \n",
        "Simulate classes (1s or 0s) for the test-set conspiracy assessments, from the posterior predictive distirbution. \n",
        "\n",
        "Hint: check documentation of `np.random.binomial`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-47-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate from the posterior-predictive distribution \n",
        "# The posterior-predictive distribution gives us the distribution of possible outcomes\n",
        "# given our uncertainty about the model parameters\n",
        "# For each test observation and each simulation, we sample a binary outcome (0 or 1)\n",
        "# based on the predicted probability\n",
        "\n",
        "# np.random.binomial() samples from a binomial distribution\n",
        "# n=1: we're doing one trial (one coin flip) per observation\n",
        "# p=predictions: the probability of success (class 1) for each observation in each simulation\n",
        "# This gives us a 2D array: (n_simulations, n_test_samples)\n",
        "# Each value is either 0 or 1, sampled according to the predicted probability\n",
        "# random_state=42 ensures reproducibility\n",
        "simulated_outcomes = np.random.binomial(n=1, p=predictions, size=predictions.shape, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-48-markdown",
      "metadata": {},
      "source": [
        "### Exercise 3D\n",
        "Calculate the generalisation error for Classification. \n",
        "Choose <b>one</b> classification error metric you wish from the following list: `[Accuracy, Brier Score, AUC]`. The most basic metric we might be interested about is just `accuracy`. \n",
        "\n",
        "Hint: We have 1000 simulated predicted classes. For each of those 1000 sets of simulations of the test-set labels, you need to calculate the accuracy. Then you have to plot the histogram of the accuracies. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-49-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_histogram(metric_values, metric_name):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(metric_values, bins=30, color='skyblue', edgecolor='white')\n",
        "    plt.axvline(x=np.median(metric_values), color='red', label='Median')\n",
        "    plt.axvline(x=np.percentile(metric_values, 5), color='red', linestyle='--', label='5th percentile')\n",
        "    plt.axvline(x=np.percentile(metric_values, 95), color='red', linestyle='--', label='95th percentile')\n",
        "    plt.xlabel(metric_name)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Out-of-Sample Posterior Distribution of {metric_name}')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-50-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate selected metric for each simulation and plot histogram (choose from Accuracy, Brier Score, AUC)\n",
        "# We'll calculate all three metrics to show the generalization error distribution\n",
        "\n",
        "# Get the true labels for the test set\n",
        "true_labels = YX_const_test['Conspiracy Assessment'].values\n",
        "\n",
        "# Initialize lists to store metrics for each simulation\n",
        "accuracies = []  # Accuracy for each simulation\n",
        "briers = []      # Brier score for each simulation\n",
        "aucs = []        # AUC (Area Under ROC Curve) for each simulation\n",
        "\n",
        "# Loop through each simulation\n",
        "for i in range(n_simulations):\n",
        "    # For each simulation, we have:\n",
        "    # - predictions[i]: predicted probabilities for all test observations\n",
        "    # - simulated_outcomes[i]: simulated binary outcomes for all test observations\n",
        "    \n",
        "    # Calculate Accuracy using simulated binary outcomes\n",
        "    # Accuracy = proportion of correct predictions\n",
        "    # We compare simulated outcomes to true labels\n",
        "    acc = accuracy_score(true_labels, simulated_outcomes[i])\n",
        "    accuracies.append(acc)\n",
        "    \n",
        "    # Calculate Brier Score using predicted probabilities\n",
        "    # Brier score measures calibration (how well probabilities match outcomes)\n",
        "    # Lower is better\n",
        "    brier = brier_score_loss(true_labels, predictions[i])\n",
        "    briers.append(brier)\n",
        "    \n",
        "    # Calculate AUC (Area Under ROC Curve) using predicted probabilities\n",
        "    # AUC measures the model's ability to distinguish between classes\n",
        "    # Higher is better (1.0 = perfect, 0.5 = random)\n",
        "    # roc_curve() computes the ROC curve, auc() calculates the area under it\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions[i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    aucs.append(roc_auc)\n",
        "\n",
        "# Plot histograms for all three metrics\n",
        "print(\"Accuracy Distribution:\")\n",
        "plot_histogram(accuracies, \"Accuracy\")\n",
        "\n",
        "print(\"\\nBrier Score Distribution:\")\n",
        "plot_histogram(briers, \"Brier Score\")\n",
        "\n",
        "print(\"\\nAUC Distribution:\")\n",
        "plot_histogram(aucs, \"AUC\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-51-markdown",
      "metadata": {},
      "source": [
        "Here is an example with the `Generalisation ROC Curve` and corresponding AUC. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-52-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lists to store TPRs (True Positive Rate), FPRs (False Positive Rare), and AUCs (Area Under the Curve) for each simulation\n",
        "tprs = []\n",
        "fprs = []\n",
        "aucs = []\n",
        "\n",
        "# Calculate ROC curve and AUC for each simulation\n",
        "for i in range(n_simulations):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions[i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    tprs.append(tpr)\n",
        "    fprs.append(fpr)\n",
        "    aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, color='lightgray', lw=1, alpha=0.5)  # Plot each ROC curve faintly\n",
        "\n",
        "# Calculate the mean AUC\n",
        "mean_auc = np.mean(aucs)\n",
        "\n",
        "# Plotting\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title(f'ROC Curve (Mean AUC = {mean_auc:.2f})')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "cgVersion": 1,
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
