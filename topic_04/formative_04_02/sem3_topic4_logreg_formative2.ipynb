{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "cgVersion": 1,
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "ta"
    },
    "language_info": {
      "name": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0-markdown",
      "source": "# Semester 3 Coding Portfolio Topic 4 Formative Part 2/2:\n## Evaluating Logistic Regression Predictions\n\nThis notebook covers the following topics:\n - logistic regression\n\nThis notebook is expected to take around 5 hours to complete.\n\n<b>Formative section</b><br>\nSimply complete the given functions such that they pass the automated tests. This part is graded Pass/Fail; you must get 100% correct!\nYou can submit your notebook through Canvas as often as you like. Make sure to start doing so early to ensure that your code passes all tests!\nYou may ask for help from fellow students and TAs on this section, and solutions might be provided later on.",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-1-code",
      "source": "# Import Necessary Libraries\nimport sys\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.discrete.discrete_model import BinaryResultsWrapper\nimport sklearn\nimport scipy\nfrom scipy.stats import multivariate_normal\nfrom scipy.special import expit as logistic_sigmoid\nfrom packaging import version\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.metrics import balanced_accuracy_score, brier_score_loss, accuracy_score, roc_curve, auc\nfrom sklearn.model_selection import KFold",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "cell-2-code",
      "source": "# These are the recommended (tested) versions of the libraries\n# A separate yaml file is provided for setting up the environment\nassert sys.version_info >= (3, 11), \"This notebook requires Python 3.11 or above.\"\nassert version.parse(pd.__version__) >= version.parse(\"2.3.3\"), \"Needs Pandas >= 2.3.3.\"\nassert version.parse(np.__version__) >= version.parse(\"2.3.4\"), \"Needs NumPy >= 2.3.4.\"\nassert version.parse(sm.__version__) >= version.parse(\"0.14\"), \"Needs Statsmodels >= 0.14.\"\nassert version.parse(matplotlib.__version__) >= version.parse(\"3.10\"), \"Needs Matplotlib >= 3.10.\"\nassert version.parse(sklearn.__version__) >= version.parse(\"1.7\"), \"Needs scikit-learn >= 1.7.\"\nassert version.parse(sns.__version__) >= version.parse(\"0.13\"), \"Needs Seaborn >= 0.13.\"\nassert version.parse(scipy.__version__) >= version.parse(\"1.16\"), \"Needs SciPy >= 1.16.\"",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "cell-3-code",
      "source": "# Set display option to avoid scientific notation in pandas, show up to 5 decimal points\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n# and numpy\nnp.set_printoptions(suppress=True, precision=5)\n\n# Set random seed for reproducibility\nnp.random.seed(42)",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-4-markdown",
      "source": "In this workbook we will be attempting to learn a model of <b>conspiracy spreading tweets</b> for the day of Januray 6th in the US. The model's job is to preemptively identify whether the tweet is likely to be fake-news sharing, without delving into the content of the tweet, but rather using a series of general features. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-5-code",
      "source": "# Load the labeled dataset of tweets \ndf_labs = pd.read_csv('sem3_topic4_logreg_formative2_data.csv', low_memory=False)",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-6-markdown",
      "source": "## Part 1: Data Cleaning & Exploration\n\nYour task is to clean the data. You need to complete the following tasks: ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "id": "cell-7-markdown",
      "source": "### Exercise 1A\nDrop incomplete records ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-8-code",
      "source": "# Drop incomplete records, keep the variable name 'df_labs' for the cleaned dataset\ndf_labs = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-9-markdown",
      "source": "### Exercise 1B \nCreate a dummy variable called `conspiracy_binary`, taking value `1` when the conspiracy-assessment is `Yes`, and `0` otherwise.  \n\nHint: use `.astype(int)` to ensure the results are numbers, not booleans. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-10-code",
      "source": "# Conspiracy spreading flag\nconspiracy_binary = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-11-markdown",
      "source": "Let's have a look at what kinds of tweets we are talking about. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-12-code",
      "source": "# Filter rows where 'conspiracy_binary' is 1\nconspiracy_texts = df_labs.loc[conspiracy_binary == 1, 'text']\n\n# Sample 10 random texts\nrandom_texts = conspiracy_texts.sample(n=10, random_state=np.random.RandomState())\n\n# Iterate through the selected texts and print each one in full\nfor index, text in enumerate(random_texts, start=1):\n    print(f\"Text {index}: {text}\\n\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-13-markdown",
      "source": "### Exercise 1C\nOne-hot encode political ideology (retain just conservative and liberal columns), sentiment (retain just negative and positive columns).\n\nNote: Name the new columns `Political Leanings_Conservative`, `Political Leanings_Liberal`, `Sentiment Analysis_Negative`, and `Sentiment Analysis_Positive`.",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-14-code",
      "source": "# Ideology\npol_lean_one_hot = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "cell-15-code",
      "source": "# Sentiment \nsentiment_one_hot = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-16-markdown",
      "source": "### Exercise 1D\nMake a binary variable indicating if the source of the tweet was an Apple device.\n\nHint: We found 6 different sources associated with Apple. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-17-code",
      "source": "# Apple product\napple_binary = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "cell-18-code",
      "source": "# Lexical diversity \nlexical_diversity_likert = df_labs['Lexical Diversity'].astype(int)\n# Spelling and Grammar \nspelling_grammar_likert = df_labs['Spelling and Grammar Quality'].astype(int)\n# Activity: \nuser_active_num = df_labs['statuses_count'].astype(int)\n# Popularity: \nuser_popular_num = df_labs['followers_count'].astype(int)\n# Tweet Popularity\ntweet_popular_num = df_labs['retweet_count'].astype(int)",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-19-markdown",
      "source": "### Exercise 1E\nOne-hot encode state identifiers, storing the results in a matrix. \nRemember to drop the first dummy (dummy-trap).",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-20-code",
      "source": "# One-hot encode state identifiers\nstates_one_hot = ...\n\n# Filtering to get just the state dummy columns\nstates_matrix = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-21-markdown",
      "source": "### Exercise 1F\nConcatenate the clean variables into a new dataframe called `X`. Exclude the `states_matrix` for now. \nDo not include the outcome (conspiracy binary).\n\nHint: There should be 10 columns.",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-22-code",
      "source": "X = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-23-markdown",
      "source": "### Exercise 1G\nCalculate the correlation matrix across the outcome and X. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-24-code",
      "source": "# Add conspiracy_binary as the first column in X to create a combined DataFrame YX\nX['conspiracy_binary'] = conspiracy_binary\nYX = X[['conspiracy_binary'] + [c for c in X.columns if c != 'conspiracy_binary']]  # Ensure conspiracy_binary is the first column\n\n# Calculate the Correlation Matrix\ncorr = ...\n\n# Plotting\nplt.figure(figsize=(10, 8))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True, fmt=\".2f\", annot_kws={\"size\": 7})\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-25-markdown",
      "source": "## Part 2: Model Assessment and Selection",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "id": "cell-26-markdown",
      "source": "### Exercise 2A \nSet up the full design matrix X, this time include the states_matrix, and a constant. \nFinally bind the outcome to it and ensure it's the first column of the resulting dataframe. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-27-code",
      "source": "# Design matrix\nX = ...\n\n# Add a constant to the feature matrix for statsmodels\nX_const = ...\n\n# Get full dataset together \nYX_const =  ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-28-markdown",
      "source": "## Exercise 2B \nCreate a training set (75%) and test set (25%). \nEnsure the rows of the full dataset selected for each set are chosen at random (use seed 42).",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-29-code",
      "source": "# Split data into train and test (75:25)\nYX_const_train, YX_const_test = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-30-markdown",
      "source": "### Exercise 2C\nUsing a dictionary, define three candidate models in terms of the columns of the design matrix involved in each. \nThe first model should be the homogeneous probability model; the second should have have all covariates except the states; the third should use all the columns. Name the keys `homogeneous`, `no_states`, and `all`.",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-31-code",
      "source": "# Define predictors for each model variant\npredictors = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-32-markdown",
      "source": "### Exercise 2D\nUsing 5-fold cross-validation on the training set, compare the models using the following metrics: Brier score, Accuracy, Balanced Accuracy, and AIC.\n\nFor this question, given we are not at this stage interested in making inference but just understand which model has the best predictive power, you can avoid simulating and simply make point-estimate predictions. \n\nYou can do this by simply fitting the model with sm.Logit, and using directly after the function 'model.predict',avoiding sampling from the approximate posterior of the betas, and then from the posterior predictive of y. \n\nThis will not give you uncertainty estimates around your predictions, but will allow you to compare models based on their point-predictions, and that's good enough for model selection purposes. When we want to make inference, we want to also have access to uncertainty.",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-33-code",
      "source": "y = YX_const_train['Conspiracy Assessment'] # Target variable\n\n# Define K\nK = 5\n\n# Setup the KFold cross-validation\nkf = KFold(n_splits=K, shuffle=True)\n\n# Initialize a dictionary to store Brier scores\nscores = {key: [] for key in predictors}\n\n# Initialize dictionaries to store scores\nbrier_scores = {key: [] for key in predictors}\nacc_scores = {key: [] for key in predictors}  \nbalanced_acc_scores = {key: [] for key in predictors}\naic_scores = {key: [] for key in predictors}  # AIC scores\n\nfor key, cols in predictors.items():\n    \n    for train_index, test_index in kf.split(YX_const_train):\n        \n        # Split into train and test according to the folds \n        X_train, X_test = YX_const_train.iloc[train_index][cols], YX_const_train.iloc[test_index][cols]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n        # For each fold split, fit the model\n        model = ...\n\n        # Predict probabilities\n        y_pred_prob = ...\n\n        # Calculate Brier score\n        brier_score = ...\n        brier_scores[key].append(brier_score)\n\n        # Convert probabilities to binary predictions (assume simple >0.5 probability as threshold)\n        y_pred_binary = ...\n\n        # Calculate Accuracy Score\n        acc_score = ...\n        acc_scores[key].append(acc_score)\n        \n        # Calculate Balanced Accuracy Score\n        bal_acc_score = ...\n        balanced_acc_scores[key].append(bal_acc_score)\n        \n        # Store AIC\n        aic_scores[key].append(model.aic)\n",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "cell-34-code",
      "source": "# Calculate and print the average scores\nresults = []\nfor key in predictors.keys():\n    average_brier_score = np.mean(brier_scores[key])\n    average_bal_acc_score = np.mean(balanced_acc_scores[key])\n    average_acc_score = np.mean(acc_scores[key])\n    average_aic_score = np.mean(aic_scores[key])  # Calculate average AIC\n    results.append({\n        'Model': key,\n        'Average Brier Score': average_brier_score,\n        'Average Accuracy': average_acc_score,\n        'Average Balanced Accuracy': average_bal_acc_score,\n        'Average AIC': average_aic_score\n    })\n\n# Convert results to DataFrame for nicer display\nresults_df = pd.DataFrame(results)\nresults_df",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-35-markdown",
      "source": "### Exercise 2E \nRe-fit the model with the lowest average AIC to the full training set. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-36-code",
      "source": "# Now fit the model to the full training set\nmodel = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "cell-37-code",
      "source": "# Get summary results\nsummary = model.summary()\nprint(summary)",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-38-markdown",
      "source": "## Part 3: Model Evaluation and Estimation of Generalisation Error",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "id": "cell-39-markdown",
      "source": "### Exercise 3A \nGenerate 1000 simulations of the regression coefficients by sampling from the empirical posterior distribution. Use seed 42.\n\nHint: check the documentation of `scipy.stats.multivariate_normal.rvs`",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-40-code",
      "source": "# Extract the coefficients (betas) and their covariance matrix from the logistic regression fit\nbeta_mean = model.params\nbeta_cov = model.cov_params()\n\n# Number of simulations\nn_simulations = 1000\n\n# Simulate beta coefficients\nsimulated_betas = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-41-markdown",
      "source": "### Exercise 3B  \nFor each simulation, generate a predicted probability for the test-set conspiracy assessments. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-42-code",
      "source": "# Initialize an array to store predictions from each simulation\npredictions = np.zeros((n_simulations, YX_const_test.shape[0]))\n\n# Generate predictions for each simulation\nfor i in range(n_simulations):\n    beta_simulation = ...\n    \n    log_odds = ...\n    \n    # Convert log-odds to probabilities\n    probabilities = ...  \n    \n    predictions[i] = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "cell-43-code",
      "source": "predictions",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-44-markdown",
      "source": "For the first 20 assessments in the test-set, we will plot the posterior distirbution of the probabilities, and highlight whether the density of each lies above or below a given `threshold` for classification. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-45-code",
      "source": "true_labels = YX_const_test['Conspiracy Assessment']\n\n# Calculate posterior median and the 90% prediction interval for each of the first 10 observations\nposterior_medians = np.median(predictions, axis=0)\nlower_bounds = np.percentile(predictions, 5, axis=0)\nupper_bounds = np.percentile(predictions, 95, axis=0)\n\n# Plotting with the adjustments for the 90% prediction interval to be shown with red lines\nfig, axes = plt.subplots(4, 5, figsize=(25, 16))\n\nfor i in range(20):\n    ax = axes[i // 5, i % 5]\n    # Histogram of simulated probabilities for observation i\n    ax.hist(predictions[:, i], bins=30, color='skyblue', edgecolor='white', alpha=0.7)\n    \n    # Draw a line for the decision boundary \n    ax.axvline(x=0.5, color='black', linewidth=1, label='Decision Boundary')\n    \n    # Draw a thick solid black line at the true label position\n    true_label_position = 0 if true_labels.iloc[i] == 0 else 1  # Determine the position based on the true label\n    ax.axvline(x=true_label_position, color='black', linewidth=3, label='True Label')\n    \n    # Add posterior median\n    ax.axvline(x=posterior_medians[i], color='red', linestyle='--', label='Posterior Median')\n    \n    # Marking the 90% prediction interval with red lines instead of shading\n    ax.axvline(x=lower_bounds[i], color='red', linestyle='-', linewidth=1, label='90% Prediction Interval' if i == 0 else \"\")\n    ax.axvline(x=upper_bounds[i], color='red', linestyle='-', linewidth=1)\n    \n    ax.set_xlim(-0.1, 1.1)\n    ax.set_title(f'Observation {i+1}')\n    if i == 0:  # Add legend to the first subplot only to avoid repetition\n        ax.legend()\n\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-46-markdown",
      "source": "### Exercise 3C \nSimulate classes (1s or 0s) for the test-set conspiracy assessments, from the posterior predictive distirbution. \n\nHint: check documentation of `np.random.binomial`",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-47-code",
      "source": "# Simulate from the posterior-predictive distirbution \nsimulated_outcomes = ...",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-48-markdown",
      "source": "### Exercise 3D\nCalculate the generalisation error for Classification. \nChoose <b>one</b> classification error metric you wish from the following list: `[Accuracy, Brier Score, AUC]`. The most basic metric we might be interested about is just `accuracy`. \n\nHint: We have 1000 simulated predicted classes. For each of those 1000 sets of simulations of the test-set labels, you need to calculate the accuracy. Then you have to plot the histogram of the accuracies. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-49-code",
      "source": "def plot_histogram(metric_values, metric_name):\n    plt.figure(figsize=(10, 6))\n    plt.hist(metric_values, bins=30, color='skyblue', edgecolor='white')\n    plt.axvline(x=np.median(metric_values), color='red', label='Median')\n    plt.axvline(x=np.percentile(metric_values, 5), color='red', linestyle='--', label='5th percentile')\n    plt.axvline(x=np.percentile(metric_values, 95), color='red', linestyle='--', label='95th percentile')\n    plt.xlabel(metric_name)\n    plt.ylabel('Frequency')\n    plt.title(f'Out-of-Sample Posterior Distribution of {metric_name}')\n    plt.legend()\n    plt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "cell-50-code",
      "source": "# Calculate selected metric for each simulation and plot histogram (choose from Accuracy, Brier Score, AUC)\naccuracies = []\nbriers = []\naucs = []\n",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-51-markdown",
      "source": "Here is an example with the `Generalisation ROC Curve` and corresponding AUC. ",
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "id": "cell-52-code",
      "source": "# Initialize lists to store TPRs (True Positive Rate), FPRs (False Positive Rare), and AUCs (Area Under the Curve) for each simulation\ntprs = []\nfprs = []\naucs = []\n\n# Calculate ROC curve and AUC for each simulation\nfor i in range(n_simulations):\n    fpr, tpr, thresholds = roc_curve(true_labels, predictions[i])\n    roc_auc = auc(fpr, tpr)\n    tprs.append(tpr)\n    fprs.append(fpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, color='lightgray', lw=1, alpha=0.5)  # Plot each ROC curve faintly\n\n# Calculate the mean AUC\nmean_auc = np.mean(aucs)\n\n# Plotting\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(f'ROC Curve (Mean AUC = {mean_auc:.2f})')\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    }
  ]
}