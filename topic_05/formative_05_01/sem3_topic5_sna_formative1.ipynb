{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0-markdown",
   "metadata": {},
   "source": [
    "# Semester 3 Coding Portfolio Topic 5 Formative Part 1/2:\n",
    "# Social network analysis 1: Create a network from text data and analyze it\n",
    "\n",
    "This notebook covers the following topics:\n",
    " - Extracting interaction data\n",
    " - Creating networkx graphs\n",
    " - Exporting for visual network analysis\n",
    "\n",
    "This notebook is expected to take around 5 hours to complete:\n",
    " - 2 hours for the formative part\n",
    " - 3 hours of self-study on the topics covered by this notebook\n",
    "\n",
    "Like all topics in this portfolio, this topic is split into two sections:\n",
    " - Formative \n",
    " - Summative\n",
    "\n",
    "<b>Formative section</b><br>\n",
    "Simply complete the given functions such that they pass the automated tests. This part is graded Pass/Fail; you must get 100% correct!\n",
    "You can submit your notebook through Canvas as often as you like. Make sure to start doing so early to insure that your code passes all tests!\n",
    "You may ask for help from fellow students and TAs on this section, and solutions might be provided later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1-markdown",
   "metadata": {},
   "source": [
    "In this workshop, we will use networkx to create a network from text data, open it in Gephi, and then use networkx to analyze its structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2-markdown",
   "metadata": {},
   "source": [
    "### Part 1. Extract interactions from textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3-markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will go from data in a dataframe to creating a network that we can analyze, and that we can open with Gephi.\n",
    "\n",
    "In your own project, you are unlikely to come across ready-to-process data in a format that you can open with networkx or Gephi. Instead, you need to create network from your own data.\n",
    "\n",
    "In this case, we will look at a classic dataset: the Enron emails. The Enron email dataset is a large collection of email data from mostly senior management of the Enron Corporation. It was made public by the Federal Energy Regulatory Commission during its investigation after Enron's collapse in 2001. The dataset contains roughly half a million emails organized into folders. It's one of the largest datasets of real-world emails available for public study and has been widely used for research in various fields like social network analysis, natural language processing, and machine learning.\n",
    "\n",
    "The Enron dataset is particularly notable for its insights into corporate communications and behaviors, and it has played a significant role in advancing the study of information processing and management. Due to its real-world nature, including the variety of topics and the informal style of communication, the dataset presents unique challenges and opportunities for data analysis and has become a benchmark in the study of electronic communications.\n",
    "\n",
    "We will here use the dataset to create a social network of emails between individuals, and we will analyze the structure of this network.\n",
    "\n",
    "The Enron case led to the conviction of two managers: Kenneth Lay (kenneth.lay@enron.com) and Jeffrey Skilling (jeffreyskilling@yahoo.com). We will examine the centrality of these nodes in the network, and identify the community to which they belong. \n",
    "\n",
    "To make the analysis more feasible, we will focus on emails in December 2001 - the month when the company went under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Install all necessary packages first to avoid kernel restart\n",
    "# This ensures all dependencies are available before running the notebook\n",
    "!pip3 install networkx python-louvain pandas matplotlib -q\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import community as community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Message-ID: &lt;7391389.1075855378477.JavaMail.ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Message-ID: &lt;8572706.1075855378498.JavaMail.ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Message-ID: &lt;32300323.1075855378519.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message\n",
       "0  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2  Message-ID: <7391389.1075855378477.JavaMail.ev...\n",
       "3  Message-ID: <8572706.1075855378498.JavaMail.ev...\n",
       "4  Message-ID: <32300323.1075855378519.JavaMail.e..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sem3_topic5_sna_formative1_data.csv.gz')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6-markdown",
   "metadata": {},
   "source": [
    "As you can see, the email is just a long text message. We need to parse out the actual data and email addresses from the text using regexps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <24743962.1075858729372.JavaMail.evans@thyme>\n",
      "Date: Tue, 23 Oct 2001 19:27:42 -0700 (PDT)\n",
      "From: expediatraveldeals_029715@expedia.customer-email.com\n",
      "To: rshapiro@enron.com\n",
      "Subject: Get away -- and save up to 30% on your vacation!\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=ANSI_X3.4-1968\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Expedia Travel Deals <ExpediaTravelDeals_029715@expedia.customer-email.com>\n",
      "X-To: rshapiro@enron.com\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\RSHAPIRO (Non-Privileged)\\Shapiro, Richard\\Deleted Items\n",
      "X-Origin: Shapiro-R\n",
      "X-FileName: RSHAPIRO (Non-Privileged).pst\n",
      "\n",
      " <http://www.expedia.com/default.asp?rfrr=-1412>\t \n",
      "\thome <http://www.expedia.com/pubspec/scripts/eap.asp?GOTO=HOME&rfrr=-1412>flights <http://www.expedia.com/pubspec/scripts/eap.asp?GOTO=FLIGHTLAUNCH&rfrr=-1412>hotels <http://www.expedia.com/pubspec/scripts/eap.asp?GOTO=HOTLAUNCH&rfrr=-1412>cars <http://www.expedia.com/pub/agent.dll?qscr=carw&rfrr=-1412>vacations <http://www.expedia.com/pubspec/scripts/eap.asp?GOTO=PKGLAUNCH&rfrr=-1412>cruises <http://www.expedia.com/pubspec/scripts/eap.asp?GOTO=CRUISELAUNCH&rfrr=-1412>deals <http://www.expedia.com/pubspec/scripts/eap.asp?GOTO=DEALS&rfrr=-1412>guides <http://www.expedia.com/daily/guides/default.asp?rfrr=-1412>maps <http://www.expedia.com/pubspec/scripts/eap.asp?GOTO=MAPS&rfrr=-1412>business <http://www.expedia.com/pubspec/scripts/eap.asp?GOTO=BIZWIZ&rfrr=-1412>\t \t\n",
      "\tsite map <Http://www.expedia.com/daily/sitetour/default.asp?rfrr=-1412>\t my trips <Http://www.expedia.com/pub/agent.dll?qscr=litn&rfrr=-1412>\t my profile <Http://www.expedia.com/pub/agent.dll?qscr=info&rfrr=-1412>\t customer support <Http://www.expedia.com/daily/service/default.asp?rfrr=-1412>\t \n",
      "\t\n",
      "\t\n",
      "CONTENTS\n",
      "\n",
      "\t  <http://www.expedia.com/daily/common/images/dot2.gif>\t\tFind a Flight \t\n",
      "\t  <http://www.expedia.com/daily/common/images/dot2.gif>\t\tTake a Vacation \t\n",
      "\t\t\n",
      "\t\t\n",
      "\t  <http://www.expedia.com/daily/common/images/dot2.gif>\t\tReserve A Room \t\n",
      "\t\t\n",
      "\t  <http://www.expedia.com/daily/common/images/dot2.gif>\t\tPlan A Cruise \t\n",
      "\t\t\n",
      "\t  <http://www.expedia.com/daily/common/images/dot2.gif>\t\tRent A Car \t\n",
      "\t\t\n",
      "\t  <http://www.expedia.com/daily/common/images/dot2.gif>\t\tSee What's New \t\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t\n",
      "\n",
      "\n",
      "Get a FREE lift ticket! <http://expedia.cq0.net/r/default.asp?R=0000199af000c5399&rfrr=-1412> \t\n",
      "\t\t\n",
      "\n",
      "\n",
      " <http://expedia.cq0.net/r/default.asp?R=0000193d0000c5399&url=http://www%2Eexpedia%2Ecom/daily/advert/EmbassySuites/default%2EaspI=http://ads%2Emsn%2Ecom/ads/EXPFTR/001295900089_SM%2Egif> \t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  <Http://www.expedia.com/daily/common/images/spaceit.gif>\t\n",
      "\n",
      "\t\n",
      "Dear Richard,\n",
      "\n",
      "\n",
      " <http://expedia.cq0.net/r/default.asp?R=0000199ab000c5399&qscr=cmsh&rfrr=-1410> If you're ready to get away, Expedia Vacations <http://expedia.cq0.net/r/default.asp?R=0000199ab000c5399&qscr=cmsh&rfrr=-1410> can save you up to 30% on your trip. We'll help you create your perfect customized vacation -- and save you money at the same time. Choose the exact flight, hotel, and rental car you want, then add activities like passes to Universal Orlando, a rafting expedition, or a sunset dinner cruise. It's everything you want in a vacation, and nothing you don't. Plus, booking is easy and hassle-free. \n",
      "  <http://expedia.com/daily/common/spaceit.gif>  <http://www.expedia.com/daily/common/images/dot2.gif>Book your ultimate vacation -- and save up to 30% -- now <http://expedia.cq0.net/r/default.asp?R=0000199ab000c5399&qscr=cmsh&rfrr=-1410>\n",
      "  <http://expedia.com/daily/common/spaceit.gif>  <http://www.expedia.com/daily/common/images/dot2.gif>Learn more about Expedia Vacations <http://expedia.cq0.net/r/default.asp?R=00001995b000c5399&rfrr=-1410> \n",
      "\n",
      "\t\n",
      "  <http://www.expedia.com/daily/tripguide/edge_left.gif>\tFIND A FLIGHT\t  <http://www.expedia.com/daily/tripguide/edge_right.gif>\t\n",
      "\t\n",
      "Save 10% on British Airways flights to Europe <http://expedia.cq0.net/r/default.asp?R=0000193b8000c5399&rfrr=-1412>\n",
      "Save on flights from 21 U.S. cities. Book by November 12. \n",
      "\n",
      "Save 15%: Fly Qantas to Australia and the South Pacific <http://expedia.cq0.net/r/default.asp?R=000019378000c5399&rfrr=-1412>\n",
      "Get away on an exotic adventure! Book by November 15. \n",
      "\n",
      "Save up to 60% on your flight with Expedia Bargain Fares <http://expedia.cq0.net/r/default.asp?R=0000193e6000c5399&rfrr=-1412>\n",
      "Great everyday savings, only on Expedia.com. \n",
      "\n",
      "Fare sales: See the latest ways to save <http://expedia.cq0.net/r/default.asp?R=0000193d2000c5399&rfrr=-1412>\n",
      "We've got deals on cross-country flights, Vegas flights, and more. \n",
      "\n",
      "\n",
      "\t\n",
      "  <http://www.expedia.com/daily/tripguide/edge_left.gif>\tTAKE A VACATION\t  <http://www.expedia.com/daily/tripguide/edge_right.gif>\t\n",
      "\t\n",
      "Boston: Three crisp, cool fall nights from $448 <http://expedia.cq0.net/r/default.asp?R=000019379000c5399&GOTO=PKGDEALDETAILS&DispSection=33&DestId=178239&rfrr=-1412>\n",
      "Stay at the four-star Westin Copley Place, in the heart of the historic Back Bay. \n",
      "\n",
      "Miami luxury deal: Three four-star nights from $423 <http://expedia.cq0.net/r/default.asp?R=0000193ba000c5399&GOTO=PKGDEALDETAILS&DispSection=32&DestId=178286&rfrr=-1412>\n",
      "Pamper yourself at the Ritz-Carlton on beautiful Key Biscayne. \n",
      "\n",
      "London: Air and four nights near Hyde Park from $648 <http://expedia.cq0.net/r/default.asp?R=0000193ba000c5399&GOTO=PKGDEALDETAILS&DispSection=29&DestId=178279&rfrr=-1412>\n",
      "Stay at the elegant, four-star Cumberland Hotel, in the heart of the city. \n",
      "\n",
      "See all our top vacation package values! <http://expedia.cq0.net/r/default.asp?R=0000193ee000c5399&rfrr=-1412> \n",
      "\n",
      "Package rates are per person (double occupancy), subject to availability, and vary by departure city. \n",
      "\n",
      "\n",
      "\t\n",
      "  <http://www.expedia.com/daily/tripguide/edge_left.gif>\tRESERVE A ROOM\t  <http://www.expedia.com/daily/tripguide/edge_right.gif>\t\n",
      "\t\n",
      "Chateau Sonesta New Orleans from $193 until November 13 <http://expedia.cq0.net/r/default.asp?R=000019379000c5399&GOTO=HotDetails&HotelID=14429&numadult=1&merchtext=Special+Deal!+Enjoy+rates+from+$193+per+night+for+stays+between+now+and+November+13%2E+Book+today+and+save!&rfrr=-1412>\n",
      "A grand French Quarter hotel, just steps from the business district. \n",
      "\n",
      "Miami: The Beacon Hotel South Beach from $185 until November 30 <http://expedia.cq0.net/r/default.asp?R=000019379000c5399&GOTO=HotDetails&HotelID=17764&numadult=1&merchtext=Special+Deal!+Enjoy+rates+from+$185+per+night+for+stays+between+now+and+November+30%2E+Book+today+and+save!&rfrr=-1412>\n",
      "A four-star, art deco, beachfront oasis with the best location in town. \n",
      "\n",
      "Save an additional 25% off Wyndham/ESR hotels everywhere <http://expedia.cq0.net/r/default.asp?R=00001937a000c5399&rfrr=-1412>\n",
      "The rates are already low, and now you'll save even more! \n",
      "\n",
      "Pssst! See our best hotel deals! <http://expedia.cq0.net/r/default.asp?R=0000196c3000c5399&rfrr=-1412> \n",
      "\n",
      "\n",
      "\t\n",
      "  <http://www.expedia.com/daily/tripguide/edge_left.gif>\tPLAN A CRUISE\t  <http://www.expedia.com/daily/tripguide/edge_right.gif>\t\n",
      "\t\n",
      "Four-day Bahamas cruise (Miami roundtrip) from $159 <http://expedia.cq0.net/r/default.asp?R=0000193d6000c5399&d=&i=776001&c=44&v=220&rfrr=-1412>\n",
      "See Nassau, Key West, and Cococay at a great low price. \n",
      "\n",
      "Seven-day western Caribbean cruise (New Orleans roundtrip) from $399 <http://expedia.cq0.net/r/default.asp?R=0000193d6000c5399&d=11/01/2001&i=744837&c=1&v=41&rfrr=-1412>\n",
      "Kick back and relax in Jamaica, Grand Cayman, Cozumel, and Playa del Carmen. \n",
      "\n",
      "Eight-day eastern Caribbean cruise (Fort Lauderdale roundtrip) from $549 <http://expedia.cq0.net/r/default.asp?R=0000193d6000c5399&d=&i=792157&c=24&v=120&rfrr=-1412>\n",
      "See five exotic islands, including St. Thomas, St. John, and St. Maarten. \n",
      "\n",
      "See all our top cruise deals <http://expedia.cq0.net/r/default.asp?R=0000193be000c5399&rfrr=-1412>! \n",
      "\n",
      "\t\n",
      "  <http://www.expedia.com/daily/tripguide/edge_left.gif>\tRENT A CAR\t  <http://www.expedia.com/daily/tripguide/edge_right.gif>\t\n",
      "\t\n",
      "Rent a car for $24 a day or $109 a week! <http://expedia.cq0.net/r/default.asp?R=00001937b000c5399&rfrr=-1412>\n",
      "Get away and relax this weekend. SUVs are on sale, too. \n",
      "\n",
      "\n",
      "\t\n",
      "  <http://www.expedia.com/daily/tripguide/edge_left.gif>\tSEE WHAT'S NEW ON EXPEDIA.COM\t  <http://www.expedia.com/daily/tripguide/edge_right.gif>\t\n",
      "\t\n",
      "Expedia Recommends: London <http://expedia.cq0.net/r/default.asp?R=0000193ef000c5399&rfrr=-1412>\n",
      "See what's new in London's best museums, theatres, restaurants, and more. \n",
      "\n",
      "Boston: Exploring the Freedom Trail with the kids <http://expedia.cq0.net/r/default.asp?R=000019630000c5399&rfrr=-1412>\n",
      "The best, most memorable history lessons are firsthand ones. \n",
      "\n",
      "Expedia Recommends: San Diego <http://expedia.cq0.net/r/default.asp?R=000019662000c5399&rfrr=-1412>\n",
      "Plan a trip -- the zoo, Old Town, Legoland, and the Hotel del are waiting! \n",
      "\n",
      "\n",
      "  <http://www.expedia.com/daily/tripguide/spaceit.gif>\t\n",
      "\n",
      "\n",
      "Thank you for traveling with Expedia.com. \n",
      "\n",
      "Expedia.com Travel Team\n",
      "\n",
      "Don't just travel. Travel Right.TM\n",
      "\n",
      " <http://expedia.cq0.net/r/default.asp?R=00001969d000c5399&rfrr=-1412>Expedia.com\n",
      "\n",
      "  <http://expedia.com/daily/faretracker/images/gold_rule3.gif>FROM OUR SPONSORS:\n",
      "\n",
      "Step Outside and Experience Chicago <http://expedia.cq0.net/r/default.asp?R=0000193d0000c5399&url=http://www%2Eexpedia%2Ecom/daily/advert/EmbassySuites/default%2EaspI=http://ads%2Emsn%2Ecom/ads/EXPFTR/001295900090_LG%2Egif>\n",
      "\n",
      "The Embassy Suites Hotel Chicago Downtown - Lakefront puts you in the middle of it all. The newest Embassy Suites Hotel is just two blocks from both Navy Pier and Michigan Avenue. \n",
      "\n",
      " <http://expedia.cq0.net/r/default.asp?R=0000193d0000c5399&url=http://www%2Eexpedia%2Ecom/daily/advert/EmbassySuites/default%2EaspI=http://ads%2Emsn%2Ecom/ads/EXPFTR/001295900090_LG%2Egif>   <http://expedia.com/daily/common/spaceit.gif>\n",
      "\n",
      "  _____  \n",
      "\n",
      "Unless otherwise noted, all prices quoted are in U.S. dollars. Prices subject to change without notice. \n",
      "\n",
      "Photo: Darryl Torckler/Stone \n",
      "\n",
      "If you do not wish to receive any further promotional e-mails from us, please reply to this mail with \"unsubscribe\" in the subject line. \n",
      "\n",
      "Questions about your privacy? Read our privacy statement <http://expedia.cq0.net/r/default.asp?R=00001969c000c5399&rfrr=-1412>. If you have other questions, visit the Expedia.com Customer Support Center <http://expedia.cq0.net/r/default.asp?R=000019634000c5399&rfrr=-1412>. If you'd like to give us feedback, click here <http://expedia.cq0.net/r/default.asp?R=0000199ab000c5399&qscr=fbak&itid=0&zz=974317886920&rfrr=-1412>. \n",
      "\n",
      "?2001 Expedia, Inc. Expedia, Expedia.com, the Airplane logo, and \"Don't just travel. Travel Right.\" are either registered trademarks or trademarks of Expedia, Inc. in the United States and/or other countries. All rights reserved. Other company and product names mentioned herein may be trademarks of their respective owners. Expedia terms of use <http://expedia.cq0.net/r/default.asp?R=00001966a000c5399&rfrr=-1412>. \n",
      "\n",
      "This email was sent to: rshapiro@enron.com\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(1).message.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(message):\n",
    "    # Regular expression pattern to match the subject line\n",
    "    pattern = r'^Subject: (.+)$'\n",
    "\n",
    "    # Search for the pattern in the message, using multiline mode\n",
    "    match = re.search(pattern, message, re.MULTILINE)\n",
    "\n",
    "    # Return the subject if a match is found, else return None\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Example: this function extracts a date from the email\n",
    "def extract_date(message):\n",
    "    match = re.search(r\"Date: .*?, (\\d+ \\w+ \\d{4})\", message)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Extract and convert date\n",
    "df['date'] = df['message'].apply(extract_date)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce', format='%d %b %Y')\n",
    "\n",
    "df['subject'] = df['message'].apply(extract_subject)\n",
    "\n",
    "#Your task is to extract from and to email, and to put these as new columns \"from_email\" and \"to_email\"\n",
    "# Regular expression patterns for \"From\", \"To\" email addresses. We ignore CC here for simplicity\n",
    "from_pattern = r\"From: (\\S+@\\S+)\"\n",
    "to_pattern = r\"To: ([\\w\\.,@]+)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9-markdown",
   "metadata": {},
   "source": [
    "### Exercise 1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_addresses(dataframe):\n",
    "    \"\"\"Extract from and to email addresses from message column and add them as new columns.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: The dataframe containing the message column with email data.\n",
    "    \n",
    "    Returns:\n",
    "        Filtered dataframe with from_email and to_email columns containing only valid emails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO 1: Write functions to extract from_email and to_email from the message text using regex patterns\n",
    "    \n",
    "    # Define a regex pattern to validate email addresses\n",
    "    # This pattern ensures emails have the format: username@domain.extension\n",
    "    # It checks for alphanumeric characters, dots, underscores, plus signs, and hyphens\n",
    "    email_validation_pattern = re.compile(r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\")\n",
    "    \n",
    "    # Helper function to extract \"From\" email address from a message\n",
    "    # Uses the from_pattern regex to find the email after \"From: \"\n",
    "    def extract_from_email(message):\n",
    "        if pd.isna(message):  # Check if message is NaN\n",
    "            return None\n",
    "        match = re.search(from_pattern, message)  # Search for the pattern in the message\n",
    "        if match:\n",
    "            email = match.group(1)  # Extract the first captured group (the email)\n",
    "            # Validate the email format - only return if it matches valid email pattern\n",
    "            if email_validation_pattern.match(email):\n",
    "                return email\n",
    "        return None\n",
    "    \n",
    "    # Helper function to extract \"To\" email address from a message\n",
    "    # The \"To\" field might contain multiple emails or be formatted differently\n",
    "    def extract_to_email(message):\n",
    "        if pd.isna(message):  # Check if message is NaN\n",
    "            return None\n",
    "        match = re.search(to_pattern, message)  # Search for the pattern in the message\n",
    "        if match:\n",
    "            email_str = match.group(1)  # Extract the first captured group\n",
    "            \n",
    "            # The \"To\" field might contain multiple emails separated by commas\n",
    "            # Split by comma and take the first email address\n",
    "            emails = [e.strip() for e in email_str.split(',')]\n",
    "            \n",
    "            # Try to find a valid email in the list\n",
    "            for email in emails:\n",
    "                # Clean up the email (remove any extra whitespace)\n",
    "                email = email.strip()\n",
    "                # Validate the email format\n",
    "                if email_validation_pattern.match(email):\n",
    "                    return email\n",
    "        return None\n",
    "    \n",
    "    # Apply the extraction functions to each message in the dataframe\n",
    "    # This creates new columns 'from_email' and 'to_email' with the extracted addresses\n",
    "    dataframe['from_email'] = dataframe['message'].apply(extract_from_email)\n",
    "    dataframe['to_email'] = dataframe['message'].apply(extract_to_email)\n",
    "    \n",
    "    # Filter the dataframe to only keep rows where both from_email and to_email are valid (not None)\n",
    "    # This ensures we only work with complete email pairs\n",
    "    dataframe = dataframe.dropna(subset=['from_email', 'to_email'])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "df = extract_email_addresses(df)\n",
    "\n",
    "# BEGIN TESTS\n",
    "# @name(\"email_columns_created\")\n",
    "# @description(\"Check that email extraction created valid email columns\")\n",
    "def test_email_columns_created():\n",
    "    assert 'from_email' in df.columns, \"Missing column from_email\"\n",
    "    assert 'to_email' in df.columns, \"Missing column to_email\"\n",
    "    assert df['from_email'].notna().any(), \"No from_email values found\"\n",
    "    assert df['to_email'].notna().any(), \"No to_email values found\"\n",
    "test_email_columns_created()\n",
    "\n",
    "# @name(\"email_format_valid\")\n",
    "# @description(\"Check that all emails match valid email pattern after filtering\")\n",
    "def test_email_format_valid():\n",
    "    email_re = re.compile(r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\")\n",
    "    # All emails should match the valid email pattern (filtering was applied)\n",
    "    assert df['from_email'].dropna().apply(lambda x: bool(email_re.match(x))).all(), \"Invalid from_email found - filtering failed\"\n",
    "    assert df['to_email'].dropna().apply(lambda x: bool(email_re.match(x))).all(), \"Invalid to_email found - filtering failed\"\n",
    "test_email_format_valid()\n",
    "\n",
    "# @name(\"dataframe_filtered\")\n",
    "# @description(\"Check that DataFrame has valid data after filtering\")\n",
    "def test_dataframe_filtered():\n",
    "    assert 245000 <=len(df) <= 246000, \"DataFrame is not filtered correctly\"\n",
    "test_dataframe_filtered()\n",
    "# END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11-markdown",
   "metadata": {},
   "source": [
    "### Exercise 1B\n",
    "\n",
    "Next step: Now that we have the date and the from and to emails, we want to aggregate on the from and to, so that we get the number of emails sent between the two emails.\n",
    "\n",
    "The result should be a dataframe with columns:\n",
    "\n",
    "from_email | to_email  | count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  from_email                  to_email  count\n",
      "0             --migrated--bmishkin@ercot.com      mockmarket@ercot.com      1\n",
      "1                --migrated--dodle@ercot.com             set@ercot.com      1\n",
      "2                         -nikole@excite.com   bill.williams@enron.com      8\n",
      "3        -persson@ricemail.ricefinancial.com  barry.tycholiz@enron.com      5\n",
      "4      01019@salespoint.dealerconnection.com     jason.wolfe@enron.com      4\n",
      "...                                      ...                       ...    ...\n",
      "46933                     zufferli@enron.com   john.lavorato@enron.com      2\n",
      "46934                 zulie.flores@enron.com   marla.barnard@enron.com      3\n",
      "46935                    zwharton@dawray.com   martin.cuilla@enron.com      6\n",
      "46936                    zwharton@dawray.com         mcuilla@enron.com      3\n",
      "46937                       zzmacmac@aol.com      j.kaminski@enron.com      2\n",
      "\n",
      "[46938 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_email_edges(dataframe):\n",
    "    \"\"\"Aggregate emails by from_email and to_email to count the number of emails between each pair.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: The dataframe containing from_email and to_email columns.\n",
    "    \n",
    "    Returns:\n",
    "        A dataframe with columns: from_email, to_email, count\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO 2: Group by from_email and to_email, then count occurrences\n",
    "    \n",
    "    # Group the dataframe by 'from_email' and 'to_email' columns\n",
    "    # This groups all rows that have the same sender-receiver pair together\n",
    "    grouped = dataframe.groupby(['from_email', 'to_email'])\n",
    "    \n",
    "    # Count the number of emails in each group using the size() method\n",
    "    # This gives us the number of emails sent from each 'from_email' to each 'to_email'\n",
    "    # The result is a Series with a MultiIndex (from_email, to_email) and values as counts\n",
    "    email_counts = grouped.size()\n",
    "    \n",
    "    # Reset the index to convert the MultiIndex back into regular columns\n",
    "    # This creates a DataFrame with columns: from_email, to_email, and the count (default name is 0)\n",
    "    edges = email_counts.reset_index()\n",
    "    \n",
    "    # Rename the count column to 'count' for clarity\n",
    "    # The default name after reset_index() is 0, so we rename it\n",
    "    edges = edges.rename(columns={0: 'count'})\n",
    "    \n",
    "    return edges\n",
    "\n",
    "email_edges = create_email_edges(df)\n",
    "print(email_edges)\n",
    "\n",
    "# BEGIN TESTS\n",
    "# @name(\"email_edges_structure\")\n",
    "# @description(\"Check that email_edges DataFrame has correct structure\")\n",
    "def test_email_edges_structure():\n",
    "    assert isinstance(email_edges, pd.DataFrame), \"email_edges should be a DataFrame\"\n",
    "    expected_cols = {'from_email', 'to_email', 'count'}\n",
    "    assert expected_cols.issubset(email_edges.columns), f\"Missing columns. Expected {expected_cols}, got {set(email_edges.columns)}\"\n",
    "test_email_edges_structure()\n",
    "\n",
    "# @name(\"email_edges_aggregation_correctness\")\n",
    "# @description(\"Check that aggregation produced valid counts without duplicates\")\n",
    "def test_email_edges_aggregation_correctness():\n",
    "    # Counts should be positive integers\n",
    "    assert pd.api.types.is_numeric_dtype(email_edges['count']), \"Column 'count' should be numeric\"\n",
    "    assert (email_edges['count'] > 0).all(), \"All counts should be positive\"\n",
    "    # No duplicate from-to pairs (proper grouping)\n",
    "    assert not email_edges[['from_email', 'to_email']].duplicated().any(), \"Should not have duplicate from_email-to_email pairs (groupby failed)\"\n",
    "test_email_edges_aggregation_correctness()\n",
    "\n",
    "# @name(\"email_edges_count_values\")\n",
    "# @description(\"Check that count values are correct\")\n",
    "def test_email_edges_count_values():\n",
    "    # Total count in email_edges should be less than or equal to original df length\n",
    "    assert 46900 <= len(email_edges) <= 47000, \"email_edges DataFrame does not have enough rows\"\n",
    "    assert email_edges['count'].sum() <= len(df), \"Sum of counts should not exceed original dataframe length\"\n",
    "test_email_edges_count_values()\n",
    "# END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13-markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create networkx graph from data \n",
    "In the next step, we're going to create a networkx graph from this network, and save to have a look at the graph in Gephi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now it's time to create the network! \n",
    "G = nx.Graph()\n",
    "\n",
    "# Two people were convicted in the Enron scandal: Kenneth Lay, and Jeffrey Skilling. \n",
    "# The nodes should have a property 'convicted'. If the email address is either 'kenneth.lay@enron.com' or 'jeffreyskilling@yahoo.com', convicted == True, otherwise False.\n",
    "\n",
    "# Use G.add_node(id, additional_data=\"data\") to add nodes\n",
    "# Use G.add_edge(from_id,to_id, weight=5) to add edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15-markdown",
   "metadata": {},
   "source": [
    "### Exercise 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-16-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(graph, edges_df):\n",
    "    \"\"\"Build a network graph from email edges with convicted attribute for specific nodes.\n",
    "    \n",
    "    Args:\n",
    "        graph: Empty NetworkX graph object.\n",
    "        edges_df: DataFrame with columns from_email, to_email, count.\n",
    "    \n",
    "    Returns:\n",
    "        Graph with nodes (having convicted attribute) and weighted edges.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO 3: Create nodes with 'convicted' attribute and add edges with weights\n",
    "    \n",
    "    # Define the email addresses of the two convicted executives from the Enron scandal\n",
    "    # These are the individuals we want to mark with the 'convicted' attribute\n",
    "    convicted_emails = {'kenneth.lay@enron.com', 'jeffreyskilling@yahoo.com'}\n",
    "    \n",
    "    # First, collect all unique email addresses that appear in either from_email or to_email columns\n",
    "    # This gives us all the nodes (people) in our network\n",
    "    all_emails = set(edges_df['from_email'].unique()) | set(edges_df['to_email'].unique())\n",
    "    \n",
    "    # Add each email address as a node in the graph\n",
    "    # For each node, we set the 'convicted' attribute:\n",
    "    # - True if the email is one of the convicted executives\n",
    "    # - False otherwise\n",
    "    for email in all_emails:\n",
    "        is_convicted = email in convicted_emails  # Check if this email belongs to a convicted executive\n",
    "        graph.add_node(email, convicted=is_convicted)  # Add node with the convicted attribute\n",
    "    \n",
    "    # Now add edges between nodes based on the email interactions\n",
    "    # Iterate through each row in the edges_df DataFrame\n",
    "    for _, row in edges_df.iterrows():\n",
    "        from_email = row['from_email']  # Get the sender's email\n",
    "        to_email = row['to_email']      # Get the receiver's email\n",
    "        weight = row['count']           # Get the number of emails (this becomes the edge weight)\n",
    "        \n",
    "        # Add an edge between from_email and to_email with the count as the weight\n",
    "        # The weight represents how many emails were exchanged between these two people\n",
    "        # NetworkX will automatically create the edge if it doesn't exist, or update it if it does\n",
    "        graph.add_edge(from_email, to_email, weight=weight)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "G = build_network(G, email_edges)\n",
    "\n",
    "# BEGIN TESTS\n",
    "# @name(\"graph_has_nodes\")\n",
    "# @description(\"Check that graph has correct number of nodes\")\n",
    "def test_graph_has_nodes():\n",
    "    assert G.number_of_nodes() >= 20000, \"Graph does not have correct number of nodes\"\n",
    "test_graph_has_nodes()\n",
    "\n",
    "# @name(\"graph_has_edges\")\n",
    "# @description(\"Check that graph has correct number of edges\")\n",
    "def test_graph_has_edges():\n",
    "    assert G.number_of_edges() > 30000, \"Graph does not have enough edges\"\n",
    "test_graph_has_edges()\n",
    "\n",
    "# @name(\"convicted_attribute_exists\")\n",
    "# @description(\"Check that nodes have convicted attribute\")\n",
    "def test_convicted_attribute_exists():\n",
    "    sample_node = list(G.nodes())[0]\n",
    "    assert 'convicted' in G.nodes[sample_node], \"Nodes missing 'convicted' attribute\"\n",
    "test_convicted_attribute_exists()\n",
    "\n",
    "# @name(\"convicted_values\")\n",
    "# @description(\"Check that nodes have correct convicted attribute values\")\n",
    "def test_convicted_values():\n",
    "    kenneth_exists = 'kenneth.lay@enron.com' in G.nodes()\n",
    "    jeffrey_exists = 'jeffreyskilling@yahoo.com' in G.nodes()\n",
    "    if kenneth_exists:\n",
    "        assert G.nodes['kenneth.lay@enron.com']['convicted'] == True, \"kenneth.lay@enron.com should have convicted=True\"\n",
    "    if jeffrey_exists:\n",
    "        assert G.nodes['jeffreyskilling@yahoo.com']['convicted'] == True, \"jeffreyskilling@yahoo.com should have convicted=True\"\n",
    "    \n",
    "    # Check that only 2 executives are marked as convicted\n",
    "    convicted_count = sum(1 for node in G.nodes() if G.nodes[node].get('convicted', False))\n",
    "    assert convicted_count == 2, \"There should be exactly 2 convicted nodes\"\n",
    "test_convicted_values()\n",
    "\n",
    "# @name(\"edge_weights\")\n",
    "# @description(\"Check that edges have weight attribute\")\n",
    "def test_edge_weights():\n",
    "    sample_edge = list(G.edges(data=True))[0]\n",
    "    assert 'weight' in sample_edge[2], \"Edges missing 'weight' attribute\"\n",
    "test_edge_weights()\n",
    "# END TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-17-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20157"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-18-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try to plot it - but you might find that the network is too big and that your python loads forever!\n",
    "\n",
    "# # When we have our network, we can plot it\n",
    "# # nx.draw(G) plots - but not very pretty\n",
    "# # The following code is more customizable \n",
    "\n",
    "# def plot_network(G):\n",
    "#     # Use a layout that spreads out the nodes, like spring_layout\n",
    "#     pos = nx.spring_layout(G, scale=2)\n",
    "    \n",
    "#     # Draw the network\n",
    "#     plt.figure(figsize=(8, 8))  # Large figure size\n",
    "    \n",
    "#     #We plot the convicted individuals with red.\n",
    "#     node_color = ['red' if G.nodes[node].get('convicted', False) else 'blue' for node in G]\n",
    "#     nx.draw_networkx_nodes(G, pos, node_size=5, node_color=node_color)\n",
    "#     nx.draw_networkx_edges(G, pos, width=0.1, alpha=0.5)\n",
    "    \n",
    "#     # Set plot limits to avoid cutting off nodes\n",
    "#     plt.xlim([-1, 1])\n",
    "#     plt.ylim([-1, 1])\n",
    "    \n",
    "#     # Turn off axis\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     # Show plot\n",
    "#     plt.show()\n",
    "# plot_network(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19-markdown",
   "metadata": {},
   "source": [
    "### Exercise 2B: Getting the giant component \n",
    "As you can see, the resulting network has a lot of small components that are not connected to the larger network. Often it makes sense to focus only on the giant component: that is, the biggest connected component in the network.\n",
    "\n",
    "To get the components of the graph, use nx.connected_components(G)\n",
    "\n",
    "Remember that the giant component is the largest component.\n",
    "\n",
    "Once you've identified it, you can use G.subgraph(component) to turn the nodes into a graph\n",
    "\n",
    "Put the result back in the variable G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-20-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_giant_component(graph):\n",
    "    \"\"\"Extract the giant component (largest connected component) from the graph.\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX graph object.\n",
    "    \n",
    "    Returns:\n",
    "        Subgraph containing only the giant component.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO 4: Find the largest connected component and return it as a subgraph\n",
    "    \n",
    "    # Get all connected components in the graph\n",
    "    # A connected component is a set of nodes where you can reach any node from any other node\n",
    "    # In a network, there might be multiple disconnected groups of nodes\n",
    "    # nx.connected_components() returns a generator of sets, where each set contains nodes in one component\n",
    "    components = nx.connected_components(graph)\n",
    "    \n",
    "    # Find the largest component (the \"giant component\")\n",
    "    # This is typically the main network we want to analyze, as it contains most of the nodes\n",
    "    # We use max() with key=len to find the component with the most nodes\n",
    "    giant_component_nodes = max(components, key=len)\n",
    "    \n",
    "    # Create a subgraph containing only the nodes in the giant component\n",
    "    # G.subgraph() creates a new graph that includes only the specified nodes and their edges\n",
    "    # This effectively filters out all the smaller, disconnected components\n",
    "    graph = graph.subgraph(giant_component_nodes)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "G = extract_giant_component(G)\n",
    "\n",
    "# BEGIN TESTS\n",
    "# @name(\"graph_is_connected\")\n",
    "# @description(\"Check that G is connected (giant component)\")\n",
    "def test_graph_is_connected():\n",
    "    assert nx.is_connected(G), \"Graph G should be connected (giant component)\"\n",
    "test_graph_is_connected()\n",
    "\n",
    "# @name(\"giant_component_size\")\n",
    "# @description(\"Check that giant component has correct size\")\n",
    "def test_giant_component_size():\n",
    "    assert G.number_of_nodes() > 17000, \"Giant component does not have correct number of nodes\"\n",
    "test_giant_component_size()\n",
    "\n",
    "# @name(\"giant_component_edges\")\n",
    "# @description(\"Check that giant component has correct number of edges\")\n",
    "def test_giant_component_edges():\n",
    "    assert G.number_of_edges() >  30000, \"Giant component does not have correct number of edges\"\n",
    "test_giant_component_edges()\n",
    "# END TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-21-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38262"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The number of nodes in our new graph\n",
    "len(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-22-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might crash your python!\n",
    "# plot_network(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23-markdown",
   "metadata": {},
   "source": [
    "## Part 3: Save the network, open with Gephi, and carry out Visual Network Analysis\n",
    "We can now save the network as a file, so that we can open it with Gephi and analyze it.\n",
    "\n",
    "There are several fileformats that we can save in, that are compatible with Gephi. GEXF and GraphML are two common formats. You can read more about the formats and their pros/cons on this website: https://networkx.org/documentation/stable/reference/readwrite/index.html \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-24-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize, networkx works poorly. \n",
    "# Let's look in gephi! \n",
    "nx.write_gexf(G,'enron_network.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-25-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the network is too large to deal with on your laptop, you might want to filter out small nodes.\n",
    "\n",
    "# Filter nodes with degree less than N\n",
    "N = 3\n",
    "filtered_nodes = [node for node, degree in G.degree() if degree >= N]\n",
    "\n",
    "# Create a new graph with the filtered nodes\n",
    "filtered_graph = G.subgraph(filtered_nodes).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26-markdown",
   "metadata": {},
   "source": [
    "Now open this file with Gephi and use it to create a better plot of the network!\n",
    "\n",
    "See if you can figure out the different communities, and what charaterizes them. Feel free to use google to search the emails and names!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27-markdown",
   "metadata": {},
   "source": [
    "## Part 4: Use networkx to analyze the convicted bankers\n",
    "We're now going to use networkx to get some exact measures on the network, and to measure the centrality and influence of the two convicted bankers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-28-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally, you would calculate these values using the following functions. You can uncomment them to try it out yourself.\n",
    "# pagerank = nx.pagerank(G)\n",
    "# betweenness = nx.betweenness_centrality(G)\n",
    "# degree_centrality = nx.degree_centrality(G)\n",
    "# eigenvector = nx.eigenvector_centrality(G)\n",
    "\n",
    "# However, this takes a long time, so we've also prepared these values for you here:\n",
    "with open('sem3_topic5_sna_formative1_data2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "pagerank = data['pagerank']\n",
    "betweenness = data['betweenness']\n",
    "degree_centrality = data['degree_centrality']\n",
    "eigenvector = data['eigenvector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-29-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your output should look as follows:\n",
    "\n",
    "# PageRank\n",
    "# Kenneth lay is the number 2 most central.\n",
    "# Jeffrey Skilling is the number 2478 most central\n",
    "\n",
    "# Betweenness\n",
    "# Kenneth lay is the number 6 most central.\n",
    "# Jeffrey Skilling is the number 2158 most central\n",
    "\n",
    "# Degree\n",
    "# Kenneth lay is the number 2 most central.\n",
    "# Jeffrey Skilling is the number 2221 most central\n",
    "\n",
    "# Eigenvector\n",
    "# Kenneth lay is the number 21 most central.\n",
    "# Jeffrey Skilling is the number 1555 most central"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30-markdown",
   "metadata": {},
   "source": [
    "### Exercise 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-31-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank\n",
      "Kenneth lay is the number 11 most central.\n",
      "Jeffrey Skilling is the number 10213 most central.\n",
      "\n",
      "Betweenness\n",
      "Kenneth lay is the number 3 most central.\n",
      "Jeffrey Skilling is the number 8780 most central.\n",
      "\n",
      "Degree\n",
      "Kenneth lay is the number 3 most central.\n",
      "Jeffrey Skilling is the number 9473 most central.\n",
      "\n",
      "Eigenvector\n",
      "Kenneth lay is the number 12 most central.\n",
      "Jeffrey Skilling is the number 6554 most central.\n",
      "Kenneth lay is the number 1 most central.\n",
      "Jeffrey Skilling is the number 4 most central.\n",
      "Kenneth lay is the number 11 most central.\n",
      "Jeffrey Skilling is the number 10213 most central.\n",
      "Kenneth lay is the number 3 most central.\n",
      "Jeffrey Skilling is the number 8780 most central.\n",
      "Kenneth lay is the number 3 most central.\n",
      "Jeffrey Skilling is the number 9473 most central.\n",
      "Kenneth lay is the number 12 most central.\n",
      "Jeffrey Skilling is the number 6554 most central.\n"
     ]
    }
   ],
   "source": [
    "def get_rank(centrality_dict):\n",
    "    \"\"\"Print the rank of Kenneth Lay and Jeffrey Skilling in a given centrality dictionary.\n",
    "    \n",
    "    Args:\n",
    "        centrality_dict: Dictionary mapping node names to centrality scores.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (kenneth_rank, jeffrey_rank) where each is an integer rank or None if not found.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO 5: Iterate through sorted centrality scores and find the rank of the two convicted individuals\n",
    "    \n",
    "    # Define the email addresses of the two convicted executives we want to rank\n",
    "    kenneth_email = 'kenneth.lay@enron.com'\n",
    "    jeffrey_email = 'jeffreyskilling@yahoo.com'\n",
    "    \n",
    "    # Sort the centrality dictionary by values in descending order\n",
    "    # Centrality measures typically have higher values for more central/important nodes\n",
    "    # sorted() with reverse=True gives us nodes from most central to least central\n",
    "    # The result is a list of tuples: (node_name, centrality_score)\n",
    "    sorted_nodes = sorted(centrality_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Initialize variables to store the ranks (1-indexed, where 1 is most central)\n",
    "    kenneth_rank = None\n",
    "    jeffrey_rank = None\n",
    "    \n",
    "    # Iterate through the sorted list with enumerate to get both the index (rank) and the node data\n",
    "    # We use enumerate starting at 1 because ranks are typically 1-indexed (1st, 2nd, 3rd, etc.)\n",
    "    for rank, (node, score) in enumerate(sorted_nodes, start=1):\n",
    "        # Check if this node is Kenneth Lay\n",
    "        if node == kenneth_email:\n",
    "            kenneth_rank = rank  # Store his rank\n",
    "        # Check if this node is Jeffrey Skilling\n",
    "        if node == jeffrey_email:\n",
    "            jeffrey_rank = rank  # Store his rank\n",
    "    \n",
    "    # Print the results in a readable format\n",
    "    # If a person is found, print their rank; otherwise indicate they weren't found\n",
    "    if kenneth_rank is not None:\n",
    "        print(f\"Kenneth lay is the number {kenneth_rank} most central.\")\n",
    "    else:\n",
    "        print(\"Kenneth lay was not found in the centrality dictionary.\")\n",
    "    \n",
    "    if jeffrey_rank is not None:\n",
    "        print(f\"Jeffrey Skilling is the number {jeffrey_rank} most central.\")\n",
    "    else:\n",
    "        print(\"Jeffrey Skilling was not found in the centrality dictionary.\")\n",
    "    \n",
    "    # Return the ranks as a tuple for potential use in tests or further analysis\n",
    "    return (kenneth_rank, jeffrey_rank)\n",
    "\n",
    "print(\"PageRank\")\n",
    "get_rank(pagerank)\n",
    "print(\"\\nBetweenness\")\n",
    "get_rank(betweenness)\n",
    "print(\"\\nDegree\")\n",
    "get_rank(degree_centrality)\n",
    "print(\"\\nEigenvector\")\n",
    "get_rank(eigenvector)\n",
    "\n",
    "# BEGIN TESTS\n",
    "# @name(\"get_rank_sorting\")\n",
    "# @description(\"Check that get_rank function correctly sorts and ranks nodes\")\n",
    "def test_get_rank_sorting():\n",
    "    test_dict = {'a': 0.5, 'kenneth.lay@enron.com': 0.9, 'b': 0.3, 'jeffreyskilling@yahoo.com': 0.1}\n",
    "    kenneth_rank, jeffrey_rank = get_rank(test_dict)\n",
    "    assert kenneth_rank == 1, \"Kenneth Lay ranking is incorrect\"\n",
    "    assert jeffrey_rank == 4, \"Jeffrey Skilling ranking is incorrect\"\n",
    "test_get_rank_sorting()\n",
    "\n",
    "# @name(\"get_rank_pagerank\")\n",
    "# @description(\"Check that Kenneth Lay and Jeffrey Skilling have correct PageRank ranks\")\n",
    "def test_get_rank_pagerank():\n",
    "    if 'kenneth.lay@enron.com' in pagerank:\n",
    "        kenneth_rank, jeffrey_rank = get_rank(pagerank)\n",
    "        assert kenneth_rank is not None and kenneth_rank < 30, \"Kenneth Lay PageRank ranking is incorrect\"\n",
    "        if jeffrey_rank is not None:\n",
    "            assert jeffrey_rank > 5000, \"Jeffrey Skilling PageRank ranking is incorrect\"\n",
    "test_get_rank_pagerank()\n",
    "\n",
    "# @name(\"get_rank_betweenness\")\n",
    "# @description(\"Check that Kenneth Lay and Jeffrey Skilling have correct Betweenness ranks\")\n",
    "def test_get_rank_betweenness():\n",
    "    if 'kenneth.lay@enron.com' in betweenness:\n",
    "        kenneth_rank, jeffrey_rank = get_rank(betweenness)\n",
    "        assert kenneth_rank is not None and kenneth_rank < 20, \"Kenneth Lay Betweenness ranking is incorrect\"\n",
    "        if jeffrey_rank is not None:\n",
    "            assert jeffrey_rank > 5000, \"Jeffrey Skilling Betweenness ranking is incorrect\"\n",
    "test_get_rank_betweenness()\n",
    "\n",
    "# @name(\"get_rank_degree\")\n",
    "# @description(\"Check that Kenneth Lay and Jeffrey Skilling have correct Degree Centrality ranks\")\n",
    "def test_get_rank_degree():\n",
    "    if 'kenneth.lay@enron.com' in degree_centrality:\n",
    "        kenneth_rank, jeffrey_rank = get_rank(degree_centrality)\n",
    "        assert kenneth_rank is not None and kenneth_rank < 20, \"Kenneth Lay Degree Centrality ranking is incorrect\"\n",
    "        if jeffrey_rank is not None:\n",
    "            assert jeffrey_rank > 5000, \"Jeffrey Skilling Degree Centrality ranking is incorrect\"\n",
    "test_get_rank_degree()\n",
    "\n",
    "# @name(\"get_rank_eigenvector\")\n",
    "# @description(\"Check that Kenneth Lay and Jeffrey Skilling have correct Eigenvector Centrality ranks\")\n",
    "def test_get_rank_eigenvector():\n",
    "    if 'kenneth.lay@enron.com' in eigenvector:\n",
    "        kenneth_rank, jeffrey_rank = get_rank(eigenvector)\n",
    "        assert kenneth_rank is not None and kenneth_rank < 30, \"Kenneth Lay Eigenvector Centrality ranking is incorrect\"\n",
    "        if jeffrey_rank is not None:\n",
    "            assert jeffrey_rank > 5000, \"Jeffrey Skilling Eigenvector Centrality ranking is incorrect\"\n",
    "test_get_rank_eigenvector()\n",
    "# END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32-markdown",
   "metadata": {},
   "source": [
    "### 2. Find their communities\n",
    "The second part is to use the Louvain community detection to identify which community the two belong to. \n",
    "\n",
    "We will then put the identified communities as a column in the original dataframe, allowing us to potentially carry out text analysis to identify what characterizes the communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-33-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modularity: 0.7657287229010155\n"
     ]
    }
   ],
   "source": [
    "# Detect communities using the Louvain method\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# Calculate the modularity\n",
    "modularity = community_louvain.modularity(partition, G)\n",
    "\n",
    "print(\"Modularity:\", modularity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34-markdown",
   "metadata": {},
   "source": [
    "### Exercise 3B\n",
    "Which communities are our two convicts in?\n",
    "\n",
    "Use the partition to create a new column in the original df called \"partition\" that contains the partition of the individual associated to the email!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-35-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_partition_columns(dataframe, partition_dict):\n",
    "    \"\"\"Add partition and partition_size columns to the dataframe based on Louvain communities.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: The original dataframe with from_email column.\n",
    "        partition_dict: Dictionary mapping email addresses to partition/community IDs.\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe with added partition and partition_size columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO 6: Create partition column mapping from_email to community ID, and partition_size with community size\n",
    "    \n",
    "    # First, we need to calculate the size of each community/partition\n",
    "    # We'll count how many nodes (email addresses) belong to each partition ID\n",
    "    # This will help us understand how large each community is\n",
    "    \n",
    "    # Import Counter to count occurrences efficiently\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Count how many nodes belong to each partition ID\n",
    "    # Counter creates a dictionary where keys are partition IDs and values are counts\n",
    "    partition_sizes = Counter(partition_dict.values())\n",
    "    \n",
    "    # Create the 'partition' column by mapping each from_email to its community ID\n",
    "    # We use the map() function to look up each email address in the partition_dict\n",
    "    # If an email is not in the partition_dict (e.g., it wasn't in the graph), it will be NaN\n",
    "    dataframe['partition'] = dataframe['from_email'].map(partition_dict)\n",
    "    \n",
    "    # Create the 'partition_size' column by mapping each partition ID to its size\n",
    "    # We use the partition column we just created and map each partition ID to its size\n",
    "    # This tells us how many people are in the same community as each email sender\n",
    "    dataframe['partition_size'] = dataframe['partition'].map(partition_sizes)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "df = add_partition_columns(df, partition)\n",
    "\n",
    "# BEGIN TESTS\n",
    "# @name(\"partition_column_exists\")\n",
    "# @description(\"Check that partition column was created\")\n",
    "def test_partition_column_exists():\n",
    "    assert 'partition' in df.columns, \"Missing column 'partition'\"\n",
    "test_partition_column_exists()\n",
    "\n",
    "# @name(\"partition_size_column_exists\")\n",
    "# @description(\"Check that partition_size column was created\")\n",
    "def test_partition_size_column_exists():\n",
    "    assert 'partition_size' in df.columns, \"Missing column 'partition_size'\"\n",
    "test_partition_size_column_exists()\n",
    "\n",
    "# @name(\"partition_values\")\n",
    "# @description(\"Check that partition column has valid values\")\n",
    "def test_partition_values():\n",
    "    assert df['partition'].notna().any(), \"partition column has no non-null values\"\n",
    "test_partition_values()\n",
    "\n",
    "# @name(\"partition_size_numeric\")\n",
    "# @description(\"Check that partition_size values are numeric\")\n",
    "def test_partition_size_numeric():\n",
    "    assert pd.api.types.is_numeric_dtype(df['partition_size']), \"partition_size should be numeric\"\n",
    "test_partition_size_numeric()\n",
    "\n",
    "# @name(\"partition_values_match_dict\")\n",
    "# @description(\"Check that partition column values correctly match partition_dict\")\n",
    "def test_partition_values_match_dict():\n",
    "    from collections import Counter\n",
    "    # Check that partition values match the partition_dict for emails that exist in both\n",
    "    for idx, row in df.iterrows():\n",
    "        email = row['from_email']\n",
    "        if email in partition:\n",
    "            expected_partition = partition[email]\n",
    "            actual_partition = row['partition']\n",
    "            assert actual_partition == expected_partition, f\"Partition value mismatch for {email}: expected {expected_partition}, got {actual_partition}\"\n",
    "test_partition_values_match_dict()\n",
    "\n",
    "# @name(\"partition_size_correctness\")\n",
    "# @description(\"Check that partition_size values correctly reflect community sizes\")\n",
    "def test_partition_size_correctness():\n",
    "    from collections import Counter\n",
    "    # Calculate expected partition sizes from partition_dict\n",
    "    partition_counts = Counter(partition.values())\n",
    "    # Check that partition_size matches the count of nodes in each partition\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row['partition']):\n",
    "            expected_size = partition_counts[row['partition']]\n",
    "            actual_size = row['partition_size']\n",
    "            assert actual_size == expected_size, f\"Partition size mismatch for partition {row['partition']}: expected {expected_size}, got {actual_size}\"\n",
    "test_partition_size_correctness()\n",
    "# END TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-36-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partition\n",
       "0.0     Mid-Year 2001 Performance Feedback Another ENA...\n",
       "1.0     NEWS Deadline We need news! Expense Market Ins...\n",
       "2.0     Your Approval is Overdue: Access Request for m...\n",
       "3.0     Trading in ERCOT Fwd: FW: sooo true Position F...\n",
       "4.0     Re: Denver trading Win A 2002 Porsche Boxster!...\n",
       "5.0     Yahoo! Newsletter, May 2001 Freidman, Billings...\n",
       "6.0     New Active X for EOL Website Follow up:   New ...\n",
       "7.0     {Resend} E234 Options/Derivatives notes Ventur...\n",
       "8.0     Credit Watch List--Week of 10/22/01 Credit Wat...\n",
       "9.0     Undeliverable: Delivery Status Notification (F...\n",
       "10.0    test mail ClickAtHome Order Verification Fw: A...\n",
       "11.0    Welcome to CustomClips, a Service of Dow Jones...\n",
       "12.0    Conference Call Today with FERC Staff Comments...\n",
       "13.0    RSVP REQUESTED - Emissions Strategy Meeting......\n",
       "14.0     \"Save the Date\" - Associate / Analyst Program...\n",
       "15.0    Quarterly Managing Director Meeting - Monday, ...\n",
       "16.0    BCP Seat Assignments WPS Yarmouth/Wyman UE off...\n",
       "17.0    Mike Grigsby Mike Grigsby EnronOnline- Change ...\n",
       "18.0    4-URGENT - OWA Please print this now. 3 - URGE...\n",
       "19.0    Re: Re: 2- SURVEY - PHILLIP ALLEN 4-URGENT - O...\n",
       "20.0    [Fwd: SF Greater Bay Area Planning Standard] F...\n",
       "21.0    RE: Action Items from Today's Lobbyist Call UR...\n",
       "22.0    FW: appropriate for us these days FW: appropri...\n",
       "23.0    Reminder:  Portland Fundamental Analysis Strat...\n",
       "24.0    Managing Director and Vice President Elections...\n",
       "25.0    NYS Reliability Council Executive Committee NY...\n",
       "26.0    CA supply offers Test List Serve Reminder:  CA...\n",
       "27.0    Confirmation of your Jackson Lewis e-subscript...\n",
       "28.0    FW: Security Smart ID Tags-Off Property Usage ...\n",
       "29.0    Trunkline Gas RE: Gas at Trunkline Lake Charle...\n",
       "30.0    RE: POAs DRAFTS Putin Reception [ENERGYFORUM] ...\n",
       "31.0    Offshore Museum Tour / Calgary Conference Ziff...\n",
       "32.0    CSC ZONE Preliminary TTC ATC COMPLETED - 2001 ...\n",
       "33.0    RE: Haina Note RE: SECLP Mariner Term Sheet RE...\n",
       "34.0    NERC Training Resources Announcement - June 1,...\n",
       "35.0    ECAR (East Bend & Beckjord) Resched Outages NP...\n",
       "36.0    Dow Jones says CPUC will \"Order\" utiltities to...\n",
       "37.0    Stage 3, rotating blackouts Energy Legislation...\n",
       "38.0    Board Book Material Board Book materials New G...\n",
       "39.0    Distribution Disclaimer Distribution Disclaime...\n",
       "40.0    IntercontinentalExchange Index Swaps New Power...\n",
       "41.0                                       Investor lists\n",
       "42.0    DELETE IF NOT ***Latin America trip**** : PRIC...\n",
       "43.0    Fwd: Fwd: FW: number 11 RE: Fwd: Re: Fwd: RE: ...\n",
       "44.0    Evaluations Are In...   Workshop 2 Coming Up A...\n",
       "45.0    WPTF Friday Crazy About U Burrito WPTF Friday ...\n",
       "46.0    Fwd: Market Data Consultancy Fwd: Market Data ...\n",
       "47.0    NuWay Energy (Nasdaq) NWAY An Important eChamp...\n",
       "48.0    Fwd: Wil's Birthday Party hey HAPPY HALLOWEEN]...\n",
       "49.0    Solar Migration - Final Notice Solar Migration...\n",
       "50.0    Removed from Crawler List Start Date: 10/18/01...\n",
       "Name: subject, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can then look at the common subjects for the emails, maybe applying TF-IDF\n",
    "df.loc[~df['subject'].isna()].groupby(['partition'])['subject'].agg(lambda x: \" \".join(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
